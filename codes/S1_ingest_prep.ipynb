{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fb0c4d-bae6-482c-bb0d-f33745f9096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, re, time, math, random, threading, base64, requests\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866763d1-8323-406e-9d75-bd8e89a85639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/lucasborges/.cache/kagglehub/datasets/himanshuwagh/spotify-million/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"himanshuwagh/spotify-million\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c92f2c4-b4dc-4397-8e83-6f1020c0ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd de slices encontrados: 1000\n",
      "Exemplos: ['mpd.slice.0-999.json', 'mpd.slice.1000-1999.json', 'mpd.slice.10000-10999.json', 'mpd.slice.100000-100999.json', 'mpd.slice.101000-101999.json']\n",
      "Arquivos carregados: ['mpd.slice.0-999.json', 'mpd.slice.1000-1999.json', 'mpd.slice.10000-10999.json', 'mpd.slice.100000-100999.json', 'mpd.slice.101000-101999.json', 'mpd.slice.102000-102999.json', 'mpd.slice.103000-103999.json', 'mpd.slice.104000-104999.json', 'mpd.slice.105000-105999.json', 'mpd.slice.106000-106999.json', 'mpd.slice.107000-107999.json', 'mpd.slice.108000-108999.json', 'mpd.slice.109000-109999.json', 'mpd.slice.11000-11999.json', 'mpd.slice.110000-110999.json', 'mpd.slice.111000-111999.json', 'mpd.slice.112000-112999.json', 'mpd.slice.113000-113999.json', 'mpd.slice.114000-114999.json', 'mpd.slice.115000-115999.json', 'mpd.slice.116000-116999.json', 'mpd.slice.117000-117999.json', 'mpd.slice.118000-118999.json', 'mpd.slice.119000-119999.json', 'mpd.slice.12000-12999.json', 'mpd.slice.120000-120999.json', 'mpd.slice.121000-121999.json', 'mpd.slice.122000-122999.json', 'mpd.slice.123000-123999.json', 'mpd.slice.124000-124999.json', 'mpd.slice.125000-125999.json', 'mpd.slice.126000-126999.json', 'mpd.slice.127000-127999.json', 'mpd.slice.128000-128999.json', 'mpd.slice.129000-129999.json', 'mpd.slice.13000-13999.json', 'mpd.slice.130000-130999.json', 'mpd.slice.131000-131999.json', 'mpd.slice.132000-132999.json', 'mpd.slice.133000-133999.json', 'mpd.slice.134000-134999.json', 'mpd.slice.135000-135999.json', 'mpd.slice.136000-136999.json', 'mpd.slice.137000-137999.json', 'mpd.slice.138000-138999.json', 'mpd.slice.139000-139999.json', 'mpd.slice.14000-14999.json', 'mpd.slice.140000-140999.json', 'mpd.slice.141000-141999.json', 'mpd.slice.142000-142999.json', 'mpd.slice.143000-143999.json', 'mpd.slice.144000-144999.json', 'mpd.slice.145000-145999.json', 'mpd.slice.146000-146999.json', 'mpd.slice.147000-147999.json', 'mpd.slice.148000-148999.json', 'mpd.slice.149000-149999.json', 'mpd.slice.15000-15999.json', 'mpd.slice.150000-150999.json', 'mpd.slice.151000-151999.json', 'mpd.slice.152000-152999.json', 'mpd.slice.153000-153999.json', 'mpd.slice.154000-154999.json', 'mpd.slice.155000-155999.json', 'mpd.slice.156000-156999.json', 'mpd.slice.157000-157999.json', 'mpd.slice.158000-158999.json', 'mpd.slice.159000-159999.json', 'mpd.slice.16000-16999.json', 'mpd.slice.160000-160999.json', 'mpd.slice.161000-161999.json', 'mpd.slice.162000-162999.json', 'mpd.slice.163000-163999.json', 'mpd.slice.164000-164999.json', 'mpd.slice.165000-165999.json', 'mpd.slice.166000-166999.json', 'mpd.slice.167000-167999.json', 'mpd.slice.168000-168999.json', 'mpd.slice.169000-169999.json', 'mpd.slice.17000-17999.json', 'mpd.slice.170000-170999.json', 'mpd.slice.171000-171999.json', 'mpd.slice.172000-172999.json', 'mpd.slice.173000-173999.json', 'mpd.slice.174000-174999.json', 'mpd.slice.175000-175999.json', 'mpd.slice.176000-176999.json', 'mpd.slice.177000-177999.json', 'mpd.slice.178000-178999.json', 'mpd.slice.179000-179999.json', 'mpd.slice.18000-18999.json', 'mpd.slice.180000-180999.json', 'mpd.slice.181000-181999.json', 'mpd.slice.182000-182999.json', 'mpd.slice.183000-183999.json', 'mpd.slice.184000-184999.json', 'mpd.slice.185000-185999.json', 'mpd.slice.186000-186999.json', 'mpd.slice.187000-187999.json', 'mpd.slice.188000-188999.json']\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(path, \"data\")\n",
    "\n",
    "# Lista todos os arquivos slice (cada slice possui 1000 playlists)\n",
    "files = sorted(glob(os.path.join(data_path, \"mpd.slice.*.json\")))\n",
    "print(f\"Qtd de slices encontrados: {len(files)}\")\n",
    "print(\"Exemplos:\", [os.path.basename(f) for f in files[:5]])\n",
    "\n",
    "\n",
    "subset_files = files[:100] # Carrega apenas os 100 primeiros slices (100.000 playlists)\n",
    "print(\"Arquivos carregados:\", [os.path.basename(f) for f in subset_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67df1eb-a862-4bed-9b2e-5e82b096cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playlists — shape: (100000, 12)\n",
      "Colunas de playlists: ['pid', 'name', 'description', 'modified_at', 'num_tracks', 'num_albums', 'num_artists', 'num_followers', 'num_edits', 'duration_ms', 'collaborative', 'tracks']\n",
      "\n",
      "Tracks — shape: (6685101, 9)\n",
      "Colunas de tracks: ['track_uri', 'track_name', 'artist_uri', 'artist_name', 'album_uri', 'album_name', 'duration_ms', 'pos', 'pid']\n"
     ]
    }
   ],
   "source": [
    "# Separa playlists e tracks em datasets distintos\n",
    "\n",
    "# Sub-datasets\n",
    "playlist_rows = []\n",
    "track_rows = []\n",
    "\n",
    "# Campos comuns do MPD\n",
    "playlist_fields = [\n",
    "    \"pid\", \"name\", \"description\", \"modified_at\", \"num_tracks\", \"num_albums\", \"num_artists\",\n",
    "    \"num_followers\", \"num_edits\", \"duration_ms\", \"collaborative\", \"tracks\"\n",
    "]\n",
    "\n",
    "track_fields = [\n",
    "    \"track_uri\", \"track_name\", \"artist_uri\", \"artist_name\",\n",
    "    \"album_uri\", \"album_name\", \"duration_ms\", \"pos\"\n",
    "]\n",
    "\n",
    "for f in subset_files:\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fp:\n",
    "        blob = json.load(fp)\n",
    "\n",
    "    playlists = blob.get(\"playlists\", [])\n",
    "    # Flatten de playlists\n",
    "    for p in playlists:\n",
    "        playlist_rows.append({k: p.get(k, None) for k in playlist_fields})\n",
    "\n",
    "        # Flatten de tracks (uma linha por faixa por playlist)\n",
    "        for t in p.get(\"tracks\", []):\n",
    "            row = {k: t.get(k, None) for k in track_fields}\n",
    "            row[\"pid\"] = p.get(\"pid\")  # chave estrangeira para ligar com a playlist\n",
    "            track_rows.append(row)\n",
    "\n",
    "# DataFrames finais\n",
    "df_playlists = pd.DataFrame(playlist_rows)\n",
    "df_tracks = pd.DataFrame(track_rows)\n",
    "\n",
    "print(\"Playlists — shape:\", df_playlists.shape)\n",
    "print(\"Colunas de playlists:\", list(df_playlists.columns))\n",
    "\n",
    "print(\"\\nTracks — shape:\", df_tracks.shape)\n",
    "print(\"Colunas de tracks:\", list(df_tracks.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af9975b-b6d4-438a-8465-28396bba3e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   pid            100000 non-null  int64 \n",
      " 1   name           100000 non-null  object\n",
      " 2   description    1796 non-null    object\n",
      " 3   modified_at    100000 non-null  int64 \n",
      " 4   num_tracks     100000 non-null  int64 \n",
      " 5   num_albums     100000 non-null  int64 \n",
      " 6   num_artists    100000 non-null  int64 \n",
      " 7   num_followers  100000 non-null  int64 \n",
      " 8   num_edits      100000 non-null  int64 \n",
      " 9   duration_ms    100000 non-null  int64 \n",
      " 10  collaborative  100000 non-null  object\n",
      " 11  tracks         100000 non-null  object\n",
      "dtypes: int64(8), object(4)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_playlists.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949969c7-83cf-4b7f-bb75-65040d81b06e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6685101 entries, 0 to 6685100\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   track_uri    object\n",
      " 1   track_name   object\n",
      " 2   artist_uri   object\n",
      " 3   artist_name  object\n",
      " 4   album_uri    object\n",
      " 5   album_name   object\n",
      " 6   duration_ms  int64 \n",
      " 7   pos          int64 \n",
      " 8   pid          int64 \n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 459.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_tracks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd8a243b-cb2b-466e-9e17-78e86ee59ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar datasets \"originais\"\n",
    "\n",
    "df_playlists.to_parquet(\"TCC/data/raw/df_playlists_v0.parquet\", index=False)\n",
    "df_tracks.to_parquet(\"TCC/data/raw/df_tracks_v0.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43f30b-3168-4cf8-a89a-b12bc865d816",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Audio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "397b5649-cba5-4218-b774-f3c2847966e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas sem Spotify ID válido: 0\n"
     ]
    }
   ],
   "source": [
    "# Extração do Spotify Track ID (Reccobeats)\n",
    "\n",
    "import re\n",
    "\n",
    "# ID do Spotify: 22 caracteres base62\n",
    "_SPOTIFY_ID_RE = re.compile(r'^[A-Za-z0-9]{22}$')\n",
    "\n",
    "# URL do Spotify (aceita variações com /intl-xx/)\n",
    "_SPOTIFY_URL_RE = re.compile(\n",
    "    r'^https?://open\\.spotify\\.com/(?:intl-[a-z]{2}/)?track/([A-Za-z0-9]{22})(?:\\?.*)?$',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# URI do Spotify\n",
    "_SPOTIFY_URI_RE = re.compile(\n",
    "    r'^spotify:track:([A-Za-z0-9]{22})$'\n",
    ")\n",
    "\n",
    "def extract_spotify_id(track_uri: object) -> str | None: #Extração do Spotify Track ID\n",
    "    if not isinstance(track_uri, str):\n",
    "        return None\n",
    "    s = track_uri.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    m = _SPOTIFY_URI_RE.match(s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    m = _SPOTIFY_URL_RE.match(s)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "\n",
    "    if _SPOTIFY_ID_RE.match(s):\n",
    "        return s\n",
    "\n",
    "    return None\n",
    "\n",
    "# Aplicação no df_tracks\n",
    "df_tracks = df_tracks.copy()\n",
    "df_tracks[\"track_spotify_id\"] = df_tracks[\"track_uri\"].apply(extract_spotify_id)\n",
    "\n",
    "# Verificar se alguma linha não possui Spotify ID válido\n",
    "missing = df_tracks[\"track_spotify_id\"].isna().sum()\n",
    "print(f\"Linhas sem Spotify ID válido: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe40541b-e84f-412b-abf5-17c6e7443285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "from typing import List, Dict, Any, Iterable, Optional\n",
    "\n",
    "# Regex para extrair Spotify ID do href \n",
    "_SPOTIFY_HREF_RE = re.compile(\n",
    "    r'^https?://open\\.spotify\\.com/(?:intl-[a-z]{2}/)?track/([A-Za-z0-9]{22})(?:\\?.*)?$',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def _extract_spotify_id_from_href(href: Optional[str]) -> Optional[str]:\n",
    "    if not href or not isinstance(href, str):\n",
    "        return None\n",
    "    m = _SPOTIFY_HREF_RE.match(href.strip())\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# Colunas esperadas (inclui UUID interno e spotify_id)\n",
    "_EXPECTED_KEYS = [\n",
    "    \"id\",          # UUID interno ReccoBeats (metadado)\n",
    "    \"spotify_id\",  # <- chave útil para join depois\n",
    "    \"href\",\n",
    "    \"acousticness\",\"danceability\",\"energy\",\"instrumentalness\",\n",
    "    \"key\",\"liveness\",\"loudness\",\"mode\",\"speechiness\",\"tempo\",\"valence\",\n",
    "]\n",
    "\n",
    "def _ensure_keys(d: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {k: d.get(k, None) for k in _EXPECTED_KEYS}\n",
    "\n",
    "def _flatten_item_from_content(item: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    A resposta real vem como {\"content\": [ {...}, ... ]}.\n",
    "    Cada item já traz as features no próprio nível + 'href' (URL Spotify).\n",
    "    'id' é UUID interno; extraímos 'spotify_id' do 'href'.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {}\n",
    "    out[\"id\"] = item.get(\"id\")\n",
    "    out[\"href\"] = item.get(\"href\")\n",
    "    out[\"spotify_id\"] = _extract_spotify_id_from_href(out[\"href\"])\n",
    "\n",
    "    out[\"acousticness\"]     = item.get(\"acousticness\")\n",
    "    out[\"danceability\"]     = item.get(\"danceability\")\n",
    "    out[\"energy\"]           = item.get(\"energy\")\n",
    "    out[\"instrumentalness\"] = item.get(\"instrumentalness\")\n",
    "    out[\"key\"]              = item.get(\"key\")\n",
    "    out[\"liveness\"]         = item.get(\"liveness\")\n",
    "    out[\"loudness\"]         = item.get(\"loudness\")\n",
    "    out[\"mode\"]             = item.get(\"mode\")\n",
    "    out[\"speechiness\"]      = item.get(\"speechiness\")\n",
    "    out[\"tempo\"]            = item.get(\"tempo\")\n",
    "    out[\"valence\"]          = item.get(\"valence\")\n",
    "    return _ensure_keys(out)\n",
    "\n",
    "def _normalize_response(data: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Normaliza para lista de dicts com _EXPECTED_KEYS.\n",
    "    Suporta o formato real: {\"content\": [ {...}, ... ]}.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    if data is None:\n",
    "        return rows\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        if \"content\" in data and isinstance(data[\"content\"], list):\n",
    "            for el in data[\"content\"]:\n",
    "                if isinstance(el, dict):\n",
    "                    rows.append(_flatten_item_from_content(el))\n",
    "            return rows\n",
    "        # Fallback defensivo\n",
    "        for alt in (\"audio_features\",\"audioFeatures\",\"items\",\"results\",\"tracks\"):\n",
    "            if alt in data and isinstance(data[alt], list):\n",
    "                for el in data[alt]:\n",
    "                    if isinstance(el, dict):\n",
    "                        rows.append(_flatten_item_from_content(el))\n",
    "                return rows\n",
    "        # Objeto único (raro aqui)\n",
    "        if data:\n",
    "            rows.append(_flatten_item_from_content(data))\n",
    "        return rows\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        for el in data:\n",
    "            if isinstance(el, dict):\n",
    "                rows.append(_flatten_item_from_content(el))\n",
    "        return rows\n",
    "\n",
    "    return rows\n",
    "\n",
    "def chunked(seq: Iterable[Any], n: int) -> Iterable[list]:\n",
    "    \"\"\"Quebra uma sequência em blocos de tamanho n.\"\"\"\n",
    "    buf = []\n",
    "    for item in seq:\n",
    "        buf.append(item)\n",
    "        if len(buf) == n:\n",
    "            yield buf\n",
    "            buf = []\n",
    "    if buf:\n",
    "        yield buf\n",
    "\n",
    "# Tratamento de rate-limit (429)\n",
    "class RateLimit(Exception):\n",
    "    def __init__(self, retry_after: float | None = None):\n",
    "        self.retry_after = retry_after or 1.0\n",
    "        super().__init__(f\"Rate limited. Retry after ~{self.retry_after}s\")\n",
    "\n",
    "def _fetch_features_batch(ids_batch: List[str], timeout: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Endpoint oficial de múltiplos:\n",
    "      GET https://api.reccobeats.com/v1/audio-features?ids=id1,id2,...,idN\n",
    "    Levanta RateLimit se 429, usando Retry-After (com jitter).\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.reccobeats.com/v1/audio-features\"\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    params = {\"ids\": \",\".join(ids_batch)}  \n",
    "\n",
    "    r = requests.get(base_url, headers=headers, params=params, timeout=timeout)\n",
    "\n",
    "    if r.status_code == 429:\n",
    "        retry_after = r.headers.get(\"Retry-After\")\n",
    "        try:\n",
    "            retry_after = float(retry_after)\n",
    "        except (TypeError, ValueError):\n",
    "            retry_after = 2.0  # fallback conservador\n",
    "        retry_after *= (1.0 + 0.2 * random.random())  # jitter até 20%\n",
    "        raise RateLimit(retry_after)\n",
    "\n",
    "    r.raise_for_status()\n",
    "    return _normalize_response(r.json())\n",
    "\n",
    "def _read_checkpoint(path: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        return pd.read_parquet(path)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _migrate_checkpoint(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Se o checkpoint antigo não tiver 'spotify_id', cria a coluna a partir de 'href'\n",
    "    e deduplica por 'spotify_id'. Retorna DF pronto para uso como cache.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if \"spotify_id\" not in df.columns:\n",
    "        if \"href\" in df.columns:\n",
    "            df[\"spotify_id\"] = df[\"href\"].apply(_extract_spotify_id_from_href)\n",
    "        else:\n",
    "            df[\"spotify_id\"] = None\n",
    "    # manter apenas colunas conhecidas (quando possível) e deduplicar por spotify_id\n",
    "    for col in _EXPECTED_KEYS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    df = df[_EXPECTED_KEYS]\n",
    "    df = df.dropna(subset=[\"spotify_id\"]).drop_duplicates(subset=[\"spotify_id\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _safe_concat_cache(prev: Optional[pd.DataFrame], cur: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatena prev + cur de forma segura (evita FutureWarning com DFs vazios/NA).\n",
    "    Deduplica por 'spotify_id'.\n",
    "    \"\"\"\n",
    "    if prev is None or prev.empty:\n",
    "        base = pd.DataFrame(columns=_EXPECTED_KEYS) if cur is None else cur\n",
    "    else:\n",
    "        base = prev\n",
    "    if cur is None or cur.empty:\n",
    "        out = base\n",
    "    else:\n",
    "        # drop colunas all-NA para evitar dtype indefinido\n",
    "        base2 = base.dropna(axis=1, how='all')\n",
    "        cur2 = cur.dropna(axis=1, how='all')\n",
    "        out = pd.concat([base2, cur2], ignore_index=True)\n",
    "    if \"spotify_id\" in out.columns:\n",
    "        out = out.dropna(subset=[\"spotify_id\"]).drop_duplicates(subset=[\"spotify_id\"], keep=\"first\")\n",
    "    # garantir todas as colunas esperadas\n",
    "    for col in _EXPECTED_KEYS:\n",
    "        if col not in out.columns:\n",
    "            out[col] = None\n",
    "    return out[_EXPECTED_KEYS].reset_index(drop=True)\n",
    "\n",
    "def fetch_audio_features_in_batches(\n",
    "    track_ids: Iterable[Optional[str]],\n",
    "    batch_size: int = 20,               #conservador por causa do 429\n",
    "    pause_seconds: float = 0.5,         #pausa base entre lotes\n",
    "    timeout: int = 20,\n",
    "    max_retries: int = 2,               #para erros != 429\n",
    "    backoff_factor: float = 0.75,\n",
    "    progress_every: int = 100,          #imprime a cada 100 lotes\n",
    "    checkpoint_every: int = 2_000,      #salva a cada 2k lotes\n",
    "    checkpoint_path: Optional[str] = \"features_checkpoint.parquet\",\n",
    "    jitter_base_pause: bool = True,     #aplica jitter na pausa base\n",
    "    early_stop_after_batches: Optional[int] = None,  #ex.: 600\n",
    "    min_hit_rate: Optional[float] = None,            #ex.: 5.0 (%)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Busca audio features em lotes e retorna um DataFrame com colunas _EXPECTED_KEYS.\n",
    "\n",
    "    - Respeita 429 (Retry-After) repetindo o mesmo lote.\n",
    "    - Retries + backoff para outros erros.\n",
    "    - Logs de progresso + hit-rate (conta apenas linhas com spotify_id).\n",
    "    - Checkpoints periódicos (parquet/csv) e retomada com migração automática.\n",
    "    - Deduplica por spotify_id.\n",
    "    - Segunda passada para 5xx com batch_size=5.\n",
    "    \"\"\"\n",
    "    # IDs únicos e válidos (lista)\n",
    "    ids = pd.Series(track_ids).dropna().astype(str)\n",
    "    ids = ids[ids.str.len() > 0].unique().tolist()\n",
    "\n",
    "    # Carregar checkpoint (se existir) + MIGRAR se faltar 'spotify_id'\n",
    "    prev_ck = None\n",
    "    collected_spotify_ids: set[str] = set()\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        ck = _read_checkpoint(checkpoint_path)\n",
    "        if ck is not None:\n",
    "            ck = _migrate_checkpoint(ck)\n",
    "            prev_ck = ck\n",
    "            collected_spotify_ids = set(ck[\"spotify_id\"].astype(str).unique())\n",
    "            # pular IDs já coletados\n",
    "            ids = [x for x in ids if x not in collected_spotify_ids]\n",
    "            print(f\"[Resume] Checkpoint: {len(collected_spotify_ids):,} já coletados | Pendentes: {len(ids):,}\")\n",
    "        else:\n",
    "            print(\"[Resume] Falha ao ler checkpoint; seguindo...\")\n",
    "\n",
    "    if not ids:\n",
    "        # Nada a fazer; devolve o checkpoint migrado como resultado (se existir)\n",
    "        if prev_ck is not None:\n",
    "            return prev_ck\n",
    "        return pd.DataFrame(columns=_EXPECTED_KEYS)\n",
    "\n",
    "    total_batches = math.ceil(len(ids) / batch_size)\n",
    "    all_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    # métricas\n",
    "    total_sent = 0\n",
    "    total_received = 0  #só com spotify_id válido\n",
    "    last_checkpoint_batch = 0\n",
    "    failed_ids_5xx: List[str] = []  #para segunda passada\n",
    "\n",
    "    for b_idx, batch in enumerate(chunked(ids, batch_size), start=1):\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                rows = _fetch_features_batch(batch, timeout=timeout)\n",
    "                # contar apenas válidos (com spotify_id)\n",
    "                valid_rows = [r for r in rows if r.get(\"spotify_id\")]\n",
    "                all_rows.extend(valid_rows)\n",
    "                total_sent += len(batch)\n",
    "                total_received += len(valid_rows)\n",
    "                attempt = 0\n",
    "                break\n",
    "            except RateLimit as rl:\n",
    "                wait_s = max(rl.retry_after, pause_seconds)\n",
    "                print(f\"[429] Lote {b_idx}/{total_batches}: aguardando {wait_s:.2f}s (Retry-After)\")\n",
    "                time.sleep(wait_s)\n",
    "                continue\n",
    "            except requests.HTTPError as e:\n",
    "                # se for 5xx e estourou os retries, guardamos o lote\n",
    "                status = getattr(getattr(e, \"response\", None), \"status_code\", None)\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    if isinstance(status, int) and 500 <= status < 600:\n",
    "                        failed_ids_5xx.extend(batch)\n",
    "                    print(f\"[ERRO] Lote {b_idx}/{total_batches}: HTTP {e}. Sem mais retries, seguindo.\")\n",
    "                    break\n",
    "                sleep_s = backoff_factor * (attempt ** 2)\n",
    "                print(f\"[WARN] Lote {b_idx}: HTTPError. Retry {attempt}/{max_retries} em {sleep_s:.2f}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "            except requests.RequestException as e:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    print(f\"[ERRO] Lote {b_idx}/{total_batches}: {e}. Sem mais retries, seguindo.\")\n",
    "                    break\n",
    "                sleep_s = backoff_factor * (attempt ** 2)\n",
    "                print(f\"[WARN] Lote {b_idx}: NetworkError. Retry {attempt}/{max_retries} em {sleep_s:.2f}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "\n",
    "        # progresso + hit-rate\n",
    "        if (b_idx % progress_every == 0) or (b_idx == 1):\n",
    "            hit_rate = (total_received / total_sent * 100.0) if total_sent else 0.0\n",
    "            print(f\"[Progresso] {b_idx}/{total_batches} lotes | enviados: {total_sent:,} | recebidos: {total_received:,} | hit-rate: {hit_rate:.2f}%\")\n",
    "\n",
    "        # early-stop opcional\n",
    "        if early_stop_after_batches and min_hit_rate is not None and b_idx % early_stop_after_batches == 0:\n",
    "            hit_rate = (total_received / total_sent * 100.0) if total_sent else 0.0\n",
    "            if hit_rate < min_hit_rate:\n",
    "                print(f\"[HALT] Hit-rate {hit_rate:.2f}% abaixo de {min_hit_rate:.2f}% após {b_idx} lotes. Interrompendo para reavaliar.\")\n",
    "                break\n",
    "\n",
    "        # checkpoint periódico\n",
    "        if checkpoint_path and (b_idx % checkpoint_every == 0 or b_idx == total_batches):\n",
    "            df_ck_new = pd.DataFrame(all_rows, columns=_EXPECTED_KEYS)\n",
    "            if not df_ck_new.empty:\n",
    "                df_ck_new = df_ck_new.dropna(subset=[\"spotify_id\"]).drop_duplicates(subset=[\"spotify_id\"], keep=\"first\")\n",
    "            # concat segura com o checkpoint anterior\n",
    "            to_write = _safe_concat_cache(prev_ck, df_ck_new)\n",
    "            # salvar\n",
    "            if checkpoint_path.lower().endswith(\".parquet\"):\n",
    "                to_write.to_parquet(checkpoint_path, index=False)\n",
    "            else:\n",
    "                to_write.to_csv(checkpoint_path, index=False)\n",
    "            prev_ck = to_write\n",
    "            last_checkpoint_batch = b_idx\n",
    "            print(f\"[Checkpoint] Salvo em '{checkpoint_path}' (lote {b_idx}/{total_batches}).\")\n",
    "\n",
    "        # pausa base (com jitter opcional)\n",
    "        if jitter_base_pause:\n",
    "            time.sleep(pause_seconds * (0.9 + 0.2 * random.random()))\n",
    "        else:\n",
    "            time.sleep(pause_seconds)\n",
    "\n",
    "    # Segunda passada para 5xx (com lotes menores)\n",
    "    if failed_ids_5xx:\n",
    "        print(f\"[Retry pass] Reprocessando {len(failed_ids_5xx)} IDs de 5xx com batch_size=5...\")\n",
    "        for b2_idx, batch2 in enumerate(chunked(failed_ids_5xx, 5), start=1):\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    rows2 = _fetch_features_batch(batch2, timeout=timeout)\n",
    "                    valid2 = [r for r in rows2 if r.get(\"spotify_id\")]\n",
    "                    all_rows.extend(valid2)\n",
    "                    total_sent += len(batch2)\n",
    "                    total_received += len(valid2)\n",
    "                    break\n",
    "                except RateLimit as rl:\n",
    "                    wait_s = max(rl.retry_after, pause_seconds)\n",
    "                    print(f\"[429][retry pass] Lote {b2_idx}: aguardando {wait_s:.2f}s...\")\n",
    "                    time.sleep(wait_s)\n",
    "                    continue\n",
    "                except requests.RequestException as e2:\n",
    "                    attempt += 1\n",
    "                    if attempt > max_retries:\n",
    "                        print(f\"[ERRO][retry pass] Lote {b2_idx}: {e2}. desistindo deste mini-lote.\")\n",
    "                        break\n",
    "                    sleep_s = backoff_factor * (attempt ** 2)\n",
    "                    print(f\"[WARN][retry pass] Lote {b2_idx}: retry {attempt}/{max_retries} em {sleep_s:.2f}s...\")\n",
    "                    time.sleep(sleep_s)\n",
    "\n",
    "        # salvar checkpoint após retry pass\n",
    "        if checkpoint_path:\n",
    "            df_ck_retry = pd.DataFrame(all_rows, columns=_EXPECTED_KEYS)\n",
    "            if not df_ck_retry.empty:\n",
    "                df_ck_retry = df_ck_retry.dropna(subset=[\"spotify_id\"]).drop_duplicates(subset=[\"spotify_id\"], keep=\"first\")\n",
    "            to_write = _safe_concat_cache(prev_ck, df_ck_retry)\n",
    "            if checkpoint_path.lower().endswith(\".parquet\"):\n",
    "                to_write.to_parquet(checkpoint_path, index=False)\n",
    "            else:\n",
    "                to_write.to_csv(checkpoint_path, index=False)\n",
    "            prev_ck = to_write\n",
    "            print(f\"[Checkpoint] Salvo após retry pass em '{checkpoint_path}'.\")\n",
    "\n",
    "    # DF final\n",
    "    df_features = pd.DataFrame(all_rows, columns=_EXPECTED_KEYS)\n",
    "    if not df_features.empty and \"spotify_id\" in df_features.columns:\n",
    "        df_features = df_features.dropna(subset=[\"spotify_id\"]).drop_duplicates(subset=[\"spotify_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    # Se houver checkpoint acumulando mais dados, devolve a união segura\n",
    "    if prev_ck is not None:\n",
    "        df_features = _safe_concat_cache(prev_ck, df_features)\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039a542d-86e3-41fe-a41f-27707f1210e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs válidos: 6,685,101\n",
      "IDs únicos : 679,889\n"
     ]
    }
   ],
   "source": [
    "# Número de IDs únicos no df_tracks\n",
    "\n",
    "n_valid = df_tracks[\"track_spotify_id\"].notna().sum()\n",
    "n_unique = df_tracks[\"track_spotify_id\"].dropna().nunique()\n",
    "\n",
    "print(f\"IDs válidos: {n_valid:,}\")\n",
    "print(f\"IDs únicos : {n_unique:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "986e311d-8f7e-43a7-b565-44b71cecd36c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Resume] Checkpoint: 572,008 já coletados | Pendentes: 292,609\n",
      "[Progresso] 1/7316 lotes | enviados: 40 | recebidos: 0 | hit-rate: 0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Coleta dos dados:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m subset_tracks \u001b[38;5;241m=\u001b[39m df_tracks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_spotify_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m----> 4\u001b[0m df_features \u001b[38;5;241m=\u001b[39m fetch_audio_features_in_batches(\n\u001b[1;32m      5\u001b[0m     track_ids\u001b[38;5;241m=\u001b[39msubset_tracks,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m      7\u001b[0m     pause_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, \n\u001b[1;32m      8\u001b[0m     progress_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      9\u001b[0m     checkpoint_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2_000\u001b[39m,\n\u001b[1;32m     10\u001b[0m     checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures_checkpoint.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_features\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[15], line 245\u001b[0m, in \u001b[0;36mfetch_audio_features_in_batches\u001b[0;34m(track_ids, batch_size, pause_seconds, timeout, max_retries, backoff_factor, progress_every, checkpoint_every, checkpoint_path, jitter_base_pause, early_stop_after_batches, min_hit_rate)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         rows \u001b[38;5;241m=\u001b[39m _fetch_features_batch(batch, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# contar apenas válidos (com spotify_id)\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         valid_rows \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rows \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspotify_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[15], line 117\u001b[0m, in \u001b[0;36m_fetch_features_batch\u001b[0;34m(ids_batch, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    115\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ids_batch)}  \n\u001b[0;32m--> 117\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(base_url, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39mparams, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m    120\u001b[0m     retry_after \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry-After\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    788\u001b[0m     conn,\n\u001b[1;32m    789\u001b[0m     method,\n\u001b[1;32m    790\u001b[0m     url,\n\u001b[1;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    705\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    706\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    199\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    201\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    202\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Coleta dos dados:\n",
    "\n",
    "subset_tracks = df_tracks[\"track_spotify_id\"].dropna().unique()\n",
    "df_features = fetch_audio_features_in_batches(\n",
    "    track_ids=subset_tracks,\n",
    "    batch_size=40,\n",
    "    pause_seconds=0.25, \n",
    "    progress_every=1000,\n",
    "    checkpoint_every=2_000,\n",
    "    checkpoint_path=\"features_checkpoint.parquet\",\n",
    ")\n",
    "print(df_features.shape)\n",
    "display(df_features.head())\n",
    "df_features.to_parquet(\"TCC/data/raw/df_audio_features.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff0992-b0fb-4296-beb1-22613f28ca83",
   "metadata": {},
   "source": [
    "## Albums features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8c5807-acd4-460e-9fb3-6b5effc40772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotify - dim_albums (paralelo, com auto-refresh, checkpoint, rate limit e cap de Retry-After)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any, Iterable, Optional\n",
    "\n",
    "# Constantes e helpers\n",
    "DIM_COLS = [\"id\", \"release_date\", \"release_date_precision\", \"album_type\", \"total_tracks\", \"label\", \"popularity\"]\n",
    "_ALBUM_URI_RE = re.compile(r\"^spotify:album:([A-Za-z0-9]{22})$\")\n",
    "\n",
    "def extract_album_id(uri: str) -> Optional[str]:\n",
    "    if isinstance(uri, str):\n",
    "        m = _ALBUM_URI_RE.match(uri.strip())\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return None\n",
    "\n",
    "def chunked(seq: Iterable[Any], n: int):\n",
    "    buf = []\n",
    "    for x in seq:\n",
    "        buf.append(x)\n",
    "        if len(buf) == n:\n",
    "            yield buf; buf = []\n",
    "    if buf:\n",
    "        yield buf\n",
    "\n",
    "# -------- Exceções --------\n",
    "class RateLimit(Exception):\n",
    "    def __init__(self, retry_after: float = 1.0):\n",
    "        self.retry_after = retry_after\n",
    "        super().__init__(f\"429 – espere ~{self.retry_after}s\")\n",
    "\n",
    "class TokenExpired(Exception):\n",
    "    pass\n",
    "\n",
    "# -------- Auth (Client Credentials) --------\n",
    "def get_spotify_token(client_id: str, client_secret: str) -> tuple[str, int]:\n",
    "    if not client_id or not client_secret:\n",
    "        raise ValueError(\"Defina SPOTIFY_CLIENT_ID e SPOTIFY_CLIENT_SECRET.\")\n",
    "    auth = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n",
    "    headers = {\"Authorization\": f\"Basic {auth}\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    data = {\"grant_type\": \"client_credentials\"}\n",
    "    r = requests.post(\"https://accounts.spotify.com/api/token\", headers=headers, data=data, timeout=20)\n",
    "    try:\n",
    "        r.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        try: detail = r.json()\n",
    "        except Exception: detail = r.text\n",
    "        raise RuntimeError(f\"Falha ao obter token ({r.status_code}). Detalhes: {detail}\") from e\n",
    "    j = r.json()\n",
    "    return j[\"access_token\"], int(j.get(\"expires_in\", 3600))\n",
    "\n",
    "def _normalize_albums_response(data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for a in data.get(\"albums\") or []:\n",
    "        if not isinstance(a, dict) or a is None:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"id\": a.get(\"id\"),\n",
    "            \"release_date\": a.get(\"release_date\"),\n",
    "            \"release_date_precision\": a.get(\"release_date_precision\"),\n",
    "            \"album_type\": a.get(\"album_type\"),\n",
    "            \"total_tracks\": a.get(\"total_tracks\"),\n",
    "            \"label\": a.get(\"label\"),\n",
    "            \"popularity\": a.get(\"popularity\"),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def _read_checkpoint(path: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        return pd.read_parquet(path)\n",
    "    except Exception:\n",
    "        try: return pd.read_csv(path)\n",
    "        except Exception: return None\n",
    "\n",
    "def _safe_concat(prev: Optional[pd.DataFrame], cur: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    if prev is None or prev.empty:\n",
    "        base = pd.DataFrame(columns=DIM_COLS) if cur is None else cur\n",
    "    else:\n",
    "        base = prev\n",
    "    if cur is None or cur.empty:\n",
    "        out = base\n",
    "    else:\n",
    "        out = pd.concat([base, cur], ignore_index=True)\n",
    "    out = out.dropna(subset=[\"id\"]).drop_duplicates(subset=[\"id\"])\n",
    "    if not out.empty:\n",
    "        out[\"total_tracks\"] = pd.to_numeric(out[\"total_tracks\"], errors=\"coerce\").astype(\"Int16\")\n",
    "        out[\"popularity\"]   = pd.to_numeric(out[\"popularity\"], errors=\"coerce\").astype(\"Int16\")\n",
    "    return out[DIM_COLS].reset_index(drop=True)\n",
    "\n",
    "# -------- Rate limit global (RPS) e cooldown 429 --------\n",
    "class GlobalCooldown:\n",
    "    \"\"\"Coordena espera global quando qualquer worker recebe 429.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._until = 0.0\n",
    "        self._lock = threading.Lock()\n",
    "    def wait(self):\n",
    "        while True:\n",
    "            with self._lock:\n",
    "                until = self._until\n",
    "            now = time.time()\n",
    "            if now >= until:\n",
    "                return\n",
    "            time.sleep(min(1.0, until - now))\n",
    "    def set(self, seconds: float):\n",
    "        with self._lock:\n",
    "            self._until = max(self._until, time.time() + float(seconds))\n",
    "\n",
    "class GlobalRateLimiter:\n",
    "    \"\"\"Limita o número de requests por segundo no conjunto de threads.\"\"\"\n",
    "    def __init__(self, rps: float):\n",
    "        self.rps = max(0.0, float(rps))\n",
    "        self.lock = threading.Lock()\n",
    "        self.next_t = 0.0\n",
    "    def wait(self):\n",
    "        if self.rps <= 0:\n",
    "            return\n",
    "        interval = 1.0 / self.rps\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            if now < self.next_t:\n",
    "                time.sleep(self.next_t - now)\n",
    "                now = time.time()\n",
    "            # agenda próximo slot\n",
    "            self.next_t = max(now, self.next_t) + interval\n",
    "\n",
    "# -------- Token manager (thread-safe) --------\n",
    "class TokenManager:\n",
    "    def __init__(self, client_id: str, client_secret: str, ttl_margin: int = 120):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.ttl_margin = ttl_margin\n",
    "        self._lock = threading.RLock()\n",
    "        self._token = None\n",
    "        self._exp = 0.0\n",
    "        self.refresh()\n",
    "    def refresh(self):\n",
    "        with self._lock:\n",
    "            tok, ttl = get_spotify_token(self.client_id, self.client_secret)\n",
    "            self._token = tok\n",
    "            self._exp = time.time() + (ttl or 3600)\n",
    "    def get(self) -> str:\n",
    "        with self._lock:\n",
    "            if time.time() > self._exp - self.ttl_margin:\n",
    "                old = self._token\n",
    "                self.refresh()\n",
    "                if old != self._token:\n",
    "                    print(\"[Auth] Token renovado proativamente.\")\n",
    "            return self._token\n",
    "    def force_refresh(self):\n",
    "        with self._lock:\n",
    "            self.refresh()\n",
    "            print(\"[Auth] Token renovado após 401.\")\n",
    "\n",
    "# -------- requests.Session por thread --------\n",
    "_thread_local = threading.local()\n",
    "def _get_session() -> requests.Session:\n",
    "    s = getattr(_thread_local, \"session\", None)\n",
    "    if s is None:\n",
    "        s = requests.Session()\n",
    "        _thread_local.session = s\n",
    "    return s\n",
    "\n",
    "def _fetch_batch_worker(\n",
    "    ids_batch: List[str],\n",
    "    token_mgr: TokenManager,\n",
    "    cooldown: GlobalCooldown,\n",
    "    ratelimiter: GlobalRateLimiter,\n",
    "    timeout: int = 20,\n",
    "    max_retries: int = 3,\n",
    "    backoff_factor: float = 0.75,\n",
    "    max_retry_after_seconds: float = 120.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Busca 1 batch (<=20 IDs). Respeita 429 global, RPS global e renova token em 401.\n",
    "       Se Retry-After > max_retry_after_seconds, sinaliza 'hard_429' para interromper a coleta.\"\"\"\n",
    "    sess = _get_session()\n",
    "    url = \"https://api.spotify.com/v1/albums\"\n",
    "    params = {\"ids\": \",\".join(ids_batch[:20])}\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        cooldown.wait()       # espera global por 429 anterior\n",
    "        ratelimiter.wait()    # impõe RPS global\n",
    "        token = token_mgr.get()\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\", \"Accept\": \"application/json\"}\n",
    "        try:\n",
    "            r = sess.get(url, headers=headers, params=params, timeout=timeout)\n",
    "        except requests.RequestException as e:\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                return {\"rows\": [], \"sent\": len(ids_batch), \"recv\": 0, \"error\": f\"NetworkError: {e}\"}\n",
    "            time.sleep(backoff_factor * (attempt ** 2) * (1.0 + 0.2*random.random()))\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            rows = _normalize_albums_response(data)\n",
    "            return {\"rows\": rows, \"sent\": len(ids_batch), \"recv\": sum(1 for x in rows if x.get(\"id\")), \"error\": None}\n",
    "\n",
    "        if r.status_code == 401:\n",
    "            token_mgr.force_refresh()\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            ra_hdr = r.headers.get(\"Retry-After\")\n",
    "            try:\n",
    "                wait = float(ra_hdr) if ra_hdr is not None else 2.0\n",
    "            except ValueError:\n",
    "                wait = 2.0\n",
    "            # se vier um Retry-After absurdo (p. ex., ~90.000s), interrompe graciosamente\n",
    "            if wait > max_retry_after_seconds:\n",
    "                return {\"rows\": [], \"sent\": len(ids_batch), \"recv\": 0, \"error\": f\"429 Retry-After={wait}\", \"hard_429\": True, \"retry_after\": wait}\n",
    "            wait *= (1.0 + 0.2*random.random())\n",
    "            print(f\"[429] Aguardando {wait:.2f}s (batch de {len(ids_batch)})\")\n",
    "            cooldown.set(wait)\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        attempt += 1\n",
    "        if attempt > max_retries:\n",
    "            return {\"rows\": [], \"sent\": len(ids_batch), \"recv\": 0, \"error\": f\"HTTP {r.status_code}: {r.text[:200]}\"}\n",
    "        time.sleep(backoff_factor * (attempt ** 2) * (1.0 + 0.2*random.random()))\n",
    "\n",
    "def fetch_dim_albums_parallel(\n",
    "    album_ids: Iterable[str],\n",
    "    client_id: str,\n",
    "    client_secret: str,\n",
    "    batch_size: int = 20,           # Spotify: máx 20\n",
    "    max_workers: int = 4,           # comece baixo\n",
    "    in_flight_multiplier: int = 2,  # poucos pedidos simultâneos\n",
    "    rate_limit_per_sec: float = 0.5,# ex.: 0.5 = 1 req a cada 2s\n",
    "    timeout: int = 12,\n",
    "    progress_every: int = 50,\n",
    "    checkpoint_every: int = 1000,\n",
    "    checkpoint_path: str = \"dim_albums_checkpoint.parquet\",\n",
    "    max_retry_after_seconds: float = 120.0,  # se vier maior que isso, interrompe e devolve\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    ids = pd.Series(album_ids).dropna().astype(str)\n",
    "    ids = ids[ids.str.len() > 0].unique().tolist()\n",
    "\n",
    "    prev_ck = None\n",
    "    collected_ids = set()\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        ck = _read_checkpoint(checkpoint_path)\n",
    "        if ck is not None and \"id\" in ck.columns:\n",
    "            prev_ck = ck[DIM_COLS].copy()\n",
    "            prev_ck = prev_ck.dropna(subset=[\"id\"]).drop_duplicates(\"id\")\n",
    "            collected_ids = set(prev_ck[\"id\"].astype(str))\n",
    "            ids = [x for x in ids if x not in collected_ids]\n",
    "            # Aqui não temos mapeamento album_id->coletado; continuamos buscando todos,\n",
    "            # mas o dedupe evita duplicar; se quiser otimizar, mantenha um mapa das ids colhidas.\n",
    "            print(f\"[Resume] Checkpoint: {len(prev_ck):,} já coletados.\")\n",
    "\n",
    "    if not ids:\n",
    "        return prev_ck if prev_ck is not None else pd.DataFrame(columns=DIM_COLS)\n",
    "\n",
    "    batch_size = max(1, min(int(batch_size), 20))\n",
    "    batches: List[List[str]] = [ids[i:i+batch_size] for i in range(0, len(ids), batch_size)]\n",
    "    total_batches = len(batches)\n",
    "\n",
    "    token_mgr = TokenManager(client_id, client_secret, ttl_margin=120)\n",
    "    cooldown = GlobalCooldown()\n",
    "    ratelimiter = GlobalRateLimiter(rate_limit_per_sec)\n",
    "\n",
    "    all_rows: List[Dict[str, Any]] = []\n",
    "    completed = 0\n",
    "    total_sent = 0\n",
    "    total_recv = 0\n",
    "\n",
    "    inflight_limit = max_workers * in_flight_multiplier\n",
    "    i = 0\n",
    "    futures = set()\n",
    "\n",
    "    def flush_checkpoint():\n",
    "        nonlocal all_rows, prev_ck\n",
    "        if not all_rows:\n",
    "            return\n",
    "        df_new = pd.DataFrame(all_rows, columns=DIM_COLS)\n",
    "        df_new = df_new.dropna(subset=[\"id\"]).drop_duplicates(\"id\")\n",
    "        to_write = _safe_concat(prev_ck, df_new)\n",
    "        if checkpoint_path.lower().endswith(\".parquet\"):\n",
    "            to_write.to_parquet(checkpoint_path, index=False)\n",
    "        else:\n",
    "            to_write.to_csv(checkpoint_path, index=False)\n",
    "        prev_ck = to_write\n",
    "        all_rows = []\n",
    "\n",
    "    print(f\"[Dispatch] Total batches: {total_batches} | max_workers={max_workers} | in_flight={inflight_limit} | rps={rate_limit_per_sec}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        # pré-enche o pipeline\n",
    "        while i < total_batches and len(futures) < inflight_limit:\n",
    "            fut = ex.submit(_fetch_batch_worker, batches[i], token_mgr, cooldown, ratelimiter,\n",
    "                            timeout, 3, 0.75, max_retry_after_seconds)\n",
    "            futures.add(fut); i += 1\n",
    "        print(f\"[Dispatch] Primeiros {len(futures)} futures submetidos.\")\n",
    "\n",
    "        # processa conforme termina\n",
    "        while futures:\n",
    "            for fut in as_completed(list(futures), timeout=None):\n",
    "                futures.remove(fut)\n",
    "                res = fut.result()\n",
    "                total_sent += res.get(\"sent\", 0)\n",
    "                total_recv += res.get(\"recv\", 0)\n",
    "                rows = res.get(\"rows\") or []\n",
    "                if rows:\n",
    "                    all_rows.extend(rows)\n",
    "\n",
    "                # hard_429: Retry-After muito alto → salva checkpoint e interrompe\n",
    "                if res.get(\"hard_429\"):\n",
    "                    wait = res.get(\"retry_after\")\n",
    "                    print(f\"[HARD 429] Retry-After ~{wait:.0f}s. Interrompendo agora e salvando checkpoint. Retome mais tarde.\")\n",
    "                    flush_checkpoint()\n",
    "                    # monta DF parcial e retorna\n",
    "                    df_dim = pd.DataFrame(all_rows, columns=DIM_COLS)\n",
    "                    if not df_dim.empty:\n",
    "                        df_dim = df_dim.dropna(subset=[\"id\"]).drop_duplicates(\"id\").reset_index(drop=True)\n",
    "                        df_dim[\"total_tracks\"] = pd.to_numeric(df_dim[\"total_tracks\"], errors=\"coerce\").astype(\"Int16\")\n",
    "                        df_dim[\"popularity\"]   = pd.to_numeric(df_dim[\"popularity\"], errors=\"coerce\").astype(\"Int16\")\n",
    "                    if prev_ck is not None and not prev_ck.empty:\n",
    "                        df_dim = _safe_concat(prev_ck, df_dim)\n",
    "                    return df_dim\n",
    "\n",
    "                completed += 1\n",
    "                if (completed % progress_every == 0) or (completed == 1):\n",
    "                    hit = (100.0 * total_recv / total_sent) if total_sent else 0.0\n",
    "                    print(f\"[Progresso] {completed}/{total_batches} lotes | enviados: {total_sent:,} | recebidos: {total_recv:,} | hit-rate: {hit:.2f}%\")\n",
    "\n",
    "                if checkpoint_path and (completed % checkpoint_every == 0 or completed == total_batches):\n",
    "                    flush_checkpoint()\n",
    "\n",
    "                # reabastece\n",
    "                if i < total_batches:\n",
    "                    fut2 = ex.submit(_fetch_batch_worker, batches[i], token_mgr, cooldown, ratelimiter,\n",
    "                                     timeout, 3, 0.75, max_retry_after_seconds)\n",
    "                    futures.add(fut2); i += 1\n",
    "\n",
    "    # DF final\n",
    "    df_dim = pd.DataFrame(all_rows, columns=DIM_COLS)\n",
    "    if not df_dim.empty:\n",
    "        df_dim = df_dim.dropna(subset=[\"id\"]).drop_duplicates(\"id\").reset_index(drop=True)\n",
    "        df_dim[\"total_tracks\"] = pd.to_numeric(df_dim[\"total_tracks\"], errors=\"coerce\").astype(\"Int16\")\n",
    "        df_dim[\"popularity\"]   = pd.to_numeric(df_dim[\"popularity\"], errors=\"coerce\").astype(\"Int16\")\n",
    "\n",
    "    if prev_ck is not None and not prev_ck.empty:\n",
    "        df_dim = _safe_concat(prev_ck, df_dim)\n",
    "\n",
    "    return df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cdb0ce8-1eb4-4564-853f-895b1c2583d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPOTIFY_CLIENT_ID=9c9030a3c0234c128f8a9d1302c4a695\n",
      "env: SPOTIFY_CLIENT_SECRET=20d8ceab184a4cff9d66c9a749f31053\n"
     ]
    }
   ],
   "source": [
    "%env SPOTIFY_CLIENT_ID=9c9030a3c0234c128f8a9d1302c4a695\n",
    "%env SPOTIFY_CLIENT_SECRET=20d8ceab184a4cff9d66c9a749f31053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925e38c8-3ace-415b-b230-2f31c6c502e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Álbuns únicos no dataset: 270,156\n",
      "[Resume] Checkpoint: 270,082 já coletados.\n",
      "[Dispatch] Total batches: 4 | max_workers=4 | in_flight=12 | rps=1\n",
      "[Dispatch] Primeiros 4 futures submetidos.\n",
      "[Progresso] 1/4 lotes | enviados: 20 | recebidos: 0 | hit-rate: 0.00%\n",
      "(270082, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>release_date</th>\n",
       "      <th>release_date_precision</th>\n",
       "      <th>album_type</th>\n",
       "      <th>total_tracks</th>\n",
       "      <th>label</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6vV5UrXcfyQD1wu4Qo2I9K</td>\n",
       "      <td>2005-07-04</td>\n",
       "      <td>day</td>\n",
       "      <td>album</td>\n",
       "      <td>16</td>\n",
       "      <td>Atlantic Records/ATG</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0z7pVBGOD7HCIB7S8eLkLI</td>\n",
       "      <td>2003-11-13</td>\n",
       "      <td>day</td>\n",
       "      <td>album</td>\n",
       "      <td>13</td>\n",
       "      <td>Jive</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25hVFAxTlDvXbx2X2QkUkE</td>\n",
       "      <td>2003-06-23</td>\n",
       "      <td>day</td>\n",
       "      <td>album</td>\n",
       "      <td>16</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6QPkyl04rXwTGlGlcYaRoW</td>\n",
       "      <td>2002-11-04</td>\n",
       "      <td>day</td>\n",
       "      <td>album</td>\n",
       "      <td>13</td>\n",
       "      <td>Jive</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6NmFmPX56pcLBOFMhIiKvF</td>\n",
       "      <td>2000</td>\n",
       "      <td>year</td>\n",
       "      <td>album</td>\n",
       "      <td>15</td>\n",
       "      <td>Geffen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id release_date release_date_precision album_type  \\\n",
       "0  6vV5UrXcfyQD1wu4Qo2I9K   2005-07-04                    day      album   \n",
       "1  0z7pVBGOD7HCIB7S8eLkLI   2003-11-13                    day      album   \n",
       "2  25hVFAxTlDvXbx2X2QkUkE   2003-06-23                    day      album   \n",
       "3  6QPkyl04rXwTGlGlcYaRoW   2002-11-04                    day      album   \n",
       "4  6NmFmPX56pcLBOFMhIiKvF         2000                   year      album   \n",
       "\n",
       "   total_tracks                 label  popularity  \n",
       "0            16  Atlantic Records/ATG          57  \n",
       "1            13                  Jive          74  \n",
       "2            16              Columbia          11  \n",
       "3            13                  Jive          75  \n",
       "4            15                Geffen           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Credenciais Spotify (no Jupyter, SEM aspas nas %env)\n",
    "CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "\n",
    "# Garante a coluna de IDs de álbuns\n",
    "if \"album_spotify_id\" not in df_tracks.columns:\n",
    "    df_tracks = df_tracks.copy()\n",
    "    df_tracks[\"album_spotify_id\"] = df_tracks[\"album_uri\"].apply(extract_album_id)\n",
    "\n",
    "album_ids_unique = df_tracks[\"album_spotify_id\"].dropna().unique().tolist()\n",
    "print(f\"Álbuns únicos no dataset: {len(album_ids_unique):,}\")\n",
    "\n",
    "# Rodada \"leve\" para destravar (pode aumentar depois)\n",
    "dim_albums = fetch_dim_albums_parallel(\n",
    "    album_ids=album_ids_unique,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    batch_size=20,                 # máx 20 por chamada\n",
    "    max_workers=4,                 \n",
    "    in_flight_multiplier=3,        \n",
    "    rate_limit_per_sec=1,        \n",
    "    timeout=12,\n",
    "    progress_every=100,\n",
    "    checkpoint_every=500,\n",
    "    checkpoint_path=\"dim_albums_checkpoint.parquet\",\n",
    "    max_retry_after_seconds=120.0, # se vier Retry-After > 120s, interrompe e salva\n",
    ")\n",
    "\n",
    "print(dim_albums.shape)\n",
    "display(dim_albums.head())\n",
    "dim_albums.to_parquet(\"TCC/data/raw/dim_albums.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c030f-7bc0-485b-bf9e-4fec40f624bd",
   "metadata": {},
   "source": [
    "## Artists features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "794ba20e-4b37-4cfc-8305-219643a71f69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spotify - dim_artists (paralelo, com auto-refresh, checkpoint, rate limit e cap de Retry-After)\n",
    "\n",
    "import os, re, time, math, random, threading, base64, requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any, Iterable, Optional\n",
    "\n",
    "# Constantes e helpers \n",
    "ARTIST_DIM_COLS = [\"id\", \"followers_total\", \"popularity\", \"genres\"]\n",
    "_ARTIST_URI_RE = re.compile(r\"^spotify:artist:([A-Za-z0-9]{22})$\")\n",
    "\n",
    "def extract_artist_id(uri: str) -> Optional[str]:\n",
    "    if isinstance(uri, str):\n",
    "        m = _ARTIST_URI_RE.match(uri.strip())\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return None\n",
    "\n",
    "def chunked(seq: Iterable[Any], n: int):\n",
    "    buf = []\n",
    "    for x in seq:\n",
    "        buf.append(x)\n",
    "        if len(buf) == n:\n",
    "            yield buf; buf = []\n",
    "    if buf:\n",
    "        yield buf\n",
    "\n",
    "# -------- Exceções --------\n",
    "class RateLimit(Exception):\n",
    "    def __init__(self, retry_after: float = 1.0):\n",
    "        self.retry_after = retry_after\n",
    "        super().__init__(f\"429 – espere ~{self.retry_after}s\")\n",
    "\n",
    "class TokenExpired(Exception):\n",
    "    pass\n",
    "\n",
    "# -------- Auth (Client Credentials) --------\n",
    "def get_spotify_token(client_id: str, client_secret: str) -> tuple[str, int]:\n",
    "    if not client_id or not client_secret:\n",
    "        raise ValueError(\"Defina SPOTIFY_CLIENT_ID e SPOTIFY_CLIENT_SECRET.\")\n",
    "    auth = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n",
    "    headers = {\"Authorization\": f\"Basic {auth}\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    data = {\"grant_type\": \"client_credentials\"}\n",
    "    r = requests.post(\"https://accounts.spotify.com/api/token\", headers=headers, data=data, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    return j[\"access_token\"], int(j.get(\"expires_in\", 3600))\n",
    "\n",
    "# -------- Normalização do /v1/artists --------\n",
    "def _normalize_artists_response(data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    out = []\n",
    "    for a in data.get(\"artists\") or []:\n",
    "        if not isinstance(a, dict) or a is None:\n",
    "            continue\n",
    "        followers = a.get(\"followers\") or {}\n",
    "        row = {\n",
    "            \"id\": a.get(\"id\"),\n",
    "            \"followers_total\": followers.get(\"total\"),\n",
    "            \"popularity\": a.get(\"popularity\"),\n",
    "            \"genres\": a.get(\"genres\") or [],\n",
    "        }\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "# -------- Checkpoint helpers --------\n",
    "def _read_checkpoint(path: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        return pd.read_parquet(path)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _safe_concat_artists(prev: Optional[pd.DataFrame], cur: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    if prev is None or prev.empty:\n",
    "        base = pd.DataFrame(columns=ARTIST_DIM_COLS) if cur is None else cur\n",
    "    else:\n",
    "        base = prev\n",
    "    if cur is None or cur.empty:\n",
    "        out = base\n",
    "    else:\n",
    "        out = pd.concat([base, cur], ignore_index=True)\n",
    "\n",
    "    # limpa e dtypes\n",
    "    out = out.dropna(subset=[\"id\"]).drop_duplicates(subset=[\"id\"])\n",
    "    if not out.empty:\n",
    "        out[\"followers_total\"] = pd.to_numeric(out[\"followers_total\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        out[\"popularity\"]      = pd.to_numeric(out[\"popularity\"], errors=\"coerce\").astype(\"Int16\")\n",
    "        # genres permanece como list[object] (dtype 'object')\n",
    "    # garante ordem/colunas\n",
    "    for c in ARTIST_DIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = pd.NA\n",
    "    return out[ARTIST_DIM_COLS].reset_index(drop=True)\n",
    "\n",
    "# -------- Rate limit global (RPS) e cooldown 429 --------\n",
    "class GlobalCooldown:\n",
    "    \"\"\"Coordena espera global quando qualquer worker recebe 429.\"\"\"\n",
    "    def __init__(self):\n",
    "        self._until = 0.0\n",
    "        self._lock = threading.Lock()\n",
    "    def wait(self):\n",
    "        while True:\n",
    "            with self._lock:\n",
    "                until = self._until\n",
    "            now = time.time()\n",
    "            if now >= until:\n",
    "                return\n",
    "            time.sleep(min(1.0, until - now))\n",
    "    def set(self, seconds: float):\n",
    "        with self._lock:\n",
    "            self._until = max(self._until, time.time() + float(seconds))\n",
    "\n",
    "class GlobalRateLimiter:\n",
    "    \"\"\"Limita o número de requests por segundo no conjunto de threads.\"\"\"\n",
    "    def __init__(self, rps: float):\n",
    "        self.rps = max(0.0, float(rps))\n",
    "        self.lock = threading.Lock()\n",
    "        self.next_t = 0.0\n",
    "    def wait(self):\n",
    "        if self.rps <= 0:\n",
    "            return\n",
    "        interval = 1.0 / self.rps\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            if now < self.next_t:\n",
    "                time.sleep(self.next_t - now)\n",
    "                now = time.time()\n",
    "            self.next_t = max(now, self.next_t) + interval\n",
    "\n",
    "# -------- Token manager (thread-safe) --------\n",
    "class TokenManager:\n",
    "    def __init__(self, client_id: str, client_secret: str, ttl_margin: int = 120):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.ttl_margin = ttl_margin\n",
    "        self._lock = threading.RLock()\n",
    "        self._token = None\n",
    "        self._exp = 0.0\n",
    "        self.refresh()\n",
    "    def refresh(self):\n",
    "        with self._lock:\n",
    "            tok, ttl = get_spotify_token(self.client_id, self.client_secret)\n",
    "            self._token = tok\n",
    "            self._exp = time.time() + (ttl or 3600)\n",
    "    def get(self) -> str:\n",
    "        with self._lock:\n",
    "            if time.time() > self._exp - self.ttl_margin:\n",
    "                old = self._token\n",
    "                self.refresh()\n",
    "                if old != self._token:\n",
    "                    print(\"[Auth] Token renovado proativamente.\")\n",
    "            return self._token\n",
    "    def force_refresh(self):\n",
    "        with self._lock:\n",
    "            self.refresh()\n",
    "            print(\"[Auth] Token renovado após 401.\")\n",
    "\n",
    "# -------- requests.Session por thread --------\n",
    "_thread_local = threading.local()\n",
    "def _get_session() -> requests.Session:\n",
    "    s = getattr(_thread_local, \"session\", None)\n",
    "    if s is None:\n",
    "        s = requests.Session()\n",
    "        _thread_local.session = s\n",
    "    return s\n",
    "\n",
    "def _fetch_artists_batch_worker(\n",
    "    ids_batch: List[str],\n",
    "    token_mgr: TokenManager,\n",
    "    cooldown: GlobalCooldown,\n",
    "    ratelimiter: GlobalRateLimiter,\n",
    "    timeout: int = 20,\n",
    "    max_retries: int = 3,\n",
    "    backoff_factor: float = 0.75,\n",
    "    max_retry_after_seconds: float = 120.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Busca 1 batch (<=50 IDs) do /v1/artists. Respeita 429 global, RPS global e renova token em 401.\n",
    "       Se Retry-After > max_retry_after_seconds, sinaliza 'hard_429' para interromper a coleta.\"\"\"\n",
    "    sess = _get_session()\n",
    "    url = \"https://api.spotify.com/v1/artists\"\n",
    "    params = {\"ids\": \",\".join(ids_batch[:50])}\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        cooldown.wait()\n",
    "        ratelimiter.wait()\n",
    "        token = token_mgr.get()\n",
    "        headers = {\"Authorization\": f\"Bearer {token}\", \"Accept\": \"application/json\"}\n",
    "        try:\n",
    "            r = sess.get(url, headers=headers, params=params, timeout=timeout)\n",
    "        except requests.RequestException as e:\n",
    "            attempt += 1\n",
    "            if attempt > max_retries:\n",
    "                return {\"rows\": [], \"sent\": len(ids_batch), \"recv\": 0, \"error\": f\"NetworkError: {e}\"}\n",
    "            time.sleep(backoff_factor * (attempt ** 2) * (1.0 + 0.2*random.random()))\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            rows = _normalize_artists_response(data)\n",
    "            return {\"rows\": rows, \"sent\": len(ids_batch), \"recv\": sum(1 for x in rows if x.get(\"id\")), \"error\": None}\n",
    "\n",
    "        if r.status_code == 401:\n",
    "            token_mgr.force_refresh()\n",
    "            continue\n",
    "\n",
    "        if r.status_code == 429:\n",
    "            ra_hdr = r.headers.get(\"Retry-After\")\n",
    "            try:\n",
    "                wait = float(ra_hdr) if ra_hdr is not None else 2.0\n",
    "            except ValueError:\n",
    "                wait = 2.0\n",
    "            if wait > max_retry_after_seconds:\n",
    "                return {\"rows\": [], \"sent\": len(ids_batch), \"recv\": 0, \"error\": f\"429 Retry-After={wait}\", \"hard_429\": True, \"retry_after\": wait}\n",
    "            wait *= (1.0 + 0.2*random.random())\n",
    "            print(f\"[429] Aguardando {wait:.2f}s (batch de {len(ids_batch)})\")\n",
    "            cooldown.set(wait)\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        attempt += 1\n",
    "        if attempt > max_retries:\n",
    "            return {\"rows\": [], \"sent\": len(ids_batch), \"recv\": 0, \"error\": f\"HTTP {r.status_code}: {r.text[:200]}\"}\n",
    "        time.sleep(backoff_factor * (attempt ** 2) * (1.0 + 0.2*random.random()))\n",
    "\n",
    "def fetch_dim_artists_parallel(\n",
    "    artist_ids: Iterable[str],\n",
    "    client_id: str,\n",
    "    client_secret: str,\n",
    "    batch_size: int = 50,           # Spotify: máx 50 por chamada\n",
    "    max_workers: int = 4,           # comece baixo\n",
    "    in_flight_multiplier: int = 2,  # poucos pedidos simultâneos\n",
    "    rate_limit_per_sec: float = 0.5,# ex.: 0.5 = 1 req a cada 2s\n",
    "    timeout: int = 12,\n",
    "    progress_every: int = 50,\n",
    "    checkpoint_every: int = 1000,\n",
    "    checkpoint_path: str = \"dim_artists_checkpoint.parquet\",\n",
    "    max_retry_after_seconds: float = 120.0,  # se vier maior que isso, interrompe e devolve\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    ids = pd.Series(artist_ids).dropna().astype(str)\n",
    "    ids = ids[ids.str.len() > 0].unique().tolist()\n",
    "\n",
    "    # checkpoint / retomar removendo já coletados\n",
    "    prev_ck = None\n",
    "    collected_ids = set()\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        ck = _read_checkpoint(checkpoint_path)\n",
    "        if ck is not None and \"id\" in ck.columns:\n",
    "            prev_ck = ck[ARTIST_DIM_COLS].copy()\n",
    "            prev_ck = prev_ck.dropna(subset=[\"id\"]).drop_duplicates(\"id\")\n",
    "            collected_ids = set(prev_ck[\"id\"].astype(str))\n",
    "            ids = [x for x in ids if x not in collected_ids]\n",
    "            print(f\"[Resume] Checkpoint: {len(collected_ids):,} já coletados | Pendentes: {len(ids):,}\")\n",
    "\n",
    "    if not ids:\n",
    "        return prev_ck if prev_ck is not None else pd.DataFrame(columns=ARTIST_DIM_COLS)\n",
    "\n",
    "    batch_size = max(1, min(int(batch_size), 50))\n",
    "    batches: List[List[str]] = [ids[i:i+batch_size] for i in range(0, len(ids), batch_size)]\n",
    "    total_batches = len(batches)\n",
    "\n",
    "    token_mgr = TokenManager(client_id, client_secret, ttl_margin=120)\n",
    "    cooldown = GlobalCooldown()\n",
    "    ratelimiter = GlobalRateLimiter(rate_limit_per_sec)\n",
    "\n",
    "    all_rows: List[Dict[str, Any]] = []\n",
    "    completed = 0\n",
    "    total_sent = 0\n",
    "    total_recv = 0\n",
    "\n",
    "    inflight_limit = max_workers * in_flight_multiplier\n",
    "    i = 0\n",
    "    futures = set()\n",
    "\n",
    "    def flush_checkpoint():\n",
    "        nonlocal all_rows, prev_ck\n",
    "        if not all_rows:\n",
    "            return\n",
    "        df_new = pd.DataFrame(all_rows, columns=ARTIST_DIM_COLS)\n",
    "        df_new = df_new.dropna(subset=[\"id\"]).drop_duplicates(\"id\")\n",
    "        to_write = _safe_concat_artists(prev_ck, df_new)\n",
    "        if checkpoint_path.lower().endswith(\".parquet\"):\n",
    "            to_write.to_parquet(checkpoint_path, index=False)\n",
    "        else:\n",
    "            to_write.to_csv(checkpoint_path, index=False)\n",
    "        prev_ck = to_write\n",
    "        all_rows = []\n",
    "\n",
    "    print(f\"[Dispatch] Total batches: {total_batches} | max_workers={max_workers} | in_flight={inflight_limit} | rps={rate_limit_per_sec}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        # pré-enche o pipeline\n",
    "        while i < total_batches and len(futures) < inflight_limit:\n",
    "            fut = ex.submit(_fetch_artists_batch_worker, batches[i], token_mgr, cooldown, ratelimiter,\n",
    "                            timeout, 3, 0.75, max_retry_after_seconds)\n",
    "            futures.add(fut); i += 1\n",
    "        print(f\"[Dispatch] Primeiros {len(futures)} futures submetidos.\")\n",
    "\n",
    "        # processa conforme termina\n",
    "        while futures:\n",
    "            for fut in as_completed(list(futures), timeout=None):\n",
    "                futures.remove(fut)\n",
    "                res = fut.result()\n",
    "                total_sent += res.get(\"sent\", 0)\n",
    "                total_recv += res.get(\"recv\", 0)\n",
    "                rows = res.get(\"rows\") or []\n",
    "                if rows:\n",
    "                    all_rows.extend(rows)\n",
    "\n",
    "                if res.get(\"hard_429\"):\n",
    "                    wait = res.get(\"retry_after\")\n",
    "                    print(f\"[HARD 429] Retry-After ~{wait:.0f}s. Interrompendo agora e salvando checkpoint. Retome mais tarde.\")\n",
    "                    flush_checkpoint()\n",
    "                    df_dim = pd.DataFrame(all_rows, columns=ARTIST_DIM_COLS)\n",
    "                    if not df_dim.empty:\n",
    "                        df_dim = df_dim.dropna(subset=[\"id\"]).drop_duplicates(\"id\").reset_index(drop=True)\n",
    "                        df_dim[\"followers_total\"] = pd.to_numeric(df_dim[\"followers_total\"], errors=\"coerce\").astype(\"Int64\")\n",
    "                        df_dim[\"popularity\"]      = pd.to_numeric(df_dim[\"popularity\"], errors=\"coerce\").astype(\"Int16\")\n",
    "                    if prev_ck is not None and not prev_ck.empty:\n",
    "                        df_dim = _safe_concat_artists(prev_ck, df_dim)\n",
    "                    return df_dim\n",
    "\n",
    "                completed += 1\n",
    "                if (completed % progress_every == 0) or (completed == 1):\n",
    "                    hit = (100.0 * total_recv / total_sent) if total_sent else 0.0\n",
    "                    print(f\"[Progresso] {completed}/{total_batches} lotes | enviados: {total_sent:,} | recebidos: {total_recv:,} | hit-rate: {hit:.2f}%\")\n",
    "\n",
    "                if checkpoint_path and (completed % checkpoint_every == 0 or completed == total_batches):\n",
    "                    flush_checkpoint()\n",
    "\n",
    "                if i < total_batches:\n",
    "                    fut2 = ex.submit(_fetch_artists_batch_worker, batches[i], token_mgr, cooldown, ratelimiter,\n",
    "                                     timeout, 3, 0.75, max_retry_after_seconds)\n",
    "                    futures.add(fut2); i += 1\n",
    "\n",
    "    df_dim = pd.DataFrame(all_rows, columns=ARTIST_DIM_COLS)\n",
    "    if not df_dim.empty:\n",
    "        df_dim = df_dim.dropna(subset=[\"id\"]).drop_duplicates(\"id\").reset_index(drop=True)\n",
    "        df_dim[\"followers_total\"] = pd.to_numeric(df_dim[\"followers_total\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df_dim[\"popularity\"]      = pd.to_numeric(df_dim[\"popularity\"], errors=\"coerce\").astype(\"Int16\")\n",
    "\n",
    "    if prev_ck is not None and not prev_ck.empty:\n",
    "        df_dim = _safe_concat_artists(prev_ck, df_dim)\n",
    "\n",
    "    return df_dim\n",
    "\n",
    "# -------- Pré-voo (opcional) --------\n",
    "def spotify_window_ready_artists(client_id: str, client_secret: str, test_artist_id: str = \"3TVXtAsR1Inumwj472S9r4\"):  # Drake\n",
    "    tok, _ = get_spotify_token(client_id, client_secret)\n",
    "    r = requests.get(\n",
    "        \"https://api.spotify.com/v1/artists\",\n",
    "        headers={\"Authorization\": f\"Bearer {tok}\"},\n",
    "        params={\"ids\": test_artist_id},\n",
    "        timeout=15,\n",
    "    )\n",
    "    if r.status_code == 200:\n",
    "        return True, 0\n",
    "    if r.status_code == 429:\n",
    "        ra = r.headers.get(\"Retry-After\")\n",
    "        try:\n",
    "            secs = int(float(ra))\n",
    "        except Exception:\n",
    "            secs = 60\n",
    "        return False, max(secs, 1)\n",
    "    return False, 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f60ed0b-2d54-4781-838e-3523e96baa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artistas únicos no dataset: 109,043\n",
      "[Pré-voo] OK! Janela aberta, iniciando coleta...\n",
      "[Resume] Checkpoint: 108,140 já coletados | Pendentes: 1,675\n",
      "[Dispatch] Total batches: 34 | max_workers=4 | in_flight=12 | rps=1\n",
      "[Dispatch] Primeiros 12 futures submetidos.\n",
      "[Progresso] 1/34 lotes | enviados: 50 | recebidos: 50 | hit-rate: 100.00%\n",
      "(108140, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>followers_total</th>\n",
       "      <th>popularity</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2wIVse2owClT7go1WT98tk</td>\n",
       "      <td>2718135</td>\n",
       "      <td>73</td>\n",
       "      <td>[hip hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26dSoYclwsYLMAKD3tpOr4</td>\n",
       "      <td>17545559</td>\n",
       "      <td>84</td>\n",
       "      <td>[pop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6vWDO969PvNqNYHIOW5v0m</td>\n",
       "      <td>41008233</td>\n",
       "      <td>88</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31TPClRtHm23RisEBtV3X7</td>\n",
       "      <td>15690525</td>\n",
       "      <td>85</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5EvFsr3kj42KNv97ZEnqij</td>\n",
       "      <td>2462659</td>\n",
       "      <td>76</td>\n",
       "      <td>[reggae]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  followers_total  popularity     genres\n",
       "0  2wIVse2owClT7go1WT98tk          2718135          73  [hip hop]\n",
       "1  26dSoYclwsYLMAKD3tpOr4         17545559          84      [pop]\n",
       "2  6vWDO969PvNqNYHIOW5v0m         41008233          88         []\n",
       "3  31TPClRtHm23RisEBtV3X7         15690525          85         []\n",
       "4  5EvFsr3kj42KNv97ZEnqij          2462659          76   [reggae]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Credenciais Spotify (no Jupyter, SEM aspas nas %env)\n",
    "CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "\n",
    "# Garante a coluna de IDs de artistas\n",
    "if \"artist_spotify_id\" not in df_tracks.columns:\n",
    "    df_tracks = df_tracks.copy()\n",
    "    df_tracks[\"artist_spotify_id\"] = df_tracks[\"artist_uri\"].apply(extract_artist_id)\n",
    "\n",
    "artist_ids_unique = df_tracks[\"artist_spotify_id\"].dropna().unique().tolist()\n",
    "print(f\"Artistas únicos no dataset: {len(artist_ids_unique):,}\")\n",
    "\n",
    "# Pré-voo: evita começar se a janela estiver fechada\n",
    "ready, wait_s = spotify_window_ready_artists(CLIENT_ID, CLIENT_SECRET)\n",
    "if not ready:\n",
    "    from datetime import datetime, timedelta\n",
    "    eta = datetime.now() + timedelta(seconds=wait_s)\n",
    "    print(f\"[Pré-voo] Ainda em rate limit. Retry-After ~{wait_s}s. Tente de novo por volta de: {eta:%Y-%m-%d %H:%M:%S}\")\n",
    "else:\n",
    "    print(\"[Pré-voo] OK! Janela aberta, iniciando coleta...\")\n",
    "\n",
    "    dim_artists = fetch_dim_artists_parallel(\n",
    "        artist_ids=artist_ids_unique,\n",
    "        client_id=CLIENT_ID,\n",
    "        client_secret=CLIENT_SECRET,\n",
    "        batch_size=50,                 # Spotify: máx 50 p/ /v1/artists\n",
    "        max_workers=4,                 # comece baixo; suba se não houver 429\n",
    "        in_flight_multiplier=3,\n",
    "        rate_limit_per_sec=1,        # 1 req a cada 2s\n",
    "        timeout=12,\n",
    "        progress_every=100,\n",
    "        checkpoint_every=200,\n",
    "        checkpoint_path=\"dim_artists_checkpoint.parquet\",\n",
    "        max_retry_after_seconds=120.0, # aborta se Retry-After > 2 min (salvando checkpoint)\n",
    "    )\n",
    "\n",
    "    print(dim_artists.shape)\n",
    "    display(dim_artists.head())\n",
    "    dim_artists.to_parquet(\"TCC/data/raw/dim_artists.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa777c7f-28c2-464f-b466-c6b4dbc43fb5",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c002fc76-8431-4de3-8825-8863befee336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Preparar os dataframes de lookup (dimensions)\n",
    "def prepare_lookup_dataframes(df_features: pd.DataFrame, dim_artists: pd.DataFrame, dim_albums: pd.DataFrame) -> tuple:\n",
    "    \n",
    "    # df_features\n",
    "    df_features_prepared = df_features.rename(columns={\n",
    "        \"id\": \"reccobeats_id\", \n",
    "        \"href\": \"spotify_href\"\n",
    "    })\n",
    "    \n",
    "    # Converter tipos para eficiência\n",
    "    feature_cols_float = [\n",
    "        \"acousticness\", \"danceability\", \"energy\", \"instrumentalness\",\n",
    "        \"liveness\", \"loudness\", \"speechiness\", \"tempo\", \"valence\"\n",
    "    ]\n",
    "    feature_cols_int = [\"key\", \"mode\"]\n",
    "    \n",
    "    for col in feature_cols_float:\n",
    "        if col in df_features_prepared.columns:\n",
    "            df_features_prepared[col] = pd.to_numeric(df_features_prepared[col], errors=\"coerce\").astype(\"float32\")\n",
    "    \n",
    "    for col in feature_cols_int:\n",
    "        if col in df_features_prepared.columns:\n",
    "            df_features_prepared[col] = pd.to_numeric(df_features_prepared[col], errors=\"coerce\").astype(\"Int8\")\n",
    "    \n",
    "    # Criar índice para merge rápido\n",
    "    df_features_indexed = df_features_prepared.set_index(\"spotify_id\")\n",
    "    \n",
    "    # dim_artists\n",
    "    dim_artists_prepared = dim_artists.rename(columns={\n",
    "        \"id\": \"artist_spotify_id\",\n",
    "        \"followers_total\": \"artist_followers\",\n",
    "        \"popularity\": \"artist_popularity\",\n",
    "        \"genres\": \"artist_genres\"\n",
    "    })\n",
    "    dim_artists_indexed = dim_artists_prepared.set_index(\"artist_spotify_id\")\n",
    "    \n",
    "    # dim_albums  \n",
    "    dim_albums_prepared = dim_albums.rename(columns={\n",
    "        \"id\": \"album_spotify_id\",\n",
    "        \"popularity\": \"album_popularity\"\n",
    "    })\n",
    "    dim_albums_indexed = dim_albums_prepared.set_index(\"album_spotify_id\")\n",
    "    \n",
    "    return df_features_indexed, dim_artists_indexed, dim_albums_indexed\n",
    "\n",
    "\n",
    "# 2) Função principal de merge em chunks\n",
    "\n",
    "def extract_spotify_id(uri: str, pattern): # extrair Spotify IDs\n",
    "    if isinstance(uri, str):\n",
    "        m = pattern.match(uri.strip())\n",
    "        if m: return m.group(1)\n",
    "    return None\n",
    "\n",
    "def merge_dataframes_chunked(\n",
    "    df_tracks: pd.DataFrame,\n",
    "    df_features: pd.DataFrame, \n",
    "    dim_artists: pd.DataFrame,\n",
    "    dim_albums: pd.DataFrame,\n",
    "    chunk_size: int = 100000,\n",
    "    output_path: str = \"df_tracks_complete_enriched.parquet\"\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # Preparar df_tracks com IDs extraídos\n",
    "    df_tracks_prepared = df_tracks.copy()\n",
    "    df_tracks_prepared[\"track_spotify_id\"] = df_tracks_prepared[\"track_uri\"].apply(\n",
    "        lambda x: extract_spotify_id(x, _SPOTIFY_URI_RE)\n",
    "    )\n",
    "    df_tracks_prepared[\"artist_spotify_id\"] = df_tracks_prepared[\"artist_uri\"].apply(\n",
    "        lambda x: extract_spotify_id(x, _SPOTIFY_ARTIST_URI_RE)\n",
    "    )\n",
    "    df_tracks_prepared[\"album_spotify_id\"] = df_tracks_prepared[\"album_uri\"].apply(\n",
    "        lambda x: extract_spotify_id(x, _SPOTIFY_ALBUM_URI_RE)\n",
    "    )\n",
    "    \n",
    "    # Preparar dataframes de lookup com índices\n",
    "    df_features_idx, dim_artists_idx, dim_albums_idx = prepare_lookup_dataframes(\n",
    "        df_features, dim_artists, dim_albums\n",
    "    )\n",
    "    \n",
    "    # Definir colunas que serão adicionadas em cada merge\n",
    "    features_cols = [\"reccobeats_id\", \"spotify_href\", \"acousticness\", \"danceability\", \n",
    "                    \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \n",
    "                    \"speechiness\", \"tempo\", \"valence\", \"key\", \"mode\"]\n",
    "    \n",
    "    artists_cols = [\"artist_followers\", \"artist_popularity\", \"artist_genres\"]\n",
    "    \n",
    "    albums_cols = [\"release_date\", \"release_date_precision\", \"album_type\", \n",
    "                  \"total_tracks\", \"label\", \"album_popularity\"]\n",
    "    \n",
    "    total_chunks = (len(df_tracks_prepared) + chunk_size - 1) // chunk_size\n",
    "    chunks_processed = 0\n",
    "    all_chunks = []\n",
    "    \n",
    "    print(f\"Iniciando merge em {total_chunks} chunks de {chunk_size} linhas...\")\n",
    "    \n",
    "    for chunk_idx in range(0, len(df_tracks_prepared), chunk_size):\n",
    "        # Pegar chunk atual\n",
    "        chunk_end = min(chunk_idx + chunk_size, len(df_tracks_prepared))\n",
    "        chunk = df_tracks_prepared.iloc[chunk_idx:chunk_end].copy()\n",
    "        \n",
    "        # Merge com features (usando join com índice para eficiência)\n",
    "        chunk = chunk.join(\n",
    "            df_features_idx[features_cols],\n",
    "            on=\"track_spotify_id\",\n",
    "            how=\"left\",\n",
    "            rsuffix=\"_features\"\n",
    "        )\n",
    "        \n",
    "        # Merge com artists\n",
    "        chunk = chunk.join(\n",
    "            dim_artists_idx[artists_cols],\n",
    "            on=\"artist_spotify_id\", \n",
    "            how=\"left\",\n",
    "            rsuffix=\"_artist\"\n",
    "        )\n",
    "        \n",
    "        # Merge com albums\n",
    "        chunk = chunk.join(\n",
    "            dim_albums_idx[albums_cols],\n",
    "            on=\"album_spotify_id\",\n",
    "            how=\"left\", \n",
    "            rsuffix=\"_album\"\n",
    "        )\n",
    "        \n",
    "        # Remover colunas duplicadas de sufixos (se houver)\n",
    "        chunk = chunk.loc[:, ~chunk.columns.duplicated()]\n",
    "        \n",
    "        all_chunks.append(chunk)\n",
    "        chunks_processed += 1\n",
    "        \n",
    "        # # Estatísticas do chunk atual\n",
    "        # chunk_with_features = chunk[\"reccobeats_id\"].notna().sum()\n",
    "        # chunk_with_artists = chunk[\"artist_followers\"].notna().sum()\n",
    "        # chunk_with_albums = chunk[\"album_popularity\"].notna().sum()\n",
    "        \n",
    "        # # print(f\"Chunk {chunks_processed}/{total_chunks} processado: \"\n",
    "        # #       f\"Features: {chunk_with_features:,} | \"\n",
    "        # #       f\"Artists: {chunk_with_artists:,} | \"\n",
    "        # #       f\"Albums: {chunk_with_albums:,}\")\n",
    "        \n",
    "        # Salvar checkpoint a cada 10 chunks para evitar perda de dados\n",
    "        if chunks_processed % 10 == 0:\n",
    "            checkpoint_temp = pd.concat(all_chunks, ignore_index=True)\n",
    "            checkpoint_temp.to_parquet(f\"checkpoint_chunk_{chunks_processed}.parquet\", index=False)\n",
    "            \n",
    "            # Limpar chunks da memória após salvar checkpoint\n",
    "            all_chunks = [checkpoint_temp.copy()]\n",
    "            del checkpoint_temp\n",
    "    \n",
    "    # Concatenar todos os chunks finais\n",
    "    print(\"Concatenando chunks finais...\")\n",
    "    df_final = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    # Resultados\n",
    "    total_tracks = len(df_final)\n",
    "    tracks_with_features = df_final[\"reccobeats_id\"].notna().sum()\n",
    "    tracks_with_artist_info = df_final[\"artist_followers\"].notna().sum()\n",
    "    tracks_with_album_info = df_final[\"album_popularity\"].notna().sum()\n",
    "    \n",
    "    print(f\"\\nESTATÍSTICAS FINAIS DO MERGE\")\n",
    "    print(f\"Total de tracks: {total_tracks:,}\")\n",
    "    print(f\"Tracks com audio features: {tracks_with_features:,} ({tracks_with_features/total_tracks*100:.2f}%)\")\n",
    "    print(f\"Tracks com info de artista: {tracks_with_artist_info:,} ({tracks_with_artist_info/total_tracks*100:.2f}%)\")\n",
    "    print(f\"Tracks com info de álbum: {tracks_with_album_info:,} ({tracks_with_album_info/total_tracks*100:.2f}%)\")\n",
    "    \n",
    "    # Salvar resultado final\n",
    "    print(f\"Salvando resultado final em: {output_path}\")\n",
    "    df_final.to_parquet(output_path, index=False)\n",
    "    \n",
    "    # Limpar checkpoints temporários\n",
    "    for i in range(5, chunks_processed + 1, 5):\n",
    "        temp_path = f\"checkpoint_chunk_{i}.parquet\"\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# 3) Versão alternativa com salvamento direto em Parquet (para datasets MUITO grandes)\n",
    "def merge_and_save_directly(\n",
    "    df_tracks: pd.DataFrame,\n",
    "    df_features: pd.DataFrame,\n",
    "    dim_artists: pd.DataFrame, \n",
    "    dim_albums: pd.DataFrame,\n",
    "    chunk_size: int = 50000,\n",
    "    output_path: str = \"df_tracks_complete_enriched.parquet\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Para datasets extremamente grandes: processa e salva diretamente sem manter tudo em memória\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Modo de salvamento direto - processando e salvando incrementalmente...\")\n",
    "    \n",
    "    # Preparar dataframes (mesmo código anterior)\n",
    "    df_tracks_prepared = df_tracks.copy()\n",
    "    df_tracks_prepared[\"track_spotify_id\"] = df_tracks_prepared[\"track_uri\"].apply(\n",
    "        lambda x: extract_spotify_id(x, _SPOTIFY_URI_RE)\n",
    "    )\n",
    "    df_tracks_prepared[\"artist_spotify_id\"] = df_tracks_prepared[\"artist_uri\"].apply(\n",
    "        lambda x: extract_spotify_id(x, _SPOTIFY_ARTIST_URI_RE)\n",
    "    )\n",
    "    df_tracks_prepared[\"album_spotify_id\"] = df_tracks_prepared[\"album_uri\"].apply(\n",
    "        lambda x: extract_spotify_id(x, _SPOTIFY_ALBUM_URI_RE)\n",
    "    )\n",
    "    \n",
    "    df_features_idx, dim_artists_idx, dim_albums_idx = prepare_lookup_dataframes(\n",
    "        df_features, dim_artists, dim_albums\n",
    "    )\n",
    "    \n",
    "    total_chunks = (len(df_tracks_prepared) + chunk_size - 1) // chunk_size\n",
    "    chunks_processed = 0\n",
    "    \n",
    "    # Iniciar arquivo Parquet (primeiro chunk define o schema)\n",
    "    first_chunk = True\n",
    "    \n",
    "    for chunk_idx in range(0, len(df_tracks_prepared), chunk_size):\n",
    "        chunk_end = min(chunk_idx + chunk_size, len(df_tracks_prepared))\n",
    "        chunk = df_tracks_prepared.iloc[chunk_idx:chunk_end].copy()\n",
    "        \n",
    "        # Fazer merges (mesma lógica anterior)\n",
    "        chunk = chunk.join(df_features_idx, on=\"track_spotify_id\", how=\"left\", rsuffix=\"_features\")\n",
    "        chunk = chunk.join(dim_artists_idx, on=\"artist_spotify_id\", how=\"left\", rsuffix=\"_artist\")\n",
    "        chunk = chunk.join(dim_albums_idx, on=\"album_spotify_id\", how=\"left\", rsuffix=\"_album\")\n",
    "        chunk = chunk.loc[:, ~chunk.columns.duplicated()]\n",
    "        \n",
    "        chunks_processed += 1\n",
    "        \n",
    "        # Salvar incrementalmente\n",
    "        if first_chunk:\n",
    "            chunk.to_parquet(output_path, index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk.to_parquet(output_path, index=False, append=True)\n",
    "    \n",
    "    print(f\"Processamento concluído! Arquivo final: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e2e7cc-3b06-48a4-b5de-e3f8eeb1bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Track URI\n",
    "_SPOTIFY_URI_RE = re.compile(r\"^spotify:track:([A-Za-z0-9]{22})$\")\n",
    "\n",
    "# Artist URI\n",
    "_SPOTIFY_ARTIST_URI_RE = re.compile(r\"^spotify:artist:([A-Za-z0-9]{22})$\")\n",
    "\n",
    "# Album URI\n",
    "_SPOTIFY_ALBUM_URI_RE = re.compile(r\"^spotify:album:([A-Za-z0-9]{22})$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fda74d-6269-4246-a8f0-c5d55f327e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_parquet(\"TCC/data/raw/features_checkpoint.parquet\")\n",
    "df_tracks = pd.read_parquet(\"TCC/data/raw/df_tracks_v0.parquet\")\n",
    "dim_artists = pd.read_parquet(\"TCC/data/raw/dim_artists.parquet\")\n",
    "dim_albums = pd.read_parquet(\"TCC/data/raw/dim_albums.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321e86b7-dc32-4279-be01-d68cd243698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando merge em 67 chunks de 100000 linhas...\n",
      "Concatenando chunks finais...\n",
      "\n",
      "ESTATÍSTICAS FINAIS DO MERGE\n",
      "Total de tracks: 6,685,101\n",
      "Tracks com audio features: 3,691,508 (55.22%)\n",
      "Tracks com info de artista: 6,669,050 (99.76%)\n",
      "Tracks com info de álbum: 6,684,922 (100.00%)\n",
      "Salvando resultado final em: TCC/data/raw/df_tracks_complete.parquet\n"
     ]
    }
   ],
   "source": [
    "df_final = merge_dataframes_chunked(\n",
    "    df_tracks=df_tracks,\n",
    "    df_features=df_features,\n",
    "    dim_artists=dim_artists,\n",
    "    dim_albums=dim_albums,\n",
    "    chunk_size=100000,  # Ajuste conforme RAM\n",
    "    output_path=\"TCC/data/raw/df_tracks_complete.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9e2692-4dcf-415c-a635-a52e02b6598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6685101 entries, 0 to 6685100\n",
      "Data columns (total 34 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   track_uri               object \n",
      " 1   track_name              object \n",
      " 2   artist_uri              object \n",
      " 3   artist_name             object \n",
      " 4   album_uri               object \n",
      " 5   album_name              object \n",
      " 6   duration_ms             int64  \n",
      " 7   pos                     int64  \n",
      " 8   pid                     int64  \n",
      " 9   track_spotify_id        object \n",
      " 10  artist_spotify_id       object \n",
      " 11  album_spotify_id        object \n",
      " 12  reccobeats_id           object \n",
      " 13  spotify_href            object \n",
      " 14  acousticness            float32\n",
      " 15  danceability            float32\n",
      " 16  energy                  float32\n",
      " 17  instrumentalness        float32\n",
      " 18  liveness                float32\n",
      " 19  loudness                float32\n",
      " 20  speechiness             float32\n",
      " 21  tempo                   float32\n",
      " 22  valence                 float32\n",
      " 23  key                     Int8   \n",
      " 24  mode                    Int8   \n",
      " 25  artist_followers        Int64  \n",
      " 26  artist_popularity       Int16  \n",
      " 27  artist_genres           object \n",
      " 28  release_date            object \n",
      " 29  release_date_precision  object \n",
      " 30  album_type              object \n",
      " 31  total_tracks            Int16  \n",
      " 32  label                   object \n",
      " 33  album_popularity        Int16  \n",
      "dtypes: Int16(3), Int64(1), Int8(2), float32(9), int64(3), object(16)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3063090-dbcb-4ceb-a02e-6e6c3ad74b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
