{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Raiz do Projeto encontrada: /Users/lucasborges/Downloads/TCC\n",
      "‚öôÔ∏è Configura√ß√£o carregada de: /Users/lucasborges/Downloads/TCC/conf/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARGA DE CONFIGURA√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "def find_project_root(anchor_file=\"conf/config.yaml\"):\n",
    "    \"\"\"\n",
    "    Sobe os diret√≥rios a partir do notebook atual at√© encontrar\n",
    "    a pasta onde 'conf/config.yaml' existe.\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    \n",
    "    # Tenta no diret√≥rio atual e sobe at√© o raiz do sistema\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        potential_config = parent / anchor_file\n",
    "        if potential_config.exists():\n",
    "            return parent\n",
    "            \n",
    "    raise FileNotFoundError(f\"N√£o foi poss√≠vel encontrar a raiz do projeto contendo '{anchor_file}'.\")\n",
    "\n",
    "# 1. Definir BASE_DIR (Raiz do Projeto)\n",
    "try:\n",
    "    BASE_DIR = find_project_root(\"conf/config.yaml\")\n",
    "    print(f\"üìÇ Raiz do Projeto encontrada: {BASE_DIR}\")\n",
    "except FileNotFoundError as e:\n",
    "    # Fallback manual caso a busca autom√°tica falhe (ajuste se necess√°rio)\n",
    "    print(\"Busca autom√°tica falhou. Usando fallback.\")\n",
    "    BASE_DIR = Path(\"/Users/lucasborges/Downloads/TCC\")\n",
    "\n",
    "# 2. Carregar o YAML da pasta conf\n",
    "CONFIG_PATH = BASE_DIR / \"conf/config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ATALHOS E VARI√ÅVEIS GLOBAIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Atalhos dos Dicion√°rios do YAML\n",
    "# P['raw'] vai virar algo como: /Users/.../TCC/data/raw\n",
    "P = {k: BASE_DIR / v for k, v in config['paths'].items()} # P de Paths\n",
    "F = config['files']                                       # F de Files\n",
    "PM = config['params']                                     # PM de Params\n",
    "\n",
    "print(f\"‚öôÔ∏è Configura√ß√£o carregada de: {CONFIG_PATH}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PONTE DE VARI√ÅVEIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminhos de Arquivos (Apontando para o YAML)\n",
    "TRAIN_EMB_PATH       = P['processed'] / F['track_embeddings']\n",
    "NEW_EMB_PATH         = P['processed'] / F['new_track_embeddings']\n",
    "X_TRAIN_PATH         = P['processed'] / F['train_features']\n",
    "X_TEST_PATH          = P['processed'] / F['test_features']\n",
    "\n",
    "# Ajuste conforme onde voc√™ salvou o df_tracks_complete (interim ou processed?)\n",
    "# Se n√£o estiver no YAML, usa o caminho constru√≠do:\n",
    "TRACKS_COMPLETE_PATH = P['interim']   / \"df_tracks_complete_v5.parquet\"\n",
    "\n",
    "# Caminhos de Grafos\n",
    "# Verifica se as chaves existem no yaml, sen√£o usa padr√£o\n",
    "MATCHING_MAP_PATH    = P.get('graphs_coarsened', P['graphs_bipartite']) / F['matching_map']\n",
    "SUPER_EMB_PATH       = P.get('graphs_super', P['graphs_bipartite'])     / F['super_embeddings']\n",
    "\n",
    "# Par√¢metros\n",
    "SEED                 = PM['seed']\n",
    "\n",
    "# Configura√ß√µes Visuais Padr√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeando arquivos...\n",
      "\n",
      "======================================================================\n",
      "MLPb COARSENING - DATA LOADING\n",
      "======================================================================\n",
      "‚úì Matriz B_lcc: 98,726 Playlists x 324,132 M√∫sicas\n",
      "‚úì √çndice Playlists: 98,726 ids carregados.\n",
      "‚úì √çndice M√∫sicas: 324,132 ids carregados.\n",
      "‚úì Features RAW alinhado e validado: (324132, 49)\n",
      "‚úì Features SCALED alinhado e validado: (324132, 49)\n",
      "‚úì Grau M√©dio M√∫sicas: 1.14\n",
      "‚úì M√°x Grau: 412.19632894993583 | M√≠n Grau: 0.06419019640788683\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 0. CARREGAMENTO E CONFIGURA√á√ÉO\n",
    "# =================================================================\n",
    "from scipy.sparse import load_npz, csr_matrix, coo_matrix\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from numba import njit\n",
    "\n",
    "# --- 1. Defini√ß√£o de Caminhos usando Config (P e F) ---\n",
    "# P vem do config.yaml carregado na c√©lula anterior\n",
    "print(\"Mapeando arquivos...\")\n",
    "\n",
    "PATHS = {\n",
    "    # Grafo e √çndices \n",
    "    \"B_lcc\":       P['graphs_bipartite'] / \"B_lcc.npz\",\n",
    "    \"p_index\":     P['graphs_bipartite'] / \"p_index_lcc.parquet\",\n",
    "    \"m_index\":     P['graphs_bipartite'] / \"m_index_lcc.parquet\",\n",
    "    \n",
    "    # Features\n",
    "    \"feat_raw\":    P['interim'] / \"content_ctx_train.parquet\", \n",
    "    \n",
    "    # feat_scaled: caso necess√°rio features normalizadas \n",
    "    \"feat_scaled\": P['processed'] / F['train_features'] # Aponta para X_train.parquet\n",
    "}\n",
    "\n",
    "# --- 2. Fun√ß√µes Auxiliares ---\n",
    "def load_to_index(path: Path, name: str) -> pd.Index:\n",
    "    \"\"\"Carrega parquet e garante retorno como pd.Index limpo.\"\"\"\n",
    "    try:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {path}\")\n",
    "            \n",
    "        data = pd.read_parquet(path)\n",
    "        # Pega a primeira coluna independente do nome\n",
    "        series = data.iloc[:, 0] if isinstance(data, pd.DataFrame) else data\n",
    "        print(f\"{name}: {len(series):,} ids carregados.\")\n",
    "        return pd.Index(series)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar {name} ({path.name}): {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def align_features(df: pd.DataFrame, target_index: pd.Index, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Reindexa o DataFrame para bater EXATAMENTE com a ordem do grafo.\"\"\"\n",
    "    # Garante que track_uri √© o √≠ndice\n",
    "    if \"track_uri\" in df.columns:\n",
    "        df = df.set_index(\"track_uri\")\n",
    "    \n",
    "    # Verifica integridade\n",
    "    missing = target_index[~target_index.isin(df.index)]\n",
    "    if len(missing) > 0:\n",
    "        # Se for muita coisa, erro. Se for pouco, aviso (depende da toler√¢ncia do seu TCC)\n",
    "        raise ValueError(f\"ERRO CR√çTICO: Faltam features em '{name}' para {len(missing)} m√∫sicas! (Ex: {missing[:3].values})\")\n",
    "\n",
    "    # Reindexa√ß√£o estrita (for√ßa a ordem do grafo)\n",
    "    df_aligned = df.reindex(target_index)\n",
    "    print(f\"‚úì {name} alinhado e validado: {df_aligned.shape}\")\n",
    "    return df_aligned\n",
    "\n",
    "# --- 3. Execu√ß√£o do Carregamento ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MLPb COARSENING - DATA LOADING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# A. Grafo\n",
    "try:\n",
    "    B_lcc = load_npz(PATHS[\"B_lcc\"]).tocsr()\n",
    "    n_playlists, n_tracks = B_lcc.shape\n",
    "    print(f\"Matriz B_lcc: {n_playlists:,} Playlists x {n_tracks:,} M√∫sicas\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Grafo n√£o encontrado em {PATHS['B_lcc']}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# B. √çndices\n",
    "p_index_lcc = load_to_index(PATHS[\"p_index\"], \"√çndice Playlists\")\n",
    "m_index_lcc_tracks = load_to_index(PATHS[\"m_index\"], \"√çndice M√∫sicas\")\n",
    "\n",
    "# Valida√ß√£o de Sanidade\n",
    "assert len(p_index_lcc) == n_playlists, \"Erro: √çndice de playlists n√£o bate com a matriz!\"\n",
    "assert len(m_index_lcc_tracks) == n_tracks, \"Erro: √çndice de m√∫sicas n√£o bate com a matriz!\"\n",
    "\n",
    "# C. Features\n",
    "# Raw (para c√°lculo de cosseno no coarsening de conte√∫do)\n",
    "df_raw_loaded = pd.read_parquet(PATHS[\"feat_raw\"])\n",
    "df_tracks_features_raw = align_features(df_raw_loaded, m_index_lcc_tracks, \"Features RAW\")\n",
    "\n",
    "# Scaled (caso precise para confer√™ncia ou m√©todo alternativo)\n",
    "df_scaled_loaded = pd.read_parquet(PATHS[\"feat_scaled\"])\n",
    "df_tracks_features_scaled = align_features(df_scaled_loaded, m_index_lcc_tracks, \"Features SCALED\")\n",
    "\n",
    "# D. Grau \n",
    "track_degrees = np.asarray(B_lcc.sum(axis=0)).ravel()\n",
    "print(f\"Grau M√©dio M√∫sicas: {track_degrees.mean():.2f}\")\n",
    "print(f\"M√°x Grau: {track_degrees.max()} | M√≠n Grau: {track_degrees.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f4d620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Calculando pesos (sigma) ===\n",
      "Peso Total: 258998.33 | M√©dio: 0.7991\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 1. PESO DO N√ì (SIGMA)\n",
    "# =================================================================\n",
    "\n",
    "def prepare_node_weights_vectorized(df_features_raw: pd.DataFrame, track_index: pd.Index):\n",
    "    print(\"\\n=== Calculando pesos (sigma) ===\")\n",
    "    if \"artist_followers\" not in df_features_raw.columns:\n",
    "         raise ValueError(\"DataFrame deve conter coluna 'artist_followers'\")\n",
    "\n",
    "    series_raw = df_features_raw[\"artist_followers\"]\n",
    "    series_aligned = series_raw.reindex(track_index)\n",
    "    \n",
    "    # Imputa√ß√£o\n",
    "    if series_aligned.isna().sum() > 0:\n",
    "        series_aligned = series_aligned.fillna(series_aligned.mean())\n",
    "\n",
    "    # Log-Scale Normalizado\n",
    "    denom = np.log1p(series_aligned.mean()) \n",
    "    weights_array = np.log1p(series_aligned.values) / denom\n",
    "    weights_array = weights_array.astype(np.float64)\n",
    "\n",
    "    print(f\"Peso Total: {weights_array.sum():.2f} | M√©dio: {weights_array.mean():.4f}\")\n",
    "    return weights_array\n",
    "\n",
    "# Execu√ß√£o\n",
    "sigma_weights_array = prepare_node_weights_vectorized(df_tracks_features_raw, m_index_lcc_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4a3eb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configura√ß√£o: Zeta=20,000, UpperBound=0.5, DegreeThresh=2\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 2. PAR√ÇMETROS\n",
    "# =================================================================\n",
    "\n",
    "ZETA_TARGET = 20_000     # Alvo dos super-n√≥s\n",
    "UPPER_BOUND = 0.5        # Margem de toler√¢ncia de peso\n",
    "T_MAX = 15               # M√°x itera√ß√µes\n",
    "TAU_FRAC = 0.05          # Toler√¢ncia de converg√™ncia\n",
    "DEGREE_THRESHOLD = 2     # Divis√£o Core vs Tail\n",
    "SEED_PRIORITY = \"degree\" \n",
    "\n",
    "print(f\"Configura√ß√£o: Zeta={ZETA_TARGET:,}, UpperBound={UPPER_BOUND}, DegreeThresh={DEGREE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6861c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pr√©-c√°lculo de Vizinhos ===\n",
      "üìâ Amostrando 886 hubs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 324132/324132 [00:00<00:00, 611157.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando Similaridade...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 325/325 [00:12<00:00, 25.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matriz Vizinhos: 23,576,404 arestas\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 3. VIZINHOS 2-HOP (VETORIZADO)\n",
    "# =================================================================\n",
    "\n",
    "def compute_neighbors_vectorized(\n",
    "    B_sparse: sp.csr_matrix,\n",
    "    track_indices=None, # None = Todos\n",
    "    top_k: int = 100,\n",
    "    min_weight: float = 0.0,\n",
    "    batch_size: int = 1000,\n",
    "    playlist_sample_size: int = 500\n",
    ") -> sp.csr_matrix:\n",
    "    print(f\"\\n=== Pr√©-c√°lculo de Vizinhos ===\")\n",
    "    n_playlists, n_tracks = B_sparse.shape\n",
    "    \n",
    "    # Amostragem de Hubs\n",
    "    B_processed = B_sparse.tocsc()\n",
    "    if playlist_sample_size > 0:\n",
    "        rng = np.random.default_rng(42)\n",
    "        degrees = np.diff(B_processed.indptr)\n",
    "        hubs = np.where(degrees > playlist_sample_size)[0]\n",
    "        if len(hubs) > 0:\n",
    "            print(f\"üìâ Amostrando {len(hubs)} hubs...\")\n",
    "            new_data, new_indices, new_indptr = [], [], [0]\n",
    "            for t in tqdm(range(n_tracks), desc=\"Sampling\"):\n",
    "                start, end = B_processed.indptr[t], B_processed.indptr[t+1]\n",
    "                indices = B_processed.indices[start:end]\n",
    "                if len(indices) > playlist_sample_size:\n",
    "                    chosen = rng.choice(indices, size=playlist_sample_size, replace=False)\n",
    "                    chosen.sort()\n",
    "                    new_indices.extend(chosen)\n",
    "                    new_data.extend([1]*len(chosen))\n",
    "                    new_indptr.append(new_indptr[-1] + len(chosen))\n",
    "                else:\n",
    "                    new_indices.extend(indices)\n",
    "                    new_data.extend(B_processed.data[start:end])\n",
    "                    new_indptr.append(new_indptr[-1] + (end-start))\n",
    "            B_processed = sp.csc_matrix((new_data, new_indices, new_indptr), shape=B_processed.shape)\n",
    "\n",
    "    # Similaridade (Cosseno via Dot Product)\n",
    "    Bt = B_processed.transpose().tocsr()\n",
    "    Bt_norm = normalize(Bt, norm='l2', axis=1)\n",
    "    \n",
    "    target_indices = np.arange(n_tracks) if track_indices is None else track_indices\n",
    "    if not np.issubdtype(target_indices.dtype, np.integer):\n",
    "         target_indices = np.arange(n_tracks) # Fallback se vier strings\n",
    "\n",
    "    final_rows, final_cols, final_data = [], [], []\n",
    "\n",
    "    print(f\"Calculando Similaridade...\")\n",
    "    for i in tqdm(range(0, len(target_indices), batch_size)):\n",
    "        batch_idx = target_indices[i : i + batch_size]\n",
    "        sim_block = Bt_norm[batch_idx, :].dot(Bt_norm.T)\n",
    "        \n",
    "        # Zera diagonal\n",
    "        for local_r, global_c in enumerate(batch_idx):\n",
    "            sim_block[local_r, global_c] = 0.0\n",
    "            \n",
    "        if min_weight > 0:\n",
    "            sim_block.data[sim_block.data < min_weight] = 0\n",
    "            sim_block.eliminate_zeros()\n",
    "\n",
    "        # Top-K manual (r√°pido p/ esparsas)\n",
    "        for r in range(sim_block.shape[0]):\n",
    "            start, end = sim_block.indptr[r], sim_block.indptr[r+1]\n",
    "            if start == end: continue\n",
    "            \n",
    "            idx = sim_block.indices[start:end]\n",
    "            dat = sim_block.data[start:end]\n",
    "            \n",
    "            if len(dat) > top_k:\n",
    "                top_args = np.argpartition(dat, -top_k)[-top_k:]\n",
    "                idx, dat = idx[top_args], dat[top_args]\n",
    "            \n",
    "            final_rows.extend([batch_idx[r]] * len(idx))\n",
    "            final_cols.extend(idx)\n",
    "            final_data.extend(dat)\n",
    "\n",
    "    S_csr = sp.csr_matrix((final_data, (final_rows, final_cols)), shape=(n_tracks, n_tracks))\n",
    "    print(f\"‚úì Matriz Vizinhos: {S_csr.nnz:,} arestas\")\n",
    "    return S_csr\n",
    "\n",
    "# Execu√ß√£o (sem argumento track_indices para evitar erro)\n",
    "S_wcn_matrix = compute_neighbors_vectorized(B_lcc, top_k=100, playlist_sample_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b94b20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 4. MLPb\n",
    "# =================================================================\n",
    "\n",
    "@njit\n",
    "def _run_label_propagation_fast(\n",
    "    indptr, indices, data, sigma_array, initial_labels, initial_cluster_weights,\n",
    "    node_order, S_max, min_vertices, max_iter, tolerance_abs\n",
    "):\n",
    "    labels = initial_labels.copy()\n",
    "    cluster_weights = initial_cluster_weights.copy()\n",
    "    num_nodes = len(labels)\n",
    "    num_sv_active = np.sum(cluster_weights > 0)\n",
    "\n",
    "    for itr in range(max_iter):\n",
    "        swap_count = 0\n",
    "        for i in range(num_nodes):\n",
    "            u = node_order[i]\n",
    "            old_label = labels[u]\n",
    "            u_weight = sigma_array[u]\n",
    "            \n",
    "            start, end = indptr[u], indptr[u+1]\n",
    "            if start == end: continue\n",
    "            \n",
    "            neighbors = indices[start:end]\n",
    "            scores = data[start:end]\n",
    "            \n",
    "            # Map manual para Numba\n",
    "            unique_lbls = np.empty(len(neighbors), dtype=np.int32)\n",
    "            unique_scores = np.empty(len(neighbors), dtype=np.float32)\n",
    "            count = 0\n",
    "            \n",
    "            for k in range(len(neighbors)):\n",
    "                lbl_v = labels[neighbors[k]]\n",
    "                w_sim = scores[k]\n",
    "                \n",
    "                # Check Capacidade\n",
    "                w_check = cluster_weights[lbl_v]\n",
    "                if lbl_v != old_label: w_check += u_weight\n",
    "                \n",
    "                if w_check <= S_max:\n",
    "                    found = False\n",
    "                    for existing in range(count):\n",
    "                        if unique_lbls[existing] == lbl_v:\n",
    "                            unique_scores[existing] += w_sim\n",
    "                            found = True; break\n",
    "                    if not found:\n",
    "                        unique_lbls[count] = lbl_v\n",
    "                        unique_scores[count] = w_sim\n",
    "                        count += 1\n",
    "            \n",
    "            # Argmax\n",
    "            max_val = -1.0\n",
    "            target = old_label\n",
    "            # Score do label atual para compara√ß√£o\n",
    "            for k in range(count):\n",
    "                if unique_lbls[k] == old_label:\n",
    "                    max_val = unique_scores[k]; break\n",
    "            \n",
    "            for k in range(count):\n",
    "                if unique_scores[k] > max_val:\n",
    "                    max_val = unique_scores[k]\n",
    "                    target = unique_lbls[k]\n",
    "            \n",
    "            if target != old_label:\n",
    "                cluster_weights[old_label] -= u_weight\n",
    "                cluster_weights[target] += u_weight\n",
    "                labels[u] = target\n",
    "                swap_count += 1\n",
    "                if cluster_weights[old_label] <= 0: num_sv_active -= 1\n",
    "                if num_sv_active <= min_vertices: return labels, cluster_weights\n",
    "        \n",
    "        if swap_count <= tolerance_abs: break\n",
    "            \n",
    "    return labels, cluster_weights\n",
    "\n",
    "def mlpb_coarsening_vectorized(\n",
    "    S_wcn_matrix, sigma_array, zeta_target=None, upper_bound=0.2, \n",
    "    max_iter=15, tolerance_frac=0.05, seed_priority=\"degree\"\n",
    "):\n",
    "    print(f\"\\n=== MLPb Coarsening ===\")\n",
    "    n_nodes = S_wcn_matrix.shape[0]\n",
    "    min_vertices = min(zeta_target, n_nodes) if zeta_target else max(1, n_nodes//2)\n",
    "    \n",
    "    # S_max baseado na mediana\n",
    "    sigma_median = np.median(sigma_array)\n",
    "    target_size = n_nodes / float(min_vertices)\n",
    "    S_max = (1.0 + upper_bound) * target_size * sigma_median\n",
    "    \n",
    "    print(f\"Zeta: {min_vertices:,} | S_max: {S_max:.4f}\")\n",
    "    \n",
    "    # Setup\n",
    "    initial_labels = np.arange(n_nodes, dtype=np.int32)\n",
    "    initial_weights = sigma_array.copy().astype(np.float64)\n",
    "    \n",
    "    indices = np.arange(n_nodes)\n",
    "    if seed_priority == \"degree\":\n",
    "        degs = np.array(S_wcn_matrix.sum(axis=1)).ravel()\n",
    "        node_order = np.argsort(-degs).astype(np.int32)\n",
    "    else:\n",
    "        node_order = np.random.permutation(n_nodes).astype(np.int32)\n",
    "        \n",
    "    print(\"Rodando LP...\")\n",
    "    lbls, w = _run_label_propagation_fast(\n",
    "        S_wcn_matrix.indptr, S_wcn_matrix.indices, S_wcn_matrix.data.astype(np.float32),\n",
    "        initial_weights, initial_labels, initial_weights, node_order,\n",
    "        float(S_max), int(min_vertices), max_iter, int(tolerance_frac * n_nodes)\n",
    "    )\n",
    "    return lbls, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb650d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 5. CONTENT-BASED MATCHING (BEST EFFORT)\n",
    "# =================================================================\n",
    "\n",
    "def content_based_matching_vectorized(\n",
    "    low_degree_indices, df_features_aligned, matching_high_degree, \n",
    "    cluster_weights, sigma_array, S_max\n",
    "):\n",
    "    print(f\"\\n=== Content Matching (Best Effort) ===\")\n",
    "    \n",
    "    # 1. Features\n",
    "    cols = [c for c in df_features_aligned.select_dtypes(include=[np.number]).columns \n",
    "            if c not in {'track_uri', 'artist_uri', 'p_index', 'm_index'}]\n",
    "    \n",
    "    if not cols: return {idx: f\"L_new_{i}\" for i, idx in enumerate(low_degree_indices)}\n",
    "    X_all = df_features_aligned[cols].values\n",
    "    \n",
    "    # 2. Centr√≥ides\n",
    "    print(\"  Calculando centr√≥ides...\")\n",
    "    high_idxs = list(matching_high_degree.keys())\n",
    "    high_lbls = list(matching_high_degree.values())\n",
    "    \n",
    "    df_centroids = pd.DataFrame(X_all[high_idxs], columns=cols)\n",
    "    df_centroids['label'] = high_lbls\n",
    "    df_centroids = df_centroids.groupby('label').mean()\n",
    "    \n",
    "    centroids_mtx = df_centroids.values\n",
    "    labels_list = df_centroids.index.tolist()\n",
    "    curr_weights = np.array([cluster_weights.get(l, 0.0) for l in labels_list])\n",
    "    \n",
    "    # 3. Matching\n",
    "    matching_low = {}\n",
    "    forced = 0\n",
    "    new_counter = 0\n",
    "    BATCH = 5000\n",
    "    \n",
    "    print(\"  Atribuindo...\")\n",
    "    for start in tqdm(range(0, len(low_degree_indices), BATCH)):\n",
    "        end = min(start + BATCH, len(low_degree_indices))\n",
    "        batch_idxs = low_degree_indices[start:end]\n",
    "        \n",
    "        batch_feat = X_all[batch_idxs]\n",
    "        batch_w = sigma_array[batch_idxs]\n",
    "        \n",
    "        sims = cosine_similarity(batch_feat, centroids_mtx)\n",
    "        pot_w = batch_w[:, None] + curr_weights[None, :]\n",
    "        valid_mask = pot_w <= S_max\n",
    "        \n",
    "        for i in range(len(batch_idxs)):\n",
    "            node_idx = batch_idxs[i]\n",
    "            node_w = batch_w[i]\n",
    "            \n",
    "            # Tenta v√°lido\n",
    "            valid_sims = np.where(valid_mask[i], sims[i], -np.inf)\n",
    "            best_idx = np.argmax(valid_sims)\n",
    "            \n",
    "            if valid_sims[best_idx] == -np.inf:\n",
    "                # FALLBACK: For√ßa no mais similar\n",
    "                best_idx = np.argmax(sims[i])\n",
    "                forced += 1\n",
    "            \n",
    "            lbl = labels_list[best_idx]\n",
    "            matching_low[node_idx] = lbl\n",
    "            curr_weights[best_idx] += node_w # Atualiza peso\n",
    "            \n",
    "    print(f\"‚úì Matching Final. For√ßados: {forced:,}\")\n",
    "    return matching_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7f92e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ORCHESTRATION\n",
      "======================================================================\n",
      "Core: 21,905 | ‚ùÑÔ∏è Tail: 302,227\n",
      "\n",
      "=== MLPb Coarsening ===\n",
      "Zeta: 20,000 | S_max: 1.6314\n",
      "Rodando LP...\n",
      "\n",
      "=== Content Matching (Best Effort) ===\n",
      "  Calculando centr√≥ides...\n",
      "  Atribuindo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61/61 [00:38<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matching Final. For√ßados: 252,404\n",
      "Total M√∫sicas: 324,132 -> Super-N√≥s: 20,047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 6. PIPELINE PRINCIPAL\n",
    "# =================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ORCHESTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Split\n",
    "high_degree_mask = track_degrees >= DEGREE_THRESHOLD\n",
    "global_high_idx = np.where(high_degree_mask)[0]\n",
    "global_low_idx = np.where(~high_degree_mask)[0]\n",
    "\n",
    "print(f\"Core: {len(global_high_idx):,} | Tail: {len(global_low_idx):,}\")\n",
    "\n",
    "# 2. MLPb no Core\n",
    "S_core = S_wcn_matrix[global_high_idx, :][:, global_high_idx]\n",
    "sigma_core = sigma_weights_array[global_high_idx]\n",
    "\n",
    "local_lbls, local_w = mlpb_coarsening_vectorized(\n",
    "    S_core, sigma_core, ZETA_TARGET, UPPER_BOUND, T_MAX, TAU_FRAC, SEED_PRIORITY\n",
    ")\n",
    "\n",
    "# Mapeamento Core\n",
    "matching_high = {}\n",
    "weights_dict = {}\n",
    "for i, l in enumerate(local_lbls):\n",
    "    gid = global_high_idx[i]\n",
    "    lbl_str = f\"SV_{l}\"\n",
    "    matching_high[gid] = lbl_str\n",
    "    weights_dict[lbl_str] = float(local_w[l])\n",
    "\n",
    "# S_max recalculado para consist√™ncia\n",
    "_target_sz = len(global_high_idx) / float(min(ZETA_TARGET, len(global_high_idx)))\n",
    "S_MAX_ACTUAL = (1.0 + UPPER_BOUND) * _target_sz * np.median(sigma_core)\n",
    "\n",
    "# 3. Matching na Tail\n",
    "matching_low = content_based_matching_vectorized(\n",
    "    global_low_idx, df_tracks_features_scaled, matching_high, \n",
    "    weights_dict, sigma_weights_array, S_MAX_ACTUAL\n",
    ")\n",
    "\n",
    "# 4. Merge\n",
    "final_matching = {**matching_high, **matching_low}\n",
    "print(f\"Total M√∫sicas: {len(final_matching):,} -> Super-N√≥s: {len(set(final_matching.values())):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42605912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Contra√ß√£o ===\n",
      "Original: (98726, 324132) -> Reduzido: (98726, 20047)\n",
      "Compress√£o Arestas: 1.0x\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 7. CONTRA√á√ÉO (MATRICIAL)\n",
    "# =================================================================\n",
    "\n",
    "def contract_graph_vectorized(B_sparse, matching_dict):\n",
    "    print(f\"\\n=== Contra√ß√£o ===\")\n",
    "    n_pl, n_tr = B_sparse.shape\n",
    "    \n",
    "    unique_lbls = sorted(list(set(matching_dict.values())))\n",
    "    lbl_map = {l: i for i, l in enumerate(unique_lbls)}\n",
    "    n_super = len(unique_lbls)\n",
    "    \n",
    "    # Matriz de Proje√ß√£o P (Tracks -> Super)\n",
    "    rows, cols = [], []\n",
    "    for t, l in matching_dict.items():\n",
    "        if t < n_tr:\n",
    "            rows.append(t)\n",
    "            cols.append(lbl_map[l])\n",
    "            \n",
    "    P = sp.csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(n_tr, n_super))\n",
    "    \n",
    "    # Contra√ß√£o: B_red = B @ P\n",
    "    B_red = B_sparse.dot(P)\n",
    "    \n",
    "    print(f\"Original: {B_sparse.shape} -> Reduzido: {B_red.shape}\")\n",
    "    print(f\"Compress√£o Arestas: {B_sparse.nnz/B_red.nnz:.1f}x\")\n",
    "    \n",
    "    return B_red, pd.Index(unique_lbls, name=\"super_track_id\")\n",
    "\n",
    "B_coarsened, super_m_index_new = contract_graph_vectorized(B_lcc, final_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b45b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== M√©tricas de Qualidade ===\n",
      "\n",
      "--- Taxas de Redu√ß√£o ---\n",
      "M√∫sicas: 324,132 ‚Üí 20,047 (16.2x)\n",
      "Arestas: 2,969,357 ‚Üí 2,861,520 (1.0x)\n",
      "\n",
      "--- Distribui√ß√£o de Super-N√≥s ---\n",
      "Tamanho m√©dio: 16.2 m√∫sicas\n",
      "Tamanho [min, med, max]: [1, 9, 845]\n",
      "Grau m√©dio interno: 3.1\n",
      "Peso m√©dio (Sigma): 12.92\n",
      "Peso [min, med, max]: [1.28, 7.78, 852.16]\n",
      "Coef. Varia√ß√£o Peso (CV): 1.47 (Ideal: baixo)\n",
      "\n",
      "--- Densidade ---\n",
      "Original: 9.279166e-05\n",
      "Coarsened: 1.445825e-03\n",
      "Raz√£o: 15.581x\n",
      "\n",
      "--- Grau das Super-M√∫sicas (Grafo Reduzido) ---\n",
      "M√©dia: 18.5\n",
      "Mediana: 10\n",
      "[Min, Max]: [2.0982708135852413, 717.6245368767895]\n",
      "\n",
      "--- Compress√£o ---\n",
      "Tamanho da matriz original: 32,000,255,832 c√©lulas\n",
      "Tamanho da matriz coarsened: 1,979,160,122 c√©lulas\n",
      "Taxa de compress√£o: 16.2x\n",
      "\n",
      "--- Efici√™ncia ---\n",
      "Redu√ß√£o efetiva por super-n√≥: 1.00x\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 8. M√âTRICAS DE QUALIDADE DO COARSENING\n",
    "# =================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def compute_quality_metrics_vectorized(\n",
    "    B_original: csr_matrix, \n",
    "    B_coarsened: csr_matrix, \n",
    "    matching_dict: dict, \n",
    "    sigma_array: np.ndarray,   # Array numpy (pesos originais)\n",
    "    track_degrees: np.ndarray  # Array numpy (graus originais)\n",
    "):\n",
    "    print(f\"\\n=== M√©tricas de Qualidade ===\")\n",
    "\n",
    "    # 1. Prepara√ß√£o dos Dados (DataFrame para agrega√ß√£o r√°pida)\n",
    "    # ---------------------------------------------------------\n",
    "    tracks = list(matching_dict.keys())\n",
    "    labels = list(matching_dict.values())\n",
    "    \n",
    "    df_metrics = pd.DataFrame({\n",
    "        'original_track_idx': tracks,\n",
    "        'super_label': labels\n",
    "    })\n",
    "    \n",
    "    # Mapeia propriedades originais\n",
    "    df_metrics['weight'] = sigma_array[df_metrics['original_track_idx']]\n",
    "    df_metrics['degree'] = track_degrees[df_metrics['original_track_idx']]\n",
    "    \n",
    "    # 2. Agrega√ß√£o por Super-N√≥ (GroupBy)\n",
    "    # -----------------------------------\n",
    "    # Calcula: Tamanho (count), Peso Total (sum), Grau Interno M√©dio (mean)\n",
    "    super_stats = df_metrics.groupby('super_label').agg(\n",
    "        size=('original_track_idx', 'count'),\n",
    "        total_weight=('weight', 'sum'),\n",
    "        avg_internal_degree=('degree', 'mean')\n",
    "    )\n",
    "    \n",
    "    # Arrays para c√°lculos estat√≠sticos\n",
    "    sizes = super_stats['size'].values\n",
    "    weights = super_stats['total_weight'].values\n",
    "    internal_degrees = super_stats['avg_internal_degree'].values\n",
    "    \n",
    "    # Grau do grafo REDUZIDO (quantas playlists cada super-n√≥ conecta)\n",
    "    super_degrees = np.asarray(B_coarsened.sum(axis=0)).ravel()\n",
    "\n",
    "    # 3. C√°lculos Gerais\n",
    "    # ------------------\n",
    "    orig_tracks = B_original.shape[1]\n",
    "    coar_tracks = B_coarsened.shape[1]\n",
    "    red_tracks = orig_tracks / coar_tracks if coar_tracks > 0 else 0\n",
    "\n",
    "    orig_edges = B_original.nnz\n",
    "    coar_edges = B_coarsened.nnz\n",
    "    red_edges = orig_edges / coar_edges if coar_edges > 0 else 0\n",
    "\n",
    "    density_orig = orig_edges / (B_original.shape[0] * orig_tracks)\n",
    "    density_coar = coar_edges / (B_coarsened.shape[0] * coar_tracks)\n",
    "    \n",
    "    # Tamanho te√≥rico da matriz (Linhas * Colunas)\n",
    "    orig_size = B_original.shape[0] * orig_tracks\n",
    "    coar_size = B_coarsened.shape[0] * coar_tracks\n",
    "    compression_ratio = orig_size / coar_size if coar_size > 0 else 0\n",
    "    \n",
    "    effective_reduction = red_tracks / np.mean(sizes)\n",
    "\n",
    "    # 4. Impress√£o dos Resultados (Formato original)\n",
    "    # ----------------------------------------------\n",
    "    print(f\"\\n--- Taxas de Redu√ß√£o ---\")\n",
    "    print(f\"M√∫sicas: {orig_tracks:,} ‚Üí {coar_tracks:,} ({red_tracks:.1f}x)\")\n",
    "    print(f\"Arestas: {orig_edges:,} ‚Üí {coar_edges:,} ({red_edges:.1f}x)\")\n",
    "\n",
    "    print(f\"\\n--- Distribui√ß√£o de Super-N√≥s ---\")\n",
    "    print(f\"Tamanho m√©dio: {np.mean(sizes):.1f} m√∫sicas\")\n",
    "    print(f\"Tamanho [min, med, max]: [{np.min(sizes)}, {int(np.median(sizes))}, {np.max(sizes)}]\")\n",
    "    print(f\"Grau m√©dio interno: {np.mean(internal_degrees):.1f}\")\n",
    "    print(f\"Peso m√©dio (Sigma): {np.mean(weights):.2f}\")\n",
    "    print(f\"Peso [min, med, max]: [{np.min(weights):.2f}, {np.median(weights):.2f}, {np.max(weights):.2f}]\")\n",
    "    \n",
    "    # Coeficiente de Varia√ß√£o (para checar balanceamento)\n",
    "    cv = (np.std(weights) / np.mean(weights)) if np.mean(weights) > 0 else 0\n",
    "    print(f\"Coef. Varia√ß√£o Peso (CV): {cv:.2f} (Ideal: baixo)\")\n",
    "\n",
    "    print(f\"\\n--- Densidade ---\")\n",
    "    print(f\"Original: {density_orig:.6e}\")\n",
    "    print(f\"Coarsened: {density_coar:.6e}\")\n",
    "    print(f\"Raz√£o: {density_coar/density_orig:.3f}x\")\n",
    "\n",
    "    print(f\"\\n--- Grau das Super-M√∫sicas (Grafo Reduzido) ---\")\n",
    "    print(f\"M√©dia: {super_degrees.mean():.1f}\")\n",
    "    print(f\"Mediana: {np.median(super_degrees):.0f}\")\n",
    "    print(f\"[Min, Max]: [{super_degrees.min()}, {super_degrees.max()}]\")\n",
    "\n",
    "    print(f\"\\n--- Compress√£o ---\")\n",
    "    print(f\"Tamanho da matriz original: {orig_size:,} c√©lulas\")\n",
    "    print(f\"Tamanho da matriz coarsened: {coar_size:,} c√©lulas\")\n",
    "    print(f\"Taxa de compress√£o: {compression_ratio:.1f}x\")\n",
    "\n",
    "    print(f\"\\n--- Efici√™ncia ---\")\n",
    "    print(f\"Redu√ß√£o efetiva por super-n√≥: {effective_reduction:.2f}x\")\n",
    "\n",
    "    # Retorna dicion√°rio completo\n",
    "    return {\n",
    "        \"reduction_tracks\": red_tracks,\n",
    "        \"reduction_edges\": red_edges,\n",
    "        \"super_node_count\": len(sizes),\n",
    "        \"avg_super_node_size\": np.mean(sizes),\n",
    "        \"median_super_node_size\": np.median(sizes),\n",
    "        \"max_super_node_size\": np.max(sizes),\n",
    "        \"avg_super_node_weight\": np.mean(weights),\n",
    "        \"std_super_node_weight\": np.std(weights),\n",
    "        \"cv_weight\": cv,\n",
    "        \"density_ratio\": density_coar/density_orig,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"effective_reduction\": effective_reduction,\n",
    "        # Retorna o DF caso queira salvar detalhes depois\n",
    "        \"stats_df\": super_stats\n",
    "    }\n",
    "\n",
    "# Execu√ß√£o\n",
    "metrics = compute_quality_metrics_vectorized(\n",
    "    B_original=B_lcc,\n",
    "    B_coarsened=B_coarsened,\n",
    "    matching_dict=final_matching,\n",
    "    sigma_array=sigma_weights_array,\n",
    "    track_degrees=track_degrees\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24cbc80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Salvando Resultados (Vetorizado) ===\n",
      "üìÅ Diret√≥rio de Sa√≠da: /Users/lucasborges/Downloads/TCC/graphs/bipartite/coarsened\n",
      "‚úì Matriz B_coarsened salva: (98726, 20047)\n",
      "‚úì √çndice super_m_index salvo.\n",
      "  Construindo DataFrame de matching...\n",
      "‚úì Matching Map salvo em 'matching_map.parquet': 324,132 registros.\n",
      "Estat√≠sticas salvas.\n",
      "\n",
      "======================================================================\n",
      "PIPELINE DE COARSENING CONCLU√çDO COM SUCESSO\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 9. SALVAMENTO\n",
    "# =================================================================\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def save_coarsening_results_optimized(\n",
    "    output_dir: Path, # <--- MUDAN√áA 1: Recebe a pasta final, n√£o a base\n",
    "    B_coarsened: sp.csr_matrix,\n",
    "    super_m_index: pd.Index,\n",
    "    final_matching: dict,\n",
    "    m_index_original: pd.Index,\n",
    "    metrics_data: dict,\n",
    "    original_nnz: int,\n",
    "    original_shape: tuple\n",
    "):\n",
    "    \"\"\"\n",
    "    Salva os resultados do Coarsening na pasta indicada pelo config.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Salvando Resultados (Vetorizado) ===\")\n",
    "    \n",
    "    # 1. Garantir que o diret√≥rio existe\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"üìÅ Diret√≥rio de Sa√≠da: {output_dir}\")\n",
    "\n",
    "    # 2. Salvar Matriz Esparsa (NPZ)\n",
    "    sp.save_npz(output_dir / \"B_coarsened.npz\", B_coarsened)\n",
    "    print(f\"‚úì Matriz B_coarsened salva: {B_coarsened.shape}\")\n",
    "\n",
    "    # 3. Salvar √çndice das Super-M√∫sicas\n",
    "    super_m_index.name = \"super_track_id\"\n",
    "    super_m_index.to_frame().to_parquet(output_dir / \"super_m_index.parquet\")\n",
    "    print(f\"‚úì √çndice super_m_index salvo.\")\n",
    "\n",
    "    # 4. Salvar Mapa de Matching\n",
    "    print(\"  Construindo DataFrame de matching...\")\n",
    "    \n",
    "    # final_matching √© {int_idx: 'label'}\n",
    "    df_matching = pd.DataFrame.from_dict(\n",
    "        final_matching, \n",
    "        orient='index', \n",
    "        columns=['super_track_id']\n",
    "    )\n",
    "    df_matching.index.name = 'original_track_idx'\n",
    "    \n",
    "    # Recuperar URIs originais usando acesso posicional r√°pido\n",
    "    if hasattr(m_index_original, 'values'):\n",
    "        uris_array = m_index_original.values\n",
    "    else:\n",
    "        uris_array = np.array(m_index_original)\n",
    "        \n",
    "    # Indexa√ß√£o segura\n",
    "    df_matching['original_track_uri'] = uris_array[df_matching.index]\n",
    "    \n",
    "    # Reordenar para ficar bonito\n",
    "    df_matching = df_matching[['original_track_uri', 'super_track_id']]\n",
    "    \n",
    "    # Usa o nome do arquivo definido no YAML (F['matching_map']) se dispon√≠vel, sen√£o default\n",
    "    filename_map = F.get('matching_map', \"matching_map.parquet\")\n",
    "    df_matching.to_parquet(output_dir / filename_map)\n",
    "    print(f\"‚úì Matching Map salvo em '{filename_map}': {len(df_matching):,} registros.\")\n",
    "\n",
    "    # 5. Salvar M√©tricas e Stats\n",
    "    metrics_clean = {k: v for k, v in metrics_data.items() if not isinstance(v, pd.DataFrame)}\n",
    "    \n",
    "    stats_full = {\n",
    "        **metrics_clean,\n",
    "        \"original_tracks\": original_shape[1],\n",
    "        \"original_edges\": original_nnz,\n",
    "        \"coarsened_tracks\": B_coarsened.shape[1],\n",
    "        \"coarsened_edges\": B_coarsened.nnz,\n",
    "        \"compression_ratio\": original_shape[1] / B_coarsened.shape[1] if B_coarsened.shape[1] > 0 else 0,\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    pd.DataFrame([stats_full]).to_parquet(output_dir / \"coarsening_stats.parquet\")\n",
    "    print(f\"Estat√≠sticas salvas.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE DE COARSENING CONCLU√çDO COM SUCESSO\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# =================================================================\n",
    "# USO FINAL \n",
    "# =================================================================\n",
    "\n",
    "\n",
    "save_coarsening_results_optimized(\n",
    "    output_dir=P['graphs_coarsened'], \n",
    "    B_coarsened=B_coarsened,\n",
    "    super_m_index=super_m_index_new,\n",
    "    final_matching=final_matching,\n",
    "    m_index_original=m_index_lcc_tracks,\n",
    "    metrics_data=metrics, \n",
    "    original_nnz=B_lcc.nnz,\n",
    "    original_shape=B_lcc.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f5a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
