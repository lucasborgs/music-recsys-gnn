{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfaa41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Raiz do Projeto encontrada: /Users/lucasborges/Downloads/TCC\n",
      "‚öôÔ∏è Configura√ß√£o carregada de: /Users/lucasborges/Downloads/TCC/conf/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARGA DE CONFIGURA√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "def find_project_root(anchor_file=\"conf/config.yaml\"):\n",
    "    \"\"\"\n",
    "    Sobe os diret√≥rios a partir do notebook atual at√© encontrar\n",
    "    a pasta onde 'conf/config.yaml' existe.\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    \n",
    "    # Tenta no diret√≥rio atual e sobe at√© o raiz do sistema\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        potential_config = parent / anchor_file\n",
    "        if potential_config.exists():\n",
    "            return parent\n",
    "            \n",
    "    raise FileNotFoundError(f\"N√£o foi poss√≠vel encontrar a raiz do projeto contendo '{anchor_file}'.\")\n",
    "\n",
    "# 1. Definir BASE_DIR (Raiz do Projeto)\n",
    "try:\n",
    "    BASE_DIR = find_project_root(\"conf/config.yaml\")\n",
    "    print(f\"üìÇ Raiz do Projeto encontrada: {BASE_DIR}\")\n",
    "except FileNotFoundError as e:\n",
    "    # Fallback manual caso a busca autom√°tica falhe (ajuste se necess√°rio)\n",
    "    print(\"Busca autom√°tica falhou. Usando fallback.\")\n",
    "    BASE_DIR = Path(\"/Users/lucasborges/Downloads/TCC\")\n",
    "\n",
    "# 2. Carregar o YAML da pasta conf\n",
    "CONFIG_PATH = BASE_DIR / \"conf/config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ATALHOS E VARI√ÅVEIS GLOBAIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Atalhos dos Dicion√°rios do YAML\n",
    "# P['raw'] vai virar algo como: /Users/.../TCC/data/raw\n",
    "P = {k: BASE_DIR / v for k, v in config['paths'].items()} # P de Paths\n",
    "F = config['files']                                       # F de Files\n",
    "PM = config['params']                                     # PM de Params\n",
    "\n",
    "print(f\"‚öôÔ∏è Configura√ß√£o carregada de: {CONFIG_PATH}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PONTE DE VARI√ÅVEIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminhos de Arquivos (Apontando para o YAML)\n",
    "TRAIN_EMB_PATH       = P['processed'] / F['track_embeddings']\n",
    "NEW_EMB_PATH         = P['processed'] / F['new_track_embeddings']\n",
    "X_TRAIN_PATH         = P['processed'] / F['train_features']\n",
    "X_TEST_PATH          = P['processed'] / F['test_features']\n",
    "\n",
    "# Ajuste conforme onde voc√™ salvou o df_tracks_complete (interim ou processed?)\n",
    "# Se n√£o estiver no YAML, usa o caminho constru√≠do:\n",
    "TRACKS_COMPLETE_PATH = P['interim']   / \"df_tracks_complete_v5.parquet\"\n",
    "\n",
    "# Caminhos de Grafos\n",
    "# Verifica se as chaves existem no yaml, sen√£o usa padr√£o\n",
    "MATCHING_MAP_PATH    = P.get('graphs_coarsened', P['graphs_bipartite']) / F['matching_map']\n",
    "SUPER_EMB_PATH       = P.get('graphs_super', P['graphs_bipartite'])     / F['super_embeddings']\n",
    "\n",
    "# Par√¢metros\n",
    "SEED                 = PM['seed']\n",
    "\n",
    "# Configura√ß√µes Visuais Padr√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8546a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeando arquivos para Infer√™ncia (Cold-Start)...\n",
      "Output definido para: new_track_embeddings_mean.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DIN√ÇMICA (Usando P e F) ---\n",
    "print(\"Mapeando arquivos para Infer√™ncia (Cold-Start)...\")\n",
    "\n",
    "# Inputs\n",
    "X_TEST_PATH       = P['processed'] / F['test_features']\n",
    "X_TRAIN_PATH      = P['processed'] / F['train_features'] # Necess√°rio para calcular centr√≥ides\n",
    "MATCHING_MAP_PATH = P['graphs_coarsened'] / F['matching_map']\n",
    "SUPER_EMB_PATH    = P['graphs_super'] / F['super_embeddings']\n",
    "\n",
    "# Output\n",
    "OUT_NEW_EMB       = P['processed'] / F['new_track_embeddings']\n",
    "\n",
    "# Valida√ß√£o\n",
    "for path in [X_TEST_PATH, MATCHING_MAP_PATH, SUPER_EMB_PATH]:\n",
    "    if not path.exists():\n",
    "        print(f\"Aviso: Arquivo n√£o encontrado: {path.name}\")\n",
    "\n",
    "print(f\"Output definido para: {OUT_NEW_EMB.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7d5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_super_features_light():\n",
    "    \"\"\"\n",
    "    Recalcula as features m√©dias dos super-n√≥s da forma mais leve poss√≠vel\n",
    "    e deleta o X_train da mem√≥ria imediatamente.\n",
    "    \"\"\"\n",
    "    print(\"Recalculando features dos Super-N√≥s (Modo Econ√¥mico)...\")\n",
    "    \n",
    "    # 1. Carregar Map\n",
    "    map_df = pd.read_parquet(MATCHING_MAP_PATH)\n",
    "    col_uri = \"original_track_uri\" if \"original_track_uri\" in map_df.columns else \"track_uri\"\n",
    "    map_df = map_df.rename(columns={col_uri: \"track_uri\"})[[ \"track_uri\", \"super_track_id\" ]]\n",
    "    \n",
    "    # 2. Carregar X_train\n",
    "    # Verifica se existe, sen√£o avisa\n",
    "    if not X_TRAIN_PATH.exists():\n",
    "        raise FileNotFoundError(f\"X_train n√£o encontrado em {X_TRAIN_PATH}\")\n",
    "        \n",
    "    df_train = pd.read_parquet(X_TRAIN_PATH)\n",
    "    if \"track_uri\" not in df_train.columns and \"id\" in df_train.columns:\n",
    "        df_train = df_train.rename(columns={\"id\": \"track_uri\"})\n",
    "        \n",
    "    # 3. Merge\n",
    "    print(f\"   Cruzando {len(df_train):,} faixas com mapa...\")\n",
    "    merged = df_train.merge(map_df, on=\"track_uri\", how=\"inner\")\n",
    "    \n",
    "    # Limpar mem√≥ria imediata\n",
    "    del df_train, map_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Identificar colunas num√©ricas (features de √°udio/meta)\n",
    "    cols = [c for c in merged.columns if c not in [\"track_uri\", \"super_track_id\"] and merged[c].dtype.kind in 'bifc']\n",
    "    \n",
    "    # 4. Groupby e Mean (C√°lculo do Centr√≥ide)\n",
    "    print(\"   Calculando centr√≥ides...\")\n",
    "    super_feats = merged.groupby(\"super_track_id\")[cols].mean().reset_index()\n",
    "    \n",
    "    # Limpar merged\n",
    "    del merged\n",
    "    gc.collect()\n",
    "    \n",
    "    return super_feats, cols\n",
    "\n",
    "def resolve_cold_start_batch(df_test, df_super_feats, feat_cols, batch_size=2000):\n",
    "    \"\"\"\n",
    "    Roda KNN em batches pequenos com limpeza agressiva de mem√≥ria.\n",
    "    \"\"\"\n",
    "    print(f\"Iniciando KNN para {len(df_test):,} m√∫sicas (Batch: {batch_size})...\")\n",
    "    \n",
    "    # Preparar Dados de Refer√™ncia (Super-N√≥s)\n",
    "    X_super = df_super_feats[feat_cols].to_numpy().astype('float32')\n",
    "    super_ids = df_super_feats[\"super_track_id\"].to_numpy()\n",
    "    \n",
    "    # Normalizar para Espa√ßo Euclidiano Padr√£o\n",
    "    scaler = StandardScaler()\n",
    "    X_super_scaled = scaler.fit_transform(X_super)\n",
    "    \n",
    "    # Treinar KNN (Leve - 1 vizinho mais pr√≥ximo)\n",
    "    # n_jobs=1 √© proposital para evitar overhead em loops pequenos\n",
    "    knn = NearestNeighbors(n_neighbors=1, metric='euclidean', n_jobs=1) \n",
    "    knn.fit(X_super_scaled)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Loop em Batches\n",
    "    for i in tqdm(range(0, len(df_test), batch_size)):\n",
    "        # Slice seguro\n",
    "        batch = df_test.iloc[i : i + batch_size].copy()\n",
    "        \n",
    "        # Features do Batch\n",
    "        X_batch = batch[feat_cols].fillna(0).to_numpy().astype('float32')\n",
    "        X_batch_scaled = scaler.transform(X_batch)\n",
    "        \n",
    "        # Busca\n",
    "        _, indices = knn.kneighbors(X_batch_scaled)\n",
    "        \n",
    "        # Atribui√ß√£o do ID do Super-N√≥\n",
    "        matched_super_ids = super_ids[indices.flatten()]\n",
    "        \n",
    "        # Resultado Parcial\n",
    "        res_df = batch[[\"track_uri\"]].copy()\n",
    "        res_df[\"super_track_id\"] = matched_super_ids\n",
    "        results.append(res_df)\n",
    "        \n",
    "        # Limpeza do Loop\n",
    "        del X_batch, X_batch_scaled, indices, batch\n",
    "        \n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf48f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalculando features dos Super-N√≥s (Modo Econ√¥mico)...\n",
      "   Cruzando 327,208 faixas com mapa...\n",
      "   Calculando centr√≥ides...\n",
      "Features de Super-N√≥s prontas: (20047, 50)\n",
      "Carregando Teste e Mapa...\n",
      " Separando casos...\n",
      "   Via Lookup direto: 5,377\n",
      "   Via KNN (Cold-Start): 57,514\n",
      "Iniciando KNN para 57,514 m√∫sicas (Batch: 2000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeamento conclu√≠do. Total: 62,891\n",
      "Anexando Embeddings Finais...\n",
      "Salvando 62,891 linhas em: /Users/lucasborges/Downloads/TCC/data/processed/new_track_embeddings_mean.parquet\n",
      "\n",
      "CONCLU√çDO! Embeddings de teste gerados.\n"
     ]
    }
   ],
   "source": [
    "# --- EXECU√á√ÉO ---\n",
    "\n",
    "# 1. Obter Features dos Centr√≥ides (Super-N√≥s)\n",
    "# Isso reconstr√≥i a \"assinatura m√©dia\" de cada g√™nero/comunidade\n",
    "df_super_feats, feat_cols = get_super_features_light()\n",
    "print(f\"Features de Super-N√≥s prontas: {df_super_feats.shape}\")\n",
    "\n",
    "# 2. Carregar Teste e Mapa\n",
    "print(\"Carregando Teste e Mapa...\")\n",
    "df_test = pd.read_parquet(X_TEST_PATH)\n",
    "if \"track_uri\" not in df_test.columns and \"id\" in df_test.columns: \n",
    "    df_test = df_test.rename(columns={\"id\": \"track_uri\"})\n",
    "\n",
    "map_df = pd.read_parquet(MATCHING_MAP_PATH)\n",
    "col_uri = \"original_track_uri\" if \"original_track_uri\" in map_df.columns else \"track_uri\"\n",
    "map_df = map_df.rename(columns={col_uri: \"track_uri\"})\n",
    "\n",
    "# 3. Separar: Quem tem Lookup (j√° existia no grafo) vs Quem precisa de KNN (Cold-Start Real)\n",
    "print(\" Separando casos...\")\n",
    "# Lookup: M√∫sicas de teste que, por acaso, cairam no grafo (raro em cold-start estrito, mas poss√≠vel)\n",
    "lookup_match = df_test.merge(map_df, on=\"track_uri\", how=\"inner\")[[\"track_uri\", \"super_track_id\"]]\n",
    "known_uris = set(lookup_match[\"track_uri\"])\n",
    "\n",
    "# Orphans: M√∫sicas realmente novas\n",
    "df_orphans = df_test[~df_test[\"track_uri\"].isin(known_uris)].copy()\n",
    "\n",
    "print(f\"   Via Lookup direto: {len(lookup_match):,}\")\n",
    "print(f\"   Via KNN (Cold-Start): {len(df_orphans):,}\")\n",
    "\n",
    "# Liberar mem√≥ria\n",
    "del map_df, df_test\n",
    "gc.collect()\n",
    "\n",
    "# 4. Rodar KNN nas √≥rf√£s\n",
    "if len(df_orphans) > 0:\n",
    "    knn_match = resolve_cold_start_batch(df_orphans, df_super_feats, feat_cols, batch_size=2000)\n",
    "    final_mapping = pd.concat([lookup_match, knn_match], ignore_index=True)\n",
    "else:\n",
    "    final_mapping = lookup_match\n",
    "\n",
    "print(f\"Mapeamento conclu√≠do. Total: {len(final_mapping):,}\")\n",
    "\n",
    "# Limpar mem√≥ria antes do merge final\n",
    "del df_orphans, df_super_feats, lookup_match\n",
    "if 'knn_match' in locals(): del knn_match\n",
    "gc.collect()\n",
    "\n",
    "# 5. Colocar os Embeddings (GraphSAGE)\n",
    "print(\"Anexando Embeddings Finais...\")\n",
    "# Carrega os embeddings aprendidos pela GNN (Notebook 6)\n",
    "df_super_emb = pd.read_parquet(SUPER_EMB_PATH)\n",
    "\n",
    "# Merge: Track Nova -> Super N√≥ (KNN) -> Embedding (GNN)\n",
    "df_final = final_mapping.merge(df_super_emb, on=\"super_track_id\", how=\"inner\")\n",
    "\n",
    "print(f\"Salvando {len(df_final):,} linhas em: {OUT_NEW_EMB}\")\n",
    "df_final.to_parquet(OUT_NEW_EMB, index=False)\n",
    "\n",
    "print(\"\\nCONCLU√çDO! Embeddings de teste gerados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606abb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
