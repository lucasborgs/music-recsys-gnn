{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb78770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Raiz: /Users/lucasborges/Downloads/TCC\n",
      "Mapeando arquivos...\n",
      "‚úÖ Setup conclu√≠do! Vari√°vel PATHS definida.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# C√âLULA 1: SETUP E CONFIGURA√á√ÉO\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Imports matem√°ticos vitais\n",
    "from scipy.sparse import load_npz, save_npz, csr_matrix, vstack\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def find_project_root(anchor_file=\"conf/config.yaml\"):\n",
    "    current_path = Path.cwd()\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        if (parent / anchor_file).exists(): return parent\n",
    "    raise FileNotFoundError(f\"Raiz do projeto n√£o encontrada.\")\n",
    "\n",
    "# 1. Carregar Configura√ß√£o\n",
    "try:\n",
    "    BASE_DIR = find_project_root()\n",
    "    print(f\"üìÇ Raiz: {BASE_DIR}\")\n",
    "except:\n",
    "    BASE_DIR = Path(\"/Users/lucasborges/Downloads/TCC\") # Fallback\n",
    "    print(f\"‚ö†Ô∏è Fallback: {BASE_DIR}\")\n",
    "\n",
    "with open(BASE_DIR / \"conf/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "P = {k: BASE_DIR / v for k, v in config['paths'].items()}\n",
    "\n",
    "# 2. Mapear Arquivos (Define a vari√°vel PATHS)\n",
    "print(\"Mapeando arquivos...\")\n",
    "PATHS = {\n",
    "    \"B_lcc\":       P['graphs_bipartite'] / \"B_lcc.npz\",\n",
    "    \"m_index\":     P['graphs_bipartite'] / \"m_index_lcc.parquet\",\n",
    "    \"B_coarse\":    P['graphs_coarsened'] / \"B_coarsened.npz\",\n",
    "    \"s_index\":     P['graphs_coarsened'] / \"super_m_index.parquet\"\n",
    "}\n",
    "\n",
    "# 3. Mapear Sa√≠das\n",
    "OUT_ORIG_DIR  = P.get('graphs_item', BASE_DIR / \"graphs/item_item\")\n",
    "OUT_SUPER_DIR = P.get('graphs_super', BASE_DIR / \"graphs/super_item_item\")\n",
    "\n",
    "OUT_ORIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_SUPER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup conclu√≠do! Vari√°vel PATHS definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e628109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUN√á√ÉO DE PROJE√á√ÉO ---\n",
    "def project_and_sparsify(\n",
    "    B,\n",
    "    k_neighbors=100,\n",
    "    chunk_size=1000, # Processar em lotes para n√£o estourar RAM\n",
    "    verbose_prefix=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Projeta B (P x M) -> S (M x M) via Cosine Similarity.\n",
    "    2. Mant√©m apenas os Top-K vizinhos por n√≥ (KNN).\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = B.shape # n_cols s√£o os itens (m√∫sicas)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{verbose_prefix} PROJE√á√ÉO (COSINE) + SPARSIFICATION (KNN={k_neighbors})\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Input B: {n_rows:,} playlists √ó {n_cols:,} itens\")\n",
    "    \n",
    "    # 1. Normaliza√ß√£o L2 nas Colunas (Itens)\n",
    "    # Transpomos para X (M x P) para normalizar as linhas (cada linha = 1 m√∫sica)\n",
    "    # Cosine Similarity(A, B) = (A . B) / (||A||*||B||)\n",
    "    # Se normalizarmos vetores para tamanho 1, Cosine = A . B\n",
    "    print(\"Normalizando vetores de itens (L2)...\")\n",
    "    X = B.T.tocsr() # Shape: (Itens, Playlists)\n",
    "    X = normalize(X, norm='l2', axis=1) # Cada m√∫sica agora √© um vetor unit√°rio\n",
    "    \n",
    "    # 2. C√°lculo de Similaridade e Filtragem Top-K em Blocos\n",
    "    # N√£o podemos fazer X.dot(X.T) direto se X for gigante (resultado denso).\n",
    "    # Vamos calcular bloco a bloco.\n",
    "    \n",
    "    print(f\"Calculando similaridade e filtrando Top-{k_neighbors} em blocos de {chunk_size}...\")\n",
    "    \n",
    "    sparse_blocks = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Iterar sobre os itens em peda√ßos (chunks)\n",
    "    for i in range(0, n_cols, chunk_size):\n",
    "        end = min(i + chunk_size, n_cols)\n",
    "        \n",
    "        # A. Pega um lote de m√∫sicas\n",
    "        X_chunk = X[i:end]\n",
    "        \n",
    "        # B. Calcula similaridade deste lote contra TODAS as m√∫sicas\n",
    "        # Resultado: (chunk_size, n_cols)\n",
    "        # Valores entre 0 e 1\n",
    "        sim_chunk = X_chunk.dot(X.T)\n",
    "        \n",
    "        # C. Zerar diagonal (apenas para o bloco onde i==j)\n",
    "        # O offset da diagonal neste bloco depende de 'i'\n",
    "        sim_chunk.setdiag(0) # Scipy sparse matrix cuida do offset automaticamente se for quadrada, mas aqui √© retangular\n",
    "        # Corre√ß√£o manual para diagonal:\n",
    "        # Se o bloco cont√©m a diagonal principal da matriz global\n",
    "        if i < end: \n",
    "             # Zera auto-loops na for√ßa bruta (seguro e r√°pido para chunks)\n",
    "             # Criamos uma m√°scara para (r, c) onde r+i == c\n",
    "             for row_idx in range(sim_chunk.shape[0]):\n",
    "                 global_idx = i + row_idx\n",
    "                 sim_chunk[row_idx, global_idx] = 0.0\n",
    "\n",
    "        # D. Manter apenas Top-K para cada linha do chunk\n",
    "        # Converter para denso temporariamente √© seguro pois chunk_size √© pequeno\n",
    "        sim_dense = sim_chunk.toarray() \n",
    "        \n",
    "        # L√≥gica de sele√ß√£o Top-K usando argpartition (r√°pido)\n",
    "        # Cria uma matriz de zeros\n",
    "        filtered_data = np.zeros_like(sim_dense)\n",
    "        \n",
    "        for row_idx in range(len(sim_dense)):\n",
    "            row = sim_dense[row_idx]\n",
    "            # Se tem mais vizinhos que K, filtra\n",
    "            if np.count_nonzero(row) > k_neighbors:\n",
    "                # √çndices dos K maiores\n",
    "                top_k_idx = np.argpartition(row, -k_neighbors)[-k_neighbors:]\n",
    "                filtered_data[row_idx, top_k_idx] = row[top_k_idx]\n",
    "            else:\n",
    "                filtered_data[row_idx] = row\n",
    "        \n",
    "        # Converter de volta para esparso e guardar\n",
    "        sparse_blocks.append(csr_matrix(filtered_data))\n",
    "        \n",
    "        if (i // chunk_size) % 10 == 0:\n",
    "            print(f\"   Processado {end}/{n_cols} itens... ({time.time()-start_time:.1f}s)\")\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"Empilhando blocos finais...\")\n",
    "    S = vstack(sparse_blocks)\n",
    "    S.eliminate_zeros()\n",
    "    \n",
    "    # Estat√≠sticas Finais\n",
    "    n = S.shape[0]\n",
    "    deg = np.diff(S.indptr)\n",
    "    print(f\"‚úì Conclu√≠do: {n:,} √ó {n:,}\")\n",
    "    print(f\"  NNZ Total: {S.nnz:,}\")\n",
    "    print(f\"  Grau M√©dio: {deg.mean():.2f}\")\n",
    "    print(f\"  Grau Max: {deg.max()} (Deve ser <= {k_neighbors})\")\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a41b8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando matriz B_lcc (playlists √ó m√∫sicas)...\n",
      "\n",
      "======================================================================\n",
      "M√öSICA‚ÄìM√öSICA (KNN) PROJE√á√ÉO (COSINE) + SPARSIFICATION (KNN=100)\n",
      "======================================================================\n",
      "Input B: 98,726 playlists √ó 324,132 itens\n",
      "Normalizando vetores de itens (L2)...\n",
      "Calculando similaridade e filtrando Top-100 em blocos de 1000...\n",
      "   Processado 1000/324132 itens... (6.5s)\n",
      "   Processado 11000/324132 itens... (64.0s)\n",
      "   Processado 21000/324132 itens... (119.7s)\n",
      "   Processado 31000/324132 itens... (175.0s)\n",
      "   Processado 41000/324132 itens... (229.3s)\n",
      "   Processado 51000/324132 itens... (283.6s)\n",
      "   Processado 61000/324132 itens... (339.1s)\n",
      "   Processado 71000/324132 itens... (395.1s)\n",
      "   Processado 81000/324132 itens... (449.4s)\n",
      "   Processado 91000/324132 itens... (503.1s)\n",
      "   Processado 101000/324132 itens... (558.8s)\n",
      "   Processado 111000/324132 itens... (613.7s)\n",
      "   Processado 121000/324132 itens... (669.5s)\n",
      "   Processado 131000/324132 itens... (722.3s)\n",
      "   Processado 141000/324132 itens... (774.4s)\n",
      "   Processado 151000/324132 itens... (826.7s)\n",
      "   Processado 161000/324132 itens... (879.4s)\n",
      "   Processado 171000/324132 itens... (931.7s)\n",
      "   Processado 181000/324132 itens... (983.6s)\n",
      "   Processado 191000/324132 itens... (1035.1s)\n",
      "   Processado 201000/324132 itens... (1084.6s)\n",
      "   Processado 211000/324132 itens... (1135.9s)\n",
      "   Processado 221000/324132 itens... (1185.9s)\n",
      "   Processado 231000/324132 itens... (1237.4s)\n",
      "   Processado 241000/324132 itens... (1287.2s)\n",
      "   Processado 251000/324132 itens... (1338.0s)\n",
      "   Processado 261000/324132 itens... (1389.2s)\n",
      "   Processado 271000/324132 itens... (2337.5s)\n",
      "   Processado 281000/324132 itens... (2383.6s)\n",
      "   Processado 291000/324132 itens... (4233.4s)\n",
      "   Processado 301000/324132 itens... (4536.2s)\n",
      "   Processado 311000/324132 itens... (4588.2s)\n",
      "   Processado 321000/324132 itens... (4640.4s)\n",
      "Empilhando blocos finais...\n",
      "‚úì Conclu√≠do: 324,132 √ó 324,132\n",
      "  NNZ Total: 23,684,266\n",
      "  Grau M√©dio: 73.07\n",
      "  Grau Max: 100 (Deve ser <= 100)\n",
      "\n",
      "‚úì Proje√ß√£o m√∫sica‚Äìm√∫sica salva em: /Users/lucasborges/Downloads/TCC/graphs/item_item\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCarregando matriz B_lcc (playlists √ó m√∫sicas)...\")\n",
    "\n",
    "# USANDO A NOVA CONFIGURA√á√ÉO DIN√ÇMICA\n",
    "B_lcc = load_npz(PATHS[\"B_lcc\"])\n",
    "\n",
    "m_index_orig = pd.read_parquet(PATHS[\"m_index\"]).squeeze()\n",
    "\n",
    "# Ajuste de robustez para index\n",
    "if isinstance(m_index_orig, pd.DataFrame): m_index_orig = m_index_orig.iloc[:, 0]\n",
    "if isinstance(m_index_orig, pd.Series): m_index_orig = pd.Index(m_index_orig)\n",
    "\n",
    "# --- PROJE√á√ÉO ---\n",
    "# K=100 √© um bom balan√ßo para m√∫sicas. \n",
    "# O chunk_size=1000 garante que n√£o trave sua mem√≥ria.\n",
    "S_tracks = project_and_sparsify(\n",
    "    B=B_lcc,\n",
    "    k_neighbors=100, \n",
    "    chunk_size=1000, \n",
    "    verbose_prefix=\"M√öSICA‚ÄìM√öSICA (KNN)\"\n",
    ")\n",
    "\n",
    "# --- SALVAMENTO ---\n",
    "# Usamos OUT_ORIG_DIR (definido na C√©lula 1)\n",
    "save_npz(OUT_ORIG_DIR / \"A_tracks_adjacency.npz\", S_tracks)\n",
    "m_index_orig.to_frame(name=\"track_uri\").to_parquet(OUT_ORIG_DIR / \"m_index_lcc_tracks.parquet\")\n",
    "\n",
    "print(f\"\\n‚úì Proje√ß√£o m√∫sica‚Äìm√∫sica salva em: {OUT_ORIG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5382cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando matriz B_coarsened (playlists √ó super-m√∫sicas)...\n",
      "\n",
      "======================================================================\n",
      "SUPER-M√öSICA (KNN) PROJE√á√ÉO (COSINE) + SPARSIFICATION (KNN=50)\n",
      "======================================================================\n",
      "Input B: 98,726 playlists √ó 20,047 itens\n",
      "Normalizando vetores de itens (L2)...\n",
      "Calculando similaridade e filtrando Top-50 em blocos de 2000...\n",
      "   Processado 2000/20047 itens... (0.7s)\n",
      "   Processado 20047/20047 itens... (8.6s)\n",
      "Empilhando blocos finais...\n",
      "‚úì Conclu√≠do: 20,047 √ó 20,047\n",
      "  NNZ Total: 1,002,350\n",
      "  Grau M√©dio: 50.00\n",
      "  Grau Max: 50 (Deve ser <= 50)\n",
      "\n",
      "‚úì Proje√ß√£o super-m√∫sica salva em: /Users/lucasborges/Downloads/TCC/graphs/super_item_item\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCarregando matriz B_coarsened (playlists √ó super-m√∫sicas)...\")\n",
    "\n",
    "# USANDO A NOVA CONFIGURA√á√ÉO DIN√ÇMICA\n",
    "B_coarsened = load_npz(PATHS[\"B_coarse\"])\n",
    "\n",
    "super_m_index = pd.read_parquet(PATHS[\"s_index\"]).squeeze()\n",
    "\n",
    "if isinstance(super_m_index, pd.DataFrame): super_m_index = super_m_index.iloc[:, 0]\n",
    "if isinstance(super_m_index, pd.Series): super_m_index = pd.Index(super_m_index)\n",
    "\n",
    "# --- PROJE√á√ÉO ---\n",
    "# K=50 para super-n√≥s (grafo menor, estrutura mais r√≠gida)\n",
    "S_super = project_and_sparsify(\n",
    "    B=B_coarsened,\n",
    "    k_neighbors=50,\n",
    "    chunk_size=2000, \n",
    "    verbose_prefix=\"SUPER-M√öSICA (KNN)\"\n",
    ")\n",
    "\n",
    "# --- SALVAMENTO ---\n",
    "# Usamos OUT_SUPER_DIR (definido na C√©lula 1)\n",
    "save_npz(OUT_SUPER_DIR / \"A_super_tracks_adjacency.npz\", S_super)\n",
    "super_m_index.to_frame(name=\"super_track_id\").to_parquet(OUT_SUPER_DIR / \"super_m_index.parquet\")\n",
    "\n",
    "print(f\"\\n‚úì Proje√ß√£o super-m√∫sica salva em: {OUT_SUPER_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c54a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- M√∫sica‚Äìm√∫sica (original) ---\n",
      "Shape: 324,132 √ó 324,132\n",
      "nnz: 23,684,266\n",
      "Densidade: 2.25432284e-04\n",
      "Grau m√©dio: 28.62\n",
      "Grau mediano: 21.65\n",
      "Grau [min, max]: [0.02, 100.00]\n",
      "\n",
      "--- Super-m√∫sica‚Äìsuper-m√∫sica (coarsenado) ---\n",
      "Shape: 20,047 √ó 20,047\n",
      "nnz: 1,002,350\n",
      "Densidade: 2.49413877e-03\n",
      "Grau m√©dio: 4.96\n",
      "Grau mediano: 4.38\n",
      "Grau [min, max]: [1.27, 23.61]\n"
     ]
    }
   ],
   "source": [
    "def summarize_matrix(name, S):\n",
    "    n = S.shape[0]\n",
    "    nnz = S.nnz\n",
    "    density = nnz / (n * n)\n",
    "    deg = np.asarray(S.sum(axis=1)).ravel()\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Shape: {n:,} √ó {n:,}\")\n",
    "    print(f\"nnz: {nnz:,}\")\n",
    "    print(f\"Densidade: {density:.8e}\")\n",
    "    print(f\"Grau m√©dio: {deg.mean():.2f}\")\n",
    "    print(f\"Grau mediano: {np.median(deg):.2f}\")\n",
    "    print(f\"Grau [min, max]: [{deg.min():.2f}, {deg.max():.2f}]\")\n",
    "\n",
    "summarize_matrix(\"M√∫sica‚Äìm√∫sica (original)\", S_tracks)\n",
    "summarize_matrix(\"Super-m√∫sica‚Äìsuper-m√∫sica (coarsenado)\", S_super)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
