{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9031d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Raiz do Projeto encontrada: /Users/lucasborges/Downloads/TCC\n",
      "‚öôÔ∏è Configura√ß√£o carregada de: /Users/lucasborges/Downloads/TCC/conf/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARGA DE CONFIGURA√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "def find_project_root(anchor_file=\"conf/config.yaml\"):\n",
    "    \"\"\"\n",
    "    Sobe os diret√≥rios a partir do notebook atual at√© encontrar\n",
    "    a pasta onde 'conf/config.yaml' existe.\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    \n",
    "    # Tenta no diret√≥rio atual e sobe at√© o raiz do sistema\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        potential_config = parent / anchor_file\n",
    "        if potential_config.exists():\n",
    "            return parent\n",
    "            \n",
    "    raise FileNotFoundError(f\"N√£o foi poss√≠vel encontrar a raiz do projeto contendo '{anchor_file}'.\")\n",
    "\n",
    "# 1. Definir BASE_DIR (Raiz do Projeto)\n",
    "try:\n",
    "    BASE_DIR = find_project_root(\"conf/config.yaml\")\n",
    "    print(f\"üìÇ Raiz do Projeto encontrada: {BASE_DIR}\")\n",
    "except FileNotFoundError as e:\n",
    "    # Fallback manual caso a busca autom√°tica falhe (ajuste se necess√°rio)\n",
    "    print(\"Busca autom√°tica falhou. Usando fallback.\")\n",
    "    BASE_DIR = Path(\"/Users/lucasborges/Downloads/TCC\")\n",
    "\n",
    "# 2. Carregar o YAML da pasta conf\n",
    "CONFIG_PATH = BASE_DIR / \"conf/config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ATALHOS E VARI√ÅVEIS GLOBAIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Atalhos dos Dicion√°rios do YAML\n",
    "# P['raw'] vai virar algo como: /Users/.../TCC/data/raw\n",
    "P = {k: BASE_DIR / v for k, v in config['paths'].items()} # P de Paths\n",
    "F = config['files']                                       # F de Files\n",
    "PM = config['params']                                     # PM de Params\n",
    "\n",
    "print(f\"‚öôÔ∏è Configura√ß√£o carregada de: {CONFIG_PATH}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PONTE DE VARI√ÅVEIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminhos de Arquivos (Apontando para o YAML)\n",
    "TRAIN_EMB_PATH       = P['processed'] / F['track_embeddings']\n",
    "NEW_EMB_PATH         = P['processed'] / F['new_track_embeddings']\n",
    "X_TRAIN_PATH         = P['processed'] / F['train_features']\n",
    "X_TEST_PATH          = P['processed'] / F['test_features']\n",
    "\n",
    "# Se n√£o estiver no YAML, usa o caminho constru√≠do:\n",
    "TRACKS_COMPLETE_PATH = P['interim']   / \"df_tracks_complete_v5.parquet\"\n",
    "\n",
    "# Caminhos de Grafos\n",
    "# Verifica se as chaves existem no yaml, sen√£o usa padr√£o\n",
    "MATCHING_MAP_PATH    = P.get('graphs_coarsened', P['graphs_bipartite']) / F['matching_map']\n",
    "SUPER_EMB_PATH       = P.get('graphs_super', P['graphs_bipartite'])     / F['super_embeddings']\n",
    "\n",
    "# Par√¢metros\n",
    "SEED                 = PM['seed']\n",
    "\n",
    "# Configura√ß√µes Visuais Padr√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0bac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeando arquivos para o GraphSAGE...\n",
      "Todos os arquivos de entrada encontrados.\n",
      "   Output Super: super_embeddings_mean.parquet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F_nn  # <--- CORRE√á√ÉO: Mudamos de F para F_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from scipy.sparse import load_npz\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DE CAMINHOS DIN√ÇMICOS ---\n",
    "# Usamos P (Paths) e F (Files) da C√©lula Mestra\n",
    "# Certifique-se de que P e F est√£o definidos (rode a C√©lula Mestra antes se necess√°rio)\n",
    "\n",
    "print(\"Mapeando arquivos para o GraphSAGE...\")\n",
    "\n",
    "# 1. INPUTS\n",
    "# Features de Treino\n",
    "X_TRAIN_PATH = P['processed'] / F['train_features']  # Agora F √© o dicion√°rio correto\n",
    "\n",
    "# Grafo de Super-M√∫sicas\n",
    "SUPER_ADJ_PATH   = P['graphs_super'] / \"A_super_tracks_adjacency.npz\"\n",
    "SUPER_INDEX_PATH = P['graphs_super'] / \"super_m_index.parquet\"\n",
    "\n",
    "# Mapa de Matching\n",
    "MATCHING_MAP_PATH = P['graphs_coarsened'] / F['matching_map']\n",
    "\n",
    "# 2. OUTPUTS\n",
    "OUT_SUPER_EMB = P['graphs_super'] / F['super_embeddings']\n",
    "OUT_TRACK_EMB = P['processed'] / F['track_embeddings']\n",
    "\n",
    "# 3. VALIDA√á√ÉO\n",
    "missing = []\n",
    "for p in [X_TRAIN_PATH, SUPER_ADJ_PATH, SUPER_INDEX_PATH, MATCHING_MAP_PATH]:\n",
    "    if not p.exists():\n",
    "        missing.append(p.name)\n",
    "\n",
    "if missing:\n",
    "    print(f\"ERRO: Arquivos de entrada n√£o encontrados: {missing}\")\n",
    "else:\n",
    "    print(\"Todos os arquivos de entrada encontrados.\")\n",
    "    print(f\"   Output Super: {OUT_SUPER_EMB.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951fea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Carregando Estrutura do Grafo ---\n",
      "   N√≥s: 20,641 | Arestas: 1,032,050\n",
      "\n",
      "--- 2. Carregando Features e Mapeando ---\n",
      "   Agregando 51 features (M√©dia)...\n",
      "\n",
      "Data Object Criado: Data(x=[20641, 51], edge_index=[2, 1032050])\n"
     ]
    }
   ],
   "source": [
    "def build_supergraph_data(features_path, super_adj_path, super_index_path, matching_map_path):\n",
    "    print(\"--- 1. Carregando Estrutura do Grafo ---\")\n",
    "    super_m_index = pd.read_parquet(super_index_path).squeeze()\n",
    "    if isinstance(super_m_index, pd.DataFrame): super_m_index = super_m_index.iloc[:, 0]\n",
    "    \n",
    "    S_super = load_npz(super_adj_path).tocsr()\n",
    "    print(f\"   N√≥s: {S_super.shape[0]:,} | Arestas: {S_super.nnz:,}\")\n",
    "\n",
    "    print(\"\\n--- 2. Carregando Features e Mapeando ---\")\n",
    "    df_feats_track = pd.read_parquet(features_path)\n",
    "    matching_df = pd.read_parquet(matching_map_path)\n",
    "    \n",
    "    # Padronizar nomes\n",
    "    col_uri = \"original_track_uri\" if \"original_track_uri\" in matching_df.columns else \"track_uri\"\n",
    "    df_map = matching_df[[col_uri, \"super_track_id\"]].rename(columns={col_uri: \"track_uri\"})\n",
    "    if \"track_uri\" not in df_feats_track.columns and \"id\" in df_feats_track.columns:\n",
    "        df_feats_track = df_feats_track.rename(columns={\"id\": \"track_uri\"})\n",
    "\n",
    "    # Merge (Inner Join)\n",
    "    df_merged = df_feats_track.merge(df_map, on=\"track_uri\", how=\"inner\")\n",
    "    \n",
    "    # Filtrar apenas colunas num√©ricas para agrega√ß√£o\n",
    "    exclude = [\"track_uri\", \"super_track_id\", \"pid\", \"release_year\"]\n",
    "    feature_cols = [c for c in df_merged.columns if c not in exclude and df_merged[c].dtype.kind in 'bifc']\n",
    "    \n",
    "    print(f\"   Agregando {len(feature_cols)} features (M√©dia)...\")\n",
    "    df_super_feats = df_merged.groupby(\"super_track_id\")[feature_cols].mean().reset_index()\n",
    "\n",
    "    # Alinhar com a ordem da matriz de adjac√™ncia\n",
    "    df_super_index = pd.DataFrame({\"super_track_id\": super_m_index.values})\n",
    "    df_super_feats = df_super_index.merge(df_super_feats, on=\"super_track_id\", how=\"left\").fillna(0.0)\n",
    "    \n",
    "    # Criar Tensores\n",
    "    x = torch.from_numpy(df_super_feats[feature_cols].to_numpy().astype(\"float32\"))\n",
    "    \n",
    "    S_coo = S_super.tocoo()\n",
    "    edge_index = torch.stack([torch.from_numpy(S_coo.row), torch.from_numpy(S_coo.col)], dim=0).long()\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    print(\"\\nData Object Criado:\", data)\n",
    "    \n",
    "    return data, df_super_feats\n",
    "\n",
    "# Execu√ß√£o\n",
    "data_super, df_super_feats = build_supergraph_data(X_TRAIN_PATH, SUPER_ADJ_PATH, SUPER_INDEX_PATH, MATCHING_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9888840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Modelo SuperGraphSAGE\n",
    "\n",
    "\n",
    "class SuperGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, out_channels=64, \n",
    "                 num_layers=2, dropout=0.2, aggr='mean'):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # Camada 1\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr=aggr))\n",
    "        \n",
    "        # Camadas Intermedi√°rias\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggr))\n",
    "        \n",
    "        # Camada Sa√≠da\n",
    "        if num_layers > 1:\n",
    "            self.convs.append(SAGEConv(hidden_channels, out_channels, aggr=aggr))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # 1. Processamento das camadas iniciais e intermedi√°rias\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F_nn.relu(x)\n",
    "            x = F_nn.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        \n",
    "        # 2. √öltima camada de convolu√ß√£o\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        \n",
    "        # 3. NORMALIZA√á√ÉO L2 DOS VETORES\n",
    "        # Garante que todos os vetores tenham norma = 1\n",
    "        # Transformando o produto escalar em Similaridade de Cosseno\n",
    "        x = F_nn.normalize(x, p=2, dim=-1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c6b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√µes auxiliares para treinamento e avalia√ß√£o\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Trava todas as sementes aleat√≥rias para resultados reproduz√≠veis.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"Seed fixada em {seed}\")\n",
    "\n",
    "\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    \"\"\"\n",
    "    Bayesian Personalized Ranking Loss.\n",
    "    Otimiza para que pos_scores > neg_scores (ordem relativa).\n",
    "    \"\"\"\n",
    "    return -F_nn.logsigmoid(pos_scores - neg_scores).mean()\n",
    "\n",
    "\n",
    "def compute_validation_metrics(model, val_data, device, num_nodes, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas de valida√ß√£o de forma eficiente.\n",
    "    Retorna apenas Ranking Accuracy e Average Margin (sem MRR que causa crash).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_val = model(val_data.x, val_data.edge_index)\n",
    "        \n",
    "        # Arestas positivas (com sampling)\n",
    "        val_pos_all = val_data.edge_label_index[:, val_data.edge_label == 1]\n",
    "        \n",
    "        if val_pos_all.size(1) > sample_size:\n",
    "            sample_indices = torch.randperm(val_pos_all.size(1))[:sample_size]\n",
    "            val_pos = val_pos_all[:, sample_indices]\n",
    "        else:\n",
    "            val_pos = val_pos_all\n",
    "        \n",
    "        # Arestas negativas (random)\n",
    "        val_neg = torch.randint(0, num_nodes, val_pos.shape, device=device)\n",
    "        \n",
    "        # Scores\n",
    "        pos_scores = (z_val[val_pos[0]] * z_val[val_pos[1]]).sum(dim=-1)\n",
    "        neg_scores = (z_val[val_neg[0]] * z_val[val_neg[1]]).sum(dim=-1)\n",
    "        \n",
    "        # M√©tricas simples (sem criar matrizes grandes)\n",
    "        ranking_acc = (pos_scores > neg_scores).float().mean().item()\n",
    "        avg_margin = (pos_scores - neg_scores).mean().item()\n",
    "    \n",
    "    return {'ranking_acc': ranking_acc, 'avg_margin': avg_margin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06033daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o principal de treinamento (simplificada)\n",
    "\n",
    "def train_model(data, hidden_channels=64, epochs=200, lr=0.001, \n",
    "                validation_interval=50, seed=42, save_logs=True):\n",
    "    \"\"\"\n",
    "    Treina o modelo SuperGraphSAGE com BPR Loss.\n",
    "    Vers√£o simplificada sem hard negatives para evitar crash de mem√≥ria.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TREINAMENTO GraphSAGE\")\n",
    "    print(f\"Epochs: {epochs} | Hidden: {hidden_channels} | LR: {lr}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Split Treino/Valida√ß√£o\n",
    "    transform = RandomLinkSplit(\n",
    "        num_val=0.1, num_test=0.0, \n",
    "        is_undirected=False, add_negative_train_samples=False\n",
    "    )\n",
    "    train_data, val_data, _ = transform(data)\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\\n\")\n",
    "    \n",
    "    model = SuperGraphSAGE(\n",
    "        in_channels=data.num_features, \n",
    "        hidden_channels=hidden_channels, \n",
    "        out_channels=hidden_channels\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_data = train_data.to(device)\n",
    "    val_data = val_data.to(device)\n",
    "    \n",
    "    training_logs = []\n",
    "    \n",
    "    # Loop de Treinamento\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z = model(train_data.x, train_data.edge_index)\n",
    "        \n",
    "        # Arestas positivas e negativas (random)\n",
    "        pos_edge = train_data.edge_label_index[:, train_data.edge_label == 1]\n",
    "        neg_edge = torch.randint(0, data.num_nodes, pos_edge.shape, device=device)\n",
    "        \n",
    "        # Scores e Loss\n",
    "        pos_scores = (z[pos_edge[0]] * z[pos_edge[1]]).sum(dim=-1)\n",
    "        neg_scores = (z[neg_edge[0]] * z[neg_edge[1]]).sum(dim=-1)\n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Valida√ß√£o peri√≥dica\n",
    "        if epoch % validation_interval == 0:\n",
    "            metrics = compute_validation_metrics(model, val_data, device, data.num_nodes)\n",
    "            training_logs.append({\n",
    "                'epoch': epoch, 'loss': loss.item(),\n",
    "                'ranking_acc': metrics['ranking_acc'], 'avg_margin': metrics['avg_margin']\n",
    "            })\n",
    "            print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | \"\n",
    "                  f\"Rank Acc: {metrics['ranking_acc']:.4f} | Margin: {metrics['avg_margin']:.4f}\")\n",
    "    \n",
    "    # Salvar logs\n",
    "    if save_logs and training_logs:\n",
    "        logs_df = pd.DataFrame(training_logs)\n",
    "        logs_path = P['metrics'] / \"training_logs.parquet\"\n",
    "        logs_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        logs_df.to_parquet(logs_path, index=False)\n",
    "        print(f\"\\nLogs salvos em: {logs_path}\")\n",
    "    \n",
    "    # Embeddings Finais\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GERANDO EMBEDDINGS FINAIS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_z = model(data.x.to(device), data.edge_index.to(device)).cpu().numpy()\n",
    "    \n",
    "    print(f\"Shape: {final_z.shape}\")\n",
    "    print(f\"Norma m√©dia: {np.linalg.norm(final_z, axis=1).mean():.4f} (esperado ~1.0)\")\n",
    "    \n",
    "    return final_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2591949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed fixada em 42\n",
      "\n",
      "============================================================\n",
      "TREINAMENTO GraphSAGE\n",
      "Epochs: 200 | Hidden: 64 | LR: 0.001\n",
      "============================================================\n",
      "\n",
      "Device: cpu\n",
      "\n",
      "Epoch 050 | Loss: 0.4512 | Rank Acc: 0.8676 | Margin: 0.7402\n",
      "Epoch 100 | Loss: 0.4242 | Rank Acc: 0.9035 | Margin: 0.7754\n",
      "Epoch 150 | Loss: 0.4109 | Rank Acc: 0.9204 | Margin: 0.7804\n",
      "Epoch 200 | Loss: 0.4028 | Rank Acc: 0.9327 | Margin: 0.7996\n",
      "\n",
      "Logs salvos em: /Users/lucasborges/Downloads/TCC/reports/metrics/training_logs.parquet\n",
      "\n",
      "============================================================\n",
      "GERANDO EMBEDDINGS FINAIS\n",
      "============================================================\n",
      "\n",
      "Shape: (20641, 64)\n",
      "Norma m√©dia: 1.0000 (esperado ~1.0)\n"
     ]
    }
   ],
   "source": [
    "# Treinamento com BPR Loss (vers√£o simplificada)\n",
    "\n",
    "z_final_bpr = train_model(\n",
    "    data_super,\n",
    "    hidden_channels=64,\n",
    "    epochs=200,\n",
    "    lr=0.001,\n",
    "    validation_interval=50,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66058686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Salvando Embeddings das Super-M√∫sicas ---\n",
      "   Salvando em: /Users/lucasborges/Downloads/TCC/graphs/super_item_item/super_embeddings_mean.parquet\n",
      "\n",
      "--- 2. Projetando Embeddings para o Treino ---\n",
      "   321,160 faixas mapeadas.\n",
      "   Salvando em: /Users/lucasborges/Downloads/TCC/data/processed/track_embeddings_mean.parquet\n",
      "\n",
      "==================================================\n",
      "PIPELINE GRAPHSAGE CONCLUIDO!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Liberar mem√≥ria do treinamento\n",
    "del data_super\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- 1. Salvando Embeddings das Super-M√∫sicas ---\")\n",
    "emb_cols = [f\"emb_mean_{i:03d}\" for i in range(z_final_bpr.shape[1])]\n",
    "\n",
    "df_super_emb = df_super_feats[[\"super_track_id\"]].copy()\n",
    "df_super_emb[emb_cols] = z_final_bpr\n",
    "\n",
    "print(f\"   Salvando em: {OUT_SUPER_EMB}\")\n",
    "df_super_emb.to_parquet(OUT_SUPER_EMB, index=False)\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. Projetando Embeddings para o Treino ---\")\n",
    "\n",
    "# Carregar Mapa (Track -> Super)\n",
    "matching_df = pd.read_parquet(MATCHING_MAP_PATH)\n",
    "col_uri = \"original_track_uri\" if \"original_track_uri\" in matching_df.columns else \"track_uri\"\n",
    "matching_df = matching_df.rename(columns={col_uri: \"track_uri\"})\n",
    "\n",
    "# Carregar IDs de Treino\n",
    "df_train = pd.read_parquet(X_TRAIN_PATH)\n",
    "if \"track_uri\" not in df_train.columns and \"id\" in df_train.columns:\n",
    "    df_train.rename(columns={\"id\": \"track_uri\"}, inplace=True)\n",
    "\n",
    "# Merge: Track -> Super -> Embedding\n",
    "df_train_emb = df_train[[\"track_uri\"]].merge(matching_df, on=\"track_uri\", how=\"inner\")\n",
    "df_train_emb = df_train_emb.merge(df_super_emb, on=\"super_track_id\", how=\"inner\")\n",
    "\n",
    "if len(df_train_emb) == 0:\n",
    "    print(\"ERRO: DataFrame vazio ap√≥s merge!\")\n",
    "else:\n",
    "    print(f\"   {len(df_train_emb):,} faixas mapeadas.\")\n",
    "    print(f\"   Salvando em: {OUT_TRACK_EMB}\")\n",
    "    df_train_emb.to_parquet(OUT_TRACK_EMB, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE GRAPHSAGE CONCLUIDO!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
