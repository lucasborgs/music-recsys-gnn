{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9031d23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Raiz do Projeto encontrada: /Users/lucasborges/Downloads/TCC\n",
      "‚öôÔ∏è Configura√ß√£o carregada de: /Users/lucasborges/Downloads/TCC/conf/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARGA DE CONFIGURA√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "def find_project_root(anchor_file=\"conf/config.yaml\"):\n",
    "    \"\"\"\n",
    "    Sobe os diret√≥rios a partir do notebook atual at√© encontrar\n",
    "    a pasta onde 'conf/config.yaml' existe.\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    \n",
    "    # Tenta no diret√≥rio atual e sobe at√© o raiz do sistema\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        potential_config = parent / anchor_file\n",
    "        if potential_config.exists():\n",
    "            return parent\n",
    "            \n",
    "    raise FileNotFoundError(f\"N√£o foi poss√≠vel encontrar a raiz do projeto contendo '{anchor_file}'.\")\n",
    "\n",
    "# 1. Definir BASE_DIR (Raiz do Projeto)\n",
    "try:\n",
    "    BASE_DIR = find_project_root(\"conf/config.yaml\")\n",
    "    print(f\"üìÇ Raiz do Projeto encontrada: {BASE_DIR}\")\n",
    "except FileNotFoundError as e:\n",
    "    # Fallback manual caso a busca autom√°tica falhe (ajuste se necess√°rio)\n",
    "    print(\"Busca autom√°tica falhou. Usando fallback.\")\n",
    "    BASE_DIR = Path(\"/Users/lucasborges/Downloads/TCC\")\n",
    "\n",
    "# 2. Carregar o YAML da pasta conf\n",
    "CONFIG_PATH = BASE_DIR / \"conf/config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ATALHOS E VARI√ÅVEIS GLOBAIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Atalhos dos Dicion√°rios do YAML\n",
    "# P['raw'] vai virar algo como: /Users/.../TCC/data/raw\n",
    "P = {k: BASE_DIR / v for k, v in config['paths'].items()} # P de Paths\n",
    "F = config['files']                                       # F de Files\n",
    "PM = config['params']                                     # PM de Params\n",
    "\n",
    "print(f\"‚öôÔ∏è Configura√ß√£o carregada de: {CONFIG_PATH}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PONTE DE VARI√ÅVEIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminhos de Arquivos (Apontando para o YAML)\n",
    "TRAIN_EMB_PATH       = P['processed'] / F['track_embeddings']\n",
    "NEW_EMB_PATH         = P['processed'] / F['new_track_embeddings']\n",
    "X_TRAIN_PATH         = P['processed'] / F['train_features']\n",
    "X_TEST_PATH          = P['processed'] / F['test_features']\n",
    "\n",
    "# Ajuste conforme onde voc√™ salvou o df_tracks_complete (interim ou processed?)\n",
    "# Se n√£o estiver no YAML, usa o caminho constru√≠do:\n",
    "TRACKS_COMPLETE_PATH = P['interim']   / \"df_tracks_complete_v5.parquet\"\n",
    "\n",
    "# Caminhos de Grafos\n",
    "# Verifica se as chaves existem no yaml, sen√£o usa padr√£o\n",
    "MATCHING_MAP_PATH    = P.get('graphs_coarsened', P['graphs_bipartite']) / F['matching_map']\n",
    "SUPER_EMB_PATH       = P.get('graphs_super', P['graphs_bipartite'])     / F['super_embeddings']\n",
    "\n",
    "# Par√¢metros\n",
    "SEED                 = PM['seed']\n",
    "\n",
    "# Configura√ß√µes Visuais Padr√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0bac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeando arquivos para o GraphSAGE...\n",
      "Todos os arquivos de entrada encontrados.\n",
      "   Output Super: super_embeddings_mean.parquet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F_nn  # <--- CORRE√á√ÉO: Mudamos de F para F_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from scipy.sparse import load_npz\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DE CAMINHOS DIN√ÇMICOS ---\n",
    "# Usamos P (Paths) e F (Files) da C√©lula Mestra\n",
    "# Certifique-se de que P e F est√£o definidos (rode a C√©lula Mestra antes se necess√°rio)\n",
    "\n",
    "print(\"Mapeando arquivos para o GraphSAGE...\")\n",
    "\n",
    "# 1. INPUTS\n",
    "# Features de Treino\n",
    "X_TRAIN_PATH = P['processed'] / F['train_features']  # Agora F √© o dicion√°rio correto\n",
    "\n",
    "# Grafo de Super-M√∫sicas\n",
    "SUPER_ADJ_PATH   = P['graphs_super'] / \"A_super_tracks_adjacency.npz\"\n",
    "SUPER_INDEX_PATH = P['graphs_super'] / \"super_m_index.parquet\"\n",
    "\n",
    "# Mapa de Matching\n",
    "MATCHING_MAP_PATH = P['graphs_coarsened'] / F['matching_map']\n",
    "\n",
    "# 2. OUTPUTS\n",
    "OUT_SUPER_EMB = P['graphs_super'] / F['super_embeddings']\n",
    "OUT_TRACK_EMB = P['processed'] / F['track_embeddings']\n",
    "\n",
    "# 3. VALIDA√á√ÉO\n",
    "missing = []\n",
    "for p in [X_TRAIN_PATH, SUPER_ADJ_PATH, SUPER_INDEX_PATH, MATCHING_MAP_PATH]:\n",
    "    if not p.exists():\n",
    "        missing.append(p.name)\n",
    "\n",
    "if missing:\n",
    "    print(f\"ERRO: Arquivos de entrada n√£o encontrados: {missing}\")\n",
    "else:\n",
    "    print(\"Todos os arquivos de entrada encontrados.\")\n",
    "    print(f\"   Output Super: {OUT_SUPER_EMB.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951fea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Carregando Estrutura do Grafo ---\n",
      "   N√≥s: 20,047 | Arestas: 1,002,350\n",
      "\n",
      "--- 2. Carregando Features e Mapeando ---\n",
      "   Agregando 49 features (M√©dia)...\n",
      "\n",
      "Data Object Criado: Data(x=[20047, 49], edge_index=[2, 1002350])\n"
     ]
    }
   ],
   "source": [
    "def build_supergraph_data(features_path, super_adj_path, super_index_path, matching_map_path):\n",
    "    print(\"--- 1. Carregando Estrutura do Grafo ---\")\n",
    "    super_m_index = pd.read_parquet(super_index_path).squeeze()\n",
    "    if isinstance(super_m_index, pd.DataFrame): super_m_index = super_m_index.iloc[:, 0]\n",
    "    \n",
    "    S_super = load_npz(super_adj_path).tocsr()\n",
    "    print(f\"   N√≥s: {S_super.shape[0]:,} | Arestas: {S_super.nnz:,}\")\n",
    "\n",
    "    print(\"\\n--- 2. Carregando Features e Mapeando ---\")\n",
    "    df_feats_track = pd.read_parquet(features_path)\n",
    "    matching_df = pd.read_parquet(matching_map_path)\n",
    "    \n",
    "    # Padronizar nomes\n",
    "    col_uri = \"original_track_uri\" if \"original_track_uri\" in matching_df.columns else \"track_uri\"\n",
    "    df_map = matching_df[[col_uri, \"super_track_id\"]].rename(columns={col_uri: \"track_uri\"})\n",
    "    if \"track_uri\" not in df_feats_track.columns and \"id\" in df_feats_track.columns:\n",
    "        df_feats_track = df_feats_track.rename(columns={\"id\": \"track_uri\"})\n",
    "\n",
    "    # Merge (Inner Join)\n",
    "    df_merged = df_feats_track.merge(df_map, on=\"track_uri\", how=\"inner\")\n",
    "    \n",
    "    # Filtrar apenas colunas num√©ricas para agrega√ß√£o\n",
    "    exclude = [\"track_uri\", \"super_track_id\", \"pid\", \"release_year\"]\n",
    "    feature_cols = [c for c in df_merged.columns if c not in exclude and df_merged[c].dtype.kind in 'bifc']\n",
    "    \n",
    "    print(f\"   Agregando {len(feature_cols)} features (M√©dia)...\")\n",
    "    df_super_feats = df_merged.groupby(\"super_track_id\")[feature_cols].mean().reset_index()\n",
    "\n",
    "    # Alinhar com a ordem da matriz de adjac√™ncia\n",
    "    df_super_index = pd.DataFrame({\"super_track_id\": super_m_index.values})\n",
    "    df_super_feats = df_super_index.merge(df_super_feats, on=\"super_track_id\", how=\"left\").fillna(0.0)\n",
    "    \n",
    "    # Criar Tensores\n",
    "    x = torch.from_numpy(df_super_feats[feature_cols].to_numpy().astype(\"float32\"))\n",
    "    \n",
    "    S_coo = S_super.tocoo()\n",
    "    edge_index = torch.stack([torch.from_numpy(S_coo.row), torch.from_numpy(S_coo.col)], dim=0).long()\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    print(\"\\nData Object Criado:\", data)\n",
    "    \n",
    "    return data, df_super_feats\n",
    "\n",
    "# Execu√ß√£o\n",
    "data_super, df_super_feats = build_supergraph_data(X_TRAIN_PATH, SUPER_ADJ_PATH, SUPER_INDEX_PATH, MATCHING_MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341ab1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperGraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, out_channels=64, num_layers=2, dropout=0.2, aggr='mean'):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # Camada 1\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr=aggr))\n",
    "        # Camadas Intermedi√°rias\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggr))\n",
    "        # Camada Sa√≠da\n",
    "        if num_layers > 1:\n",
    "            self.convs.append(SAGEConv(hidden_channels, out_channels, aggr=aggr))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index)\n",
    "            # CORRE√á√ÉO AQUI: Usando F_nn em vez de F\n",
    "            x = F_nn.relu(x)\n",
    "            x = F_nn.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85121e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Trava todas as sementes aleat√≥rias para resultados reproduz√≠veis.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"üå± Seed fixada em {seed}\")\n",
    "\n",
    "def train_model(data, hidden_channels=64, epochs=200, lr=0.001, aggr_type='mean', seed=42):\n",
    "    # 1. Fixar Seed\n",
    "    set_seed(seed)\n",
    "    \n",
    "    print(f\"\\n--- Iniciando Treino (Aggr: {aggr_type}, Epochs: {epochs}, Hidden: {hidden_channels}) ---\")\n",
    "    \n",
    "    transform = RandomLinkSplit(\n",
    "        num_val=0.1, \n",
    "        num_test=0.0, \n",
    "        is_undirected=False, \n",
    "        add_negative_train_samples=False\n",
    "    )\n",
    "    train_data, val_data, _ = transform(data)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = SuperGraphSAGE(data.num_features, hidden_channels, hidden_channels, 2, 0.2, aggr_type).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_data = train_data.to(device)\n",
    "    val_data = val_data.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z = model(train_data.x, train_data.edge_index)\n",
    "        \n",
    "        pos_edge = train_data.edge_label_index[:, train_data.edge_label == 1]\n",
    "        neg_edge = torch.randint(0, data.num_nodes, pos_edge.shape, device=device)\n",
    "        \n",
    "        pos_out = (z[pos_edge[0]] * z[pos_edge[1]]).sum(dim=-1)\n",
    "        neg_out = (z[neg_edge[0]] * z[neg_edge[1]]).sum(dim=-1)\n",
    "        \n",
    "        logits = torch.cat([pos_out, neg_out])\n",
    "        labels = torch.cat([torch.ones_like(pos_out), torch.zeros_like(neg_out)])\n",
    "        \n",
    "        # CORRE√á√ÉO AQUI: Usando F_nn\n",
    "        loss = F_nn.binary_cross_entropy_with_logits(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                z_val = model(val_data.x, val_data.edge_index)\n",
    "                \n",
    "                val_pos = val_data.edge_label_index[:, val_data.edge_label == 1]\n",
    "                val_neg = torch.randint(0, data.num_nodes, val_pos.shape, device=device)\n",
    "                \n",
    "                pos_prob = (z_val[val_pos[0]] * z_val[val_pos[1]]).sum(dim=-1).sigmoid()\n",
    "                neg_prob = (z_val[val_neg[0]] * z_val[val_neg[1]]).sum(dim=-1).sigmoid()\n",
    "                \n",
    "                pos_acc = (pos_prob > 0.5).float().mean()\n",
    "                neg_acc = (neg_prob < 0.5).float().mean()\n",
    "                acc = (pos_acc + neg_acc) / 2\n",
    "                \n",
    "            print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Val Acc: {acc:.4f} (Pos: {pos_acc:.2f}, Neg: {neg_acc:.2f})\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_z = model(data.x.to(device), data.edge_index.to(device)).cpu().numpy()\n",
    "    return final_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c16977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå± Seed fixada em 42\n",
      "\n",
      "--- Iniciando Treino (Aggr: mean, Epochs: 200, Hidden: 64) ---\n",
      "Epoch 020 | Loss: 0.9909 | Val Acc: 0.5695 (Pos: 0.95, Neg: 0.19)\n",
      "Epoch 040 | Loss: 0.7274 | Val Acc: 0.6580 (Pos: 0.90, Neg: 0.41)\n",
      "Epoch 060 | Loss: 0.6671 | Val Acc: 0.6854 (Pos: 0.92, Neg: 0.45)\n",
      "Epoch 080 | Loss: 0.6357 | Val Acc: 0.7021 (Pos: 0.93, Neg: 0.48)\n",
      "Epoch 100 | Loss: 0.6116 | Val Acc: 0.7137 (Pos: 0.93, Neg: 0.50)\n",
      "Epoch 120 | Loss: 0.5927 | Val Acc: 0.7240 (Pos: 0.94, Neg: 0.51)\n",
      "Epoch 140 | Loss: 0.5760 | Val Acc: 0.7283 (Pos: 0.94, Neg: 0.52)\n",
      "Epoch 160 | Loss: 0.5661 | Val Acc: 0.7334 (Pos: 0.94, Neg: 0.52)\n",
      "Epoch 180 | Loss: 0.5557 | Val Acc: 0.7362 (Pos: 0.94, Neg: 0.53)\n",
      "Epoch 200 | Loss: 0.5490 | Val Acc: 0.7367 (Pos: 0.95, Neg: 0.53)\n",
      "\n",
      "Shape dos Embeddings Finais: (20047, 64)\n"
     ]
    }
   ],
   "source": [
    "# MEAN aggregation, 64 canais, 200 √©pocas.\n",
    "z_final = train_model(\n",
    "    data_super,\n",
    "    hidden_channels=64,\n",
    "    epochs=200,\n",
    "    lr=0.001,\n",
    "    aggr_type='mean'\n",
    ")\n",
    "\n",
    "print(\"\\nShape dos Embeddings Finais:\", z_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bbb7ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Salvando Embeddings das Super-M√∫sicas ---\n",
      "   Salvando em: /Users/lucasborges/Downloads/TCC/graphs/super_item_item/super_embeddings_mean.parquet\n",
      "\n",
      "--- 2. Projetando Embeddings para o Treino ---\n",
      "   Merge realizado com sucesso: 324,132 faixas mapeadas.\n",
      "   Salvando em: /Users/lucasborges/Downloads/TCC/data/processed/track_embeddings_mean.parquet\n",
      "\n",
      "==================================================\n",
      "PIPELINE GRAPHSAGE CONCLU√çDO!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 1. Salvando Embeddings das Super-M√∫sicas ---\")\n",
    "# Criar colunas emb_mean_000, emb_mean_001, ...\n",
    "emb_cols = [f\"emb_mean_{i:03d}\" for i in range(z_final.shape[1])]\n",
    "\n",
    "# Montar DataFrame\n",
    "df_super_emb = df_super_feats[[\"super_track_id\"]].copy()\n",
    "# Atribuir valores (z_final vem da C√©lula 6)\n",
    "df_super_emb[emb_cols] = z_final\n",
    "\n",
    "# Salvar\n",
    "print(f\"   Salvando em: {OUT_SUPER_EMB}\")\n",
    "df_super_emb.to_parquet(OUT_SUPER_EMB, index=False)\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. Projetando Embeddings para o Treino ---\")\n",
    "# Isso serve para que o Avaliador n√£o precise recalcular o hist√≥rico toda vez\n",
    "\n",
    "# Carregar Mapa (Track -> Super)\n",
    "matching_df = pd.read_parquet(MATCHING_MAP_PATH)\n",
    "# Padronizar nome da coluna de URI\n",
    "col_uri = \"original_track_uri\" if \"original_track_uri\" in matching_df.columns else \"track_uri\"\n",
    "matching_df = matching_df.rename(columns={col_uri: \"track_uri\"})\n",
    "\n",
    "# Carregar IDs de Treino\n",
    "df_train = pd.read_parquet(X_TRAIN_PATH)\n",
    "if \"track_uri\" not in df_train.columns and \"id\" in df_train.columns:\n",
    "    df_train.rename(columns={\"id\": \"track_uri\"}, inplace=True)\n",
    "\n",
    "# Merge: Track -> Super -> Embedding\n",
    "# 1. Pega tracks de treino e descobre o super-n√≥\n",
    "df_train_emb = df_train[[\"track_uri\"]].merge(matching_df, on=\"track_uri\", how=\"inner\")\n",
    "# 2. Pega o embedding do super-n√≥\n",
    "df_train_emb = df_train_emb.merge(df_super_emb, on=\"super_track_id\", how=\"inner\")\n",
    "\n",
    "# Valida√ß√£o R√°pida\n",
    "if len(df_train_emb) == 0:\n",
    "    print(\"ALERTA CR√çTICO: O DataFrame de treino ficou vazio ap√≥s o merge!\")\n",
    "    print(\"   Verifique se os IDs no X_train batem com o matching_map.\")\n",
    "else:\n",
    "    print(f\"   Merge realizado com sucesso: {len(df_train_emb):,} faixas mapeadas.\")\n",
    "    print(f\"   Salvando em: {OUT_TRACK_EMB}\")\n",
    "    df_train_emb.to_parquet(OUT_TRACK_EMB, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE GRAPHSAGE CONCLU√çDO!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5068e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
