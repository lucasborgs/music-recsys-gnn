{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfaa41a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:00:15.373793Z",
     "iopub.status.busy": "2026-01-15T18:00:15.373496Z",
     "iopub.status.idle": "2026-01-15T18:00:17.182965Z",
     "shell.execute_reply": "2026-01-15T18:00:17.182703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Raiz do Projeto encontrada: /Users/lucasborges/Downloads/TCC\n",
      "‚öôÔ∏è Configura√ß√£o carregada de: /Users/lucasborges/Downloads/TCC/conf/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CARGA DE CONFIGURA√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "def find_project_root(anchor_file=\"conf/config.yaml\"):\n",
    "    \"\"\"\n",
    "    Sobe os diret√≥rios a partir do notebook atual at√© encontrar\n",
    "    a pasta onde 'conf/config.yaml' existe.\n",
    "    \"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    \n",
    "    # Tenta no diret√≥rio atual e sobe at√© o raiz do sistema\n",
    "    for parent in [current_path] + list(current_path.parents):\n",
    "        potential_config = parent / anchor_file\n",
    "        if potential_config.exists():\n",
    "            return parent\n",
    "            \n",
    "    raise FileNotFoundError(f\"N√£o foi poss√≠vel encontrar a raiz do projeto contendo '{anchor_file}'.\")\n",
    "\n",
    "# 1. Definir BASE_DIR (Raiz do Projeto)\n",
    "try:\n",
    "    BASE_DIR = find_project_root(\"conf/config.yaml\")\n",
    "    print(f\"üìÇ Raiz do Projeto encontrada: {BASE_DIR}\")\n",
    "except FileNotFoundError as e:\n",
    "    # Fallback manual caso a busca autom√°tica falhe (ajuste se necess√°rio)\n",
    "    print(\"Busca autom√°tica falhou. Usando fallback.\")\n",
    "    BASE_DIR = Path(\"/Users/lucasborges/Downloads/TCC\")\n",
    "\n",
    "# 2. Carregar o YAML da pasta conf\n",
    "CONFIG_PATH = BASE_DIR / \"conf/config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. ATALHOS E VARI√ÅVEIS GLOBAIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Atalhos dos Dicion√°rios do YAML\n",
    "P = {k: BASE_DIR / v for k, v in config['paths'].items()} # P de Paths\n",
    "F = config['files']                                       # F de Files\n",
    "PM = config['params']                                     # PM de Params\n",
    "\n",
    "print(f\"‚öôÔ∏è Configura√ß√£o carregada de: {CONFIG_PATH}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PONTE DE VARI√ÅVEIS\n",
    "# ==============================================================================\n",
    "\n",
    "# Caminhos de Arquivos (Apontando para o YAML)\n",
    "TRAIN_EMB_PATH       = P['processed'] / F['track_embeddings']\n",
    "NEW_EMB_PATH         = P['processed'] / F['new_track_embeddings']\n",
    "X_TRAIN_PATH         = P['processed'] / F['train_features']\n",
    "X_TEST_PATH          = P['processed'] / F['test_features']\n",
    "MATCHING_MAP_PATH    = P.get('graphs_coarsened', P['graphs_bipartite']) / F['matching_map']\n",
    "SUPER_EMB_PATH       = P.get('graphs_super', P['graphs_bipartite'])     / F['super_embeddings']\n",
    "\n",
    "# Par√¢metros\n",
    "SEED                 = PM['seed']\n",
    "\n",
    "# Configura√ß√µes Visuais Padr√£o\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8546a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:00:17.184245Z",
     "iopub.status.busy": "2026-01-15T18:00:17.184122Z",
     "iopub.status.idle": "2026-01-15T18:00:17.521873Z",
     "shell.execute_reply": "2026-01-15T18:00:17.521627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeando arquivos para Infer√™ncia (Cold-Start)...\n",
      "Output definido para: new_track_embeddings_mean.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DIN√ÇMICA (Usando P e F) ---\n",
    "print(\"Mapeando arquivos para Infer√™ncia (Cold-Start)...\")\n",
    "\n",
    "# Inputs\n",
    "X_TEST_PATH       = P['processed'] / F['test_features']\n",
    "X_TRAIN_PATH      = P['processed'] / F['train_features'] # Necess√°rio para calcular centr√≥ides\n",
    "MATCHING_MAP_PATH = P['graphs_coarsened'] / F['matching_map']\n",
    "SUPER_EMB_PATH    = P['graphs_super'] / F['super_embeddings']\n",
    "\n",
    "# Output\n",
    "OUT_NEW_EMB       = P['processed'] / F['new_track_embeddings']\n",
    "\n",
    "# Valida√ß√£o\n",
    "for path in [X_TEST_PATH, MATCHING_MAP_PATH, SUPER_EMB_PATH]:\n",
    "    if not path.exists():\n",
    "        print(f\"Aviso: Arquivo n√£o encontrado: {path.name}\")\n",
    "\n",
    "print(f\"Output definido para: {OUT_NEW_EMB.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9566bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_cold_start_weighted(df_test, df_super_feats, df_super_emb, feat_cols, batch_size=2000, k=5):\n",
    "    print(f\"Iniciando KNN Ponderado (K={k}) para {len(df_test):,} m√∫sicas...\")\n",
    "\n",
    "    # 1. Prepara√ß√£o\n",
    "    df_super_feats = df_super_feats.sort_values(\"super_track_id\").reset_index(drop=True)\n",
    "    X_super = df_super_feats[feat_cols].to_numpy().astype('float32')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_super_scaled = scaler.fit_transform(X_super)\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=k, metric='euclidean', n_jobs=-1)\n",
    "    knn.fit(X_super_scaled)\n",
    "    \n",
    "    emb_cols = [c for c in df_super_emb.columns if c.startswith(\"emb_\")]\n",
    "    emb_lookup = df_super_feats[[\"super_track_id\"]].merge(df_super_emb, on=\"super_track_id\", how=\"left\")\n",
    "    emb_matrix = emb_lookup[emb_cols].fillna(0).to_numpy().astype('float32')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(df_test), batch_size)):\n",
    "        batch = df_test.iloc[i : i + batch_size].copy()\n",
    "        X_batch = batch[feat_cols].fillna(0).to_numpy().astype('float32')\n",
    "        X_batch_scaled = scaler.transform(X_batch)\n",
    "        \n",
    "        dists, indices = knn.kneighbors(X_batch_scaled)\n",
    "        \n",
    "        # --- C√ÅLCULO DE PESOS (Softmax Est√°vel) ---\n",
    "        # Mantendo sua l√≥gica excelente de estabilidade num√©rica\n",
    "        neg_dists = -dists\n",
    "        max_neg_dists = np.max(neg_dists, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(neg_dists - max_neg_dists)\n",
    "        weights = exp_x / np.sum(exp_x, axis=1, keepdims=True) # [Batch, K]\n",
    "        \n",
    "        # --- INTERPOLA√á√ÉO VETORIZADA ---\n",
    "        # Multiplica√ß√£o de matrizes 3D para evitar o loop j in range(k)\n",
    "        # neighbor_embs shape: [Batch, K, Emb_Dim]\n",
    "        neighbor_embs = emb_matrix[indices] \n",
    "        \n",
    "        # Pondera√ß√£o: (Batch, 1, K) @ (Batch, K, Emb_Dim) -> (Batch, 1, Emb_Dim)\n",
    "        batch_embs = np.matmul(weights[:, np.newaxis, :], neighbor_embs).squeeze(1)\n",
    "        \n",
    "        # --- RE-NORMALIZA√á√ÉO L2 ---\n",
    "        # Garante que a m√∫sica nova tenha a mesma escala m√©trica que as m√∫sicas de treino\n",
    "        norms = np.linalg.norm(batch_embs, axis=1, keepdims=True)\n",
    "        batch_embs = batch_embs / (norms + 1e-10) # 1e-10 evita divis√£o por zero\n",
    "        \n",
    "        res_df = batch[[\"track_uri\"]].copy()\n",
    "        res_df[emb_cols] = batch_embs\n",
    "        results.append(res_df)\n",
    "        \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "def get_super_features_light():\n",
    "    \"\"\"\n",
    "    Recalcula as features m√©dias dos super-n√≥s da forma mais leve poss√≠vel\n",
    "    e deleta o X_train da mem√≥ria imediatamente.\n",
    "    \"\"\"\n",
    "    print(\"Recalculando features dos Super-N√≥s (Modo Econ√¥mico)...\")\n",
    "    \n",
    "    # 1. Carregar Map (Track -> Super_Node)\n",
    "    # Usamos MATCHING_MAP_PATH definido na c√©lula mestra do S6.1\n",
    "    map_df = pd.read_parquet(MATCHING_MAP_PATH)\n",
    "    col_uri = \"original_track_uri\" if \"original_track_uri\" in map_df.columns else \"track_uri\"\n",
    "    map_df = map_df.rename(columns={col_uri: \"track_uri\"})[[\"track_uri\", \"super_track_id\"]]\n",
    "    \n",
    "    # 2. Carregar X_train (Features de √Åudio originais)\n",
    "    if not X_TRAIN_PATH.exists():\n",
    "        raise FileNotFoundError(f\"X_train n√£o encontrado em {X_TRAIN_PATH}\")\n",
    "        \n",
    "    df_train = pd.read_parquet(X_TRAIN_PATH)\n",
    "    if \"track_uri\" not in df_train.columns and \"id\" in df_train.columns:\n",
    "        df_train = df_train.rename(columns={\"id\": \"track_uri\"})\n",
    "        \n",
    "    # 3. Merge para associar features aos super-n√≥s\n",
    "    print(f\"   Cruzando {len(df_train):,} faixas com mapa...\")\n",
    "    merged = df_train.merge(map_df, on=\"track_uri\", how=\"inner\")\n",
    "    \n",
    "    # Limpeza imediata de mem√≥ria para evitar crash\n",
    "    del df_train, map_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Identificar colunas num√©ricas (features que o KNN usar√°)\n",
    "    cols = [c for c in merged.columns if c not in [\"track_uri\", \"super_track_id\"] and merged[c].dtype.kind in 'bifc']\n",
    "    \n",
    "    # 4. Groupby e Mean (C√°lculo do Centr√≥ide de √Åudio do Super-N√≥)\n",
    "    print(\"   Calculando centr√≥ides...\")\n",
    "    super_feats = merged.groupby(\"super_track_id\")[cols].mean().reset_index()\n",
    "    \n",
    "    del merged\n",
    "    gc.collect()\n",
    "    \n",
    "    return super_feats, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf48f348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:00:17.529186Z",
     "iopub.status.busy": "2026-01-15T18:00:17.529092Z",
     "iopub.status.idle": "2026-01-15T18:00:20.513225Z",
     "shell.execute_reply": "2026-01-15T18:00:20.512866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando Embeddings dos Super-N√≥s...\n",
      "Recalculando features dos Super-N√≥s (Modo Econ√¥mico)...\n",
      "   Cruzando 324,305 faixas com mapa...\n",
      "   Calculando centr√≥ides...\n",
      "Features de Super-N√≥s prontas: (20641, 52)\n",
      "Carregando Teste e Mapa...\n",
      " Separando casos (Lookup vs KNN)...\n",
      "   Via Lookup direto: 0\n",
      "   Via KNN (Cold-Start): 60,417\n",
      "Processando KNN Ponderado...\n",
      "Iniciando KNN Ponderado (K=5) para 60,417 m√∫sicas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [00:02<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando 60,417 linhas em: /Users/lucasborges/Downloads/TCC/data/processed/new_track_embeddings_mean.parquet\n",
      "\n",
      "CONCLU√çDO! Embeddings de teste gerados (Refined Approach).\n"
     ]
    }
   ],
   "source": [
    "# --- EXECU√á√ÉO ---\n",
    "\n",
    "# 1. Carregar Embeddings GNN\n",
    "print(\"Carregando Embeddings dos Super-N√≥s...\")\n",
    "df_super_emb = pd.read_parquet(SUPER_EMB_PATH)\n",
    "\n",
    "# 2. Obter Features dos Centr√≥ides (Super-N√≥s)\n",
    "df_super_feats, feat_cols = get_super_features_light()\n",
    "print(f\"Features de Super-N√≥s prontas: {df_super_feats.shape}\")\n",
    "\n",
    "# 3. Carregar Teste e Mapa\n",
    "print(\"Carregando Teste e Mapa...\")\n",
    "df_test = pd.read_parquet(X_TEST_PATH)\n",
    "if \"track_uri\" not in df_test.columns and \"id\" in df_test.columns: \n",
    "    df_test = df_test.rename(columns={\"id\": \"track_uri\"})\n",
    "\n",
    "map_df = pd.read_parquet(MATCHING_MAP_PATH)\n",
    "col_uri = \"original_track_uri\" if \"original_track_uri\" in map_df.columns else \"track_uri\"\n",
    "map_df = map_df.rename(columns={col_uri: \"track_uri\"})\n",
    "\n",
    "# 4. Separar Casos\n",
    "print(\" Separando casos (Lookup vs KNN)...\")\n",
    "# Lookup: M√∫sicas que j√° cairam no grafo\n",
    "lookup_ids = df_test.merge(map_df, on=\"track_uri\", how=\"inner\")[[\"track_uri\", \"super_track_id\"]]\n",
    "known_uris = set(lookup_ids[\"track_uri\"])\n",
    "\n",
    "# Orphans: M√∫sicas novas\n",
    "df_orphans = df_test[~df_test[\"track_uri\"].isin(known_uris)].copy()\n",
    "\n",
    "print(f\"   Via Lookup direto: {len(lookup_ids):,}\")\n",
    "print(f\"   Via KNN (Cold-Start): {len(df_orphans):,}\")\n",
    "\n",
    "final_parts = []\n",
    "\n",
    "# CASO A: Lookup Direto\n",
    "if len(lookup_ids) > 0:\n",
    "    print(\"Processando Lookup Direto...\")\n",
    "    # Merge simples\n",
    "    df_lookup = lookup_ids.merge(df_super_emb, on=\"super_track_id\", how=\"inner\")\n",
    "    if \"super_track_id\" in df_lookup.columns:\n",
    "        df_lookup = df_lookup.drop(columns=[\"super_track_id\"])\n",
    "    final_parts.append(df_lookup)\n",
    "\n",
    "# CASO B: KNN Ponderado\n",
    "if len(df_orphans) > 0:\n",
    "    print(\"Processando KNN Ponderado...\")\n",
    "    df_knn = resolve_cold_start_weighted(\n",
    "        df_orphans, \n",
    "        df_super_feats, \n",
    "        df_super_emb, \n",
    "        feat_cols, \n",
    "        batch_size=2000, \n",
    "        k=5 \n",
    "    )\n",
    "    final_parts.append(df_knn)\n",
    "\n",
    "# 5. Concatenar e Salvar\n",
    "if final_parts:\n",
    "    df_final = pd.concat(final_parts, ignore_index=True)\n",
    "    print(f\"Salvando {len(df_final):,} linhas em: {OUT_NEW_EMB}\")\n",
    "    df_final.to_parquet(OUT_NEW_EMB, index=False)\n",
    "    print(\"\\nCONCLU√çDO! Embeddings de teste gerados (Refined Approach).\")\n",
    "else:\n",
    "    print(\"Aviso: Nenhuma m√∫sica processada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8063c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Valida√ß√£o: Tracks de Treino (GraphSAGE) ===\n",
      "Norma M√©dia: 1.000000 (¬±0.000000)\n",
      "Range: [1.000000 a 1.000000]\n",
      "‚úÖ SUCESSO: Todos os vetores possuem norma unit√°ria (1.0).\n",
      "------------------------------\n",
      "=== Valida√ß√£o: Tracks Novas (Weighted k-NN) ===\n",
      "Norma M√©dia: 1.000000 (¬±0.000000)\n",
      "Range: [1.000000 a 1.000000]\n",
      "‚úÖ SUCESSO: Todos os vetores possuem norma unit√°ria (1.0).\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def validate_embeddings_norm(df, name=\"Dataset\"):\n",
    "    # 1. Identificar colunas de embedding (ex: emb_mean_000...)\n",
    "    emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "    if not emb_cols:\n",
    "        print(f\"‚ùå {name}: Nenhuma coluna de embedding encontrada.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Converter para numpy\n",
    "    embs = df[emb_cols].to_numpy()\n",
    "    \n",
    "    # 3. Calcular a norma L2 de cada linha\n",
    "    norms = np.linalg.norm(embs, axis=1)\n",
    "    \n",
    "    # 4. Estat√≠sticas\n",
    "    mean_norm = np.mean(norms)\n",
    "    std_norm = np.std(norms)\n",
    "    min_norm = np.min(norms)\n",
    "    max_norm = np.max(norms)\n",
    "    \n",
    "    # 5. Verifica√ß√£o de Toler√¢ncia (erro de ponto flutuante)\n",
    "    # Todos devem estar muito pr√≥ximos de 1.0\n",
    "    is_valid = np.allclose(norms, 1.0, atol=1e-3)\n",
    "    \n",
    "    print(f\"=== Valida√ß√£o: {name} ===\")\n",
    "    print(f\"Norma M√©dia: {mean_norm:.6f} (¬±{std_norm:.6f})\")\n",
    "    print(f\"Range: [{min_norm:.6f} a {max_norm:.6f}]\")\n",
    "    \n",
    "    if is_valid:\n",
    "        print(\"‚úÖ SUCESSO: Todos os vetores possuem norma unit√°ria (1.0).\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ALERTA: Detectados vetores fora da norma unit√°ria.\")\n",
    "        print(\"   Isso pode causar vi√©s na Similaridade de Cosseno no S7.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- EXECU√á√ÉO ---\n",
    "# Verificando os dois arquivos principais gerados/usados no S6.1\n",
    "try:\n",
    "    # Substitua pelos nomes reais dos seus DataFrames se eles ainda estiverem em mem√≥ria\n",
    "    # Ou carregue-os dos caminhos definidos (OUT_TRACK_EMB e o retorno da resolve_cold_start)\n",
    "    \n",
    "    # Exemplo com os arquivos salvos:\n",
    "    df_train_val = pd.read_parquet(P['processed'] / F['track_embeddings'])\n",
    "    df_test_val = pd.read_parquet(P['processed'] / F['new_track_embeddings'])\n",
    "    \n",
    "    validate_embeddings_norm(df_train_val, \"Tracks de Treino (GraphSAGE)\")\n",
    "    validate_embeddings_norm(df_test_val, \"Tracks Novas (Weighted k-NN)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar arquivos para valida√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d42dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
